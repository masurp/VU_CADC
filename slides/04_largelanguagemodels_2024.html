<!DOCTYPE html>
<html lang="en"><head>
<script src="04_largelanguagemodels_2024_files/libs/clipboard/clipboard.min.js"></script>
<script src="04_largelanguagemodels_2024_files/libs/quarto-html/tabby.min.js"></script>
<script src="04_largelanguagemodels_2024_files/libs/quarto-html/popper.min.js"></script>
<script src="04_largelanguagemodels_2024_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="04_largelanguagemodels_2024_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04_largelanguagemodels_2024_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="04_largelanguagemodels_2024_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="04_largelanguagemodels_2024_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.4.553">

  <meta name="author" content="Dr.&nbsp;Philipp K. Masur">
  <title>Transformers and Large Language Models</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="04_largelanguagemodels_2024_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="04_largelanguagemodels_2024_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="04_largelanguagemodels_2024_files/libs/revealjs/dist/theme/quarto.css">
  <link href="04_largelanguagemodels_2024_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="04_largelanguagemodels_2024_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="04_largelanguagemodels_2024_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="04_largelanguagemodels_2024_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="https://storage.googleapis.com/exenzo-jobboard/bb/public/thumb/logo-jpg-57.jpg" data-background-position="top right" data-background-size="auto" class="quarto-title-block center">
  <h1 class="title">Transformers and Large Language Models</h1>
  <p class="subtitle">Week 4: Bert, Llama, GPT, and Co</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;Philipp K. Masur 
</div>
</div>
</div>

</section>
<section id="section" class="slide level2" data-background="#43464B" data-background-image="img/GPT_viz_imagination.jpg">
<h2></h2>
</section>
<section id="a-look-back-at-the-chronology-of-nlp" class="slide level2">
<h2>A Look Back at the Chronology of NLP</h2>

<img data-src="img/timeline/Slide1b.png" class="r-stretch"></section>
<section id="a-look-back-at-the-chronology-of-nlp-1" class="slide level2">
<h2>A Look Back at the Chronology of NLP</h2>

<img data-src="img/timeline/Slide2b.png" class="r-stretch"></section>
<section id="what-we-focus-on-today" class="slide level2">
<h2>What we focus on today…</h2>

<img data-src="img/timeline/Slide3b.png" class="r-stretch"></section>
<section id="the-classic-machine-learning-approach" class="slide level2">
<h2>The classic machine learning approach</h2>
<ul>
<li><p>A lot of different steps…</p></li>
<li><p>Training takes a lot of data and time… and performance was still somewhat</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href=""></a><span class="co"># Get data</span></span>
<span id="cb1-2"><a href=""></a>science_data <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/science.csv"</span>) <span class="sc">|&gt;</span>  <span class="fu">filter</span>(label <span class="sc">!=</span> <span class="st">"biology"</span> <span class="sc">&amp;</span> label <span class="sc">!=</span> <span class="st">"finance"</span> <span class="sc">&amp;</span> label <span class="sc">!=</span> <span class="st">"mathematics"</span>)</span>
<span id="cb1-3"><a href=""></a></span>
<span id="cb1-4"><a href=""></a><span class="co"># Create test and train data sets</span></span>
<span id="cb1-5"><a href=""></a>split <span class="ot">&lt;-</span> <span class="fu">initial_split</span>(science_data, <span class="at">prop =</span> .<span class="dv">50</span>)</span>
<span id="cb1-6"><a href=""></a></span>
<span id="cb1-7"><a href=""></a><span class="co"># Feature engineering</span></span>
<span id="cb1-8"><a href=""></a>rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(sentiment <span class="sc">~</span> lemmata, <span class="at">data =</span> science_data) <span class="sc">|&gt;</span> </span>
<span id="cb1-9"><a href=""></a>  <span class="fu">step_tokenize</span>(lemmata) <span class="sc">|&gt;</span> </span>
<span id="cb1-10"><a href=""></a>  <span class="fu">step_tf</span>(<span class="fu">all_predictors</span>())  <span class="sc">|&gt;</span> </span>
<span id="cb1-11"><a href=""></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())</span>
<span id="cb1-12"><a href=""></a></span>
<span id="cb1-13"><a href=""></a><span class="co"># Setup algorithm/model</span></span>
<span id="cb1-14"><a href=""></a>mlp_spec <span class="ot">&lt;-</span> <span class="fu">mlp</span>(<span class="at">epochs =</span> <span class="dv">600</span>, <span class="at">hidden_units =</span> <span class="fu">c</span>(<span class="dv">6</span>),  </span>
<span id="cb1-15"><a href=""></a>                <span class="at">penalty =</span> <span class="fl">0.01</span>, <span class="at">learn_rate =</span> <span class="fl">0.2</span>) <span class="sc">|&gt;</span>   </span>
<span id="cb1-16"><a href=""></a>  <span class="fu">set_engine</span>(<span class="st">"brulee"</span>) <span class="sc">|&gt;</span>    </span>
<span id="cb1-17"><a href=""></a>  <span class="fu">set_mode</span>(<span class="st">"classification"</span>)</span>
<span id="cb1-18"><a href=""></a></span>
<span id="cb1-19"><a href=""></a><span class="co"># Create workflow</span></span>
<span id="cb1-20"><a href=""></a>mlp_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">|&gt;</span> </span>
<span id="cb1-21"><a href=""></a>  <span class="fu">add_recipe</span>(rec) <span class="sc">|&gt;</span> </span>
<span id="cb1-22"><a href=""></a>  <span class="fu">add_model</span>(mlp_spec)</span>
<span id="cb1-23"><a href=""></a></span>
<span id="cb1-24"><a href=""></a><span class="co"># Fit model</span></span>
<span id="cb1-25"><a href=""></a>m_mlp <span class="ot">&lt;-</span> <span class="fu">fit</span>(mlp_workflow, <span class="at">data =</span> <span class="fu">training</span>(split))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="section-1" class="slide level2" data-background-image="https://ychef.files.bbci.co.uk/976x549/p01pqw85.jpg">
<h2></h2>
</section>
<section id="what-if-things-were-easier" class="slide level2">
<h2>What if things were easier?</h2>
<ul>
<li><p>Wouldn’t it be great if we would not have to wrangle with the data, not engage in any text preprocessing, and simply let the “computer” figure this out?</p></li>
<li><p>In fact, aren’t we living in times were we can simply ask the computer, similar as Theodore in the movie “Her”?</p></li>
</ul>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href=""></a><span class="fu">library</span>(tidyllm)</span>
<span id="cb2-2"><a href=""></a><span class="fu">library</span>(glue)</span>
<span id="cb2-3"><a href=""></a></span>
<span id="cb2-4"><a href=""></a><span class="fu">glue</span>(<span class="st">'Classify this scientific abstract: {abstract}</span></span>
<span id="cb2-5"><a href=""></a></span>
<span id="cb2-6"><a href=""></a><span class="st">     Pick one of the following fields.</span></span>
<span id="cb2-7"><a href=""></a><span class="st">     Provide only the name of the field:</span></span>
<span id="cb2-8"><a href=""></a><span class="st">     </span></span>
<span id="cb2-9"><a href=""></a><span class="st">     Physics</span></span>
<span id="cb2-10"><a href=""></a><span class="st">     Computer Science</span></span>
<span id="cb2-11"><a href=""></a><span class="st">     Statistics'</span>, </span>
<span id="cb2-12"><a href=""></a>     <span class="at">abstract =</span> science_data<span class="sc">$</span>text[<span class="dv">2</span>]) <span class="sc">|&gt;</span> </span>
<span id="cb2-13"><a href=""></a>  <span class="fu">llm_message</span>() <span class="sc">|&gt;</span> </span>
<span id="cb2-14"><a href=""></a>  <span class="fu">chat</span>(<span class="fu">ollama</span>(<span class="at">.model =</span> <span class="st">"llama3"</span>, <span class="at">.temperature =</span> <span class="dv">0</span>)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output cell-output-stdout">
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource output number-lines code-with-copy"><code class="sourceCode"><span id="cb3-1"><a href=""></a>Message History:</span>
<span id="cb3-2"><a href=""></a>system: You are a helpful assistant</span>
<span id="cb3-3"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb3-4"><a href=""></a>user: Classify this scientific abstract: Rotation Invariance Neural Network Rotation invariance and translation invariance have great values in image</span>
<span id="cb3-5"><a href=""></a>recognition tasks. In this paper, we bring a new architecture in convolutional</span>
<span id="cb3-6"><a href=""></a>neural network (CNN) named cyclic convolutional layer to achieve rotation</span>
<span id="cb3-7"><a href=""></a>invariance in 2-D symbol recognition. We can also get the position and</span>
<span id="cb3-8"><a href=""></a>orientation of the 2-D symbol by the network to achieve detection purpose for</span>
<span id="cb3-9"><a href=""></a>multiple non-overlap target. Last but not least, this architecture can achieve</span>
<span id="cb3-10"><a href=""></a>one-shot learning in some cases using those invariance.</span>
<span id="cb3-11"><a href=""></a></span>
<span id="cb3-12"><a href=""></a></span>
<span id="cb3-13"><a href=""></a>Pick one of the following fields.</span>
<span id="cb3-14"><a href=""></a>Provide only the name of the field:</span>
<span id="cb3-15"><a href=""></a></span>
<span id="cb3-16"><a href=""></a>Physics</span>
<span id="cb3-17"><a href=""></a>Computer Science</span>
<span id="cb3-18"><a href=""></a>Statistics</span>
<span id="cb3-19"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb3-20"><a href=""></a>assistant: Computer Science</span>
<span id="cb3-21"><a href=""></a>--------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="content-of-this-lecture" class="slide level2 smaller">
<h2>Content of this Lecture</h2>
<div class="columns">
<div class="column" style="width:45%;">
<ol type="1">
<li><p>Machine Learning vs.&nbsp;Deep Learning</p>
<p>1.1. Reminder: Text Classification Pipeline</p>
<p>1.2. How did the Field move on?</p></li>
<li><p>The Rise of Transformers and Transfer Learning</p>
<p>3.1. Overview</p>
<p>3.2. Transfer Learning</p>
<p>3.3. Architecture of the Transformer Model</p>
<p>3.4. The Transformer Text Classification Pipeline Using BERT</p></li>
<li><p>Large Language Models: BERT, Llama, GPT and Co</p>
<p>3.1. What are Large Language Models and Generative AI?</p>
<p>3.2. General Idea: Next-Token-Prediction</p>
<p>3.3. A Peek into the Architecture of GPT</p></li>
</ol>
</div><div class="column" style="width:5%;">

</div><div class="column" style="width:45%;">
<ol start="4" type="1">
<li><p>Using LLMs for Text Classification</p>
<p>4.1. Zero-Shot, One-Short, and Few-Shot Classification</p>
<p>4.2. Text Classification Using Llama and GPT</p>
<p>4.3. Validation, validation, validation!</p>
<p>4.4. Examples in the Literature</p></li>
<li><p>Summary and conclusion</p>
<p>5.1. State-of-the-Art in Classification</p>
<p>5.2. Ethical considerations</p>
<p>5.3. Conclusion</p></li>
</ol>
</div>
</div>
</section>
<section>
<section id="machine-learning-vs.-deep-learning" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Machine Learning vs.&nbsp;Deep Learning</h1>

</section>
<section id="our-text-classification-pipeline" class="slide level2">
<h2>Our Text Classification Pipeline</h2>

<img data-src="img/text_analysis_fundamentals/Slide01.png" class="r-stretch"></section>
<section id="classic-machine-learning-1990-2013" class="slide level2">
<h2>Classic Machine Learning (1990-2013)</h2>

<img data-src="img/text_analysis_fundamentals/Slide03.png" class="r-stretch"></section>
<section id="word-embeddings-2013-2020" class="slide level2">
<h2>Word-Embeddings (2013-2020)</h2>

<img data-src="img/text_analysis_fundamentals/Slide04.png" class="r-stretch"></section>
<section id="but-massive-advancements-in-recent-years" class="slide level2">
<h2>But Massive advancements in recent years</h2>
<ol type="1">
<li><p>Massive advancement in how text can be represented at numbers</p>
<ul>
<li>From simple word counts to word embeddings</li>
<li>From Static to contextual word embeddings</li>
<li>Increasingly better embedding of meaning</li>
</ul></li>
<li><p>Pretraining and transfer learning</p>
<ul>
<li>Word embeddings can be trained on large scale corpus</li>
<li>Pretrained word embeddings can fine-tuned (less training data) and then used for downstream tasks</li>
</ul></li>
<li><p>Transformers and Generative AI</p>
<ul>
<li>Larger and larger “language models”</li>
<li>New mechanisms for better embedding (e.g., Attention)</li>
<li>Conversational frameworks and Generative AI</li>
</ul></li>
</ol>
</section></section>
<section>
<section id="the-rise-of-transformers-and-transfer-learning" class="title-slide slide level1 centered center" data-background-color="steelblue">
<h1>The Rise of Transformers and Transfer Learning</h1>
<center>
<img data-src="https://pyxis.nymag.com/v1/imgs/7e2/b83/01a7d3094f5856a53f409a59b9d16e392e-22-transformers-fighting.2x.rhorizontal.w710.jpg" style="width:65.0%">
</center>
</section>
<section id="origin-of-the-transformer" class="slide level2">
<h2>Origin of the Transformer</h2>
<ul>
<li><p>Until 2017, the state-of-the-art for natural language processing was using a deep neural network (e.g., recurrent neural networks, long short-term memory and gated recurrent neural networks)</p></li>
<li><p>In a preprint called “Attention is all you need”, published in 2017 and cited more than 95,000 times, the team of Google Brain introduced the so-called <strong>Transformer</strong></p></li>
<li><p>It represents a neural network-type architecture that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence</p></li>
</ul>

<img data-src="img/transformer_paper.png" class="r-stretch"></section>
<section id="what-was-so-special-about-transformers" class="slide level2">
<h2>What was so special about transformers?</h2>
<ul>
<li><p>Transformer models apply an evolving set of mathematical techniques, called <strong>attention</strong> or <strong>self-attention</strong>, to detect subtle ways even distant data elements in a series influence and depend on each other.</p></li>
<li><p>The proposed network structure had notable characteristics:</p>
<ul>
<li>No need for recurrent or convolutional network structures</li>
<li>Based solely on attention mechanism (stacked on top of one another)</li>
<li>Required less training time (can be parallelized)</li>
</ul></li>
<li><p>It thereby outperformed prior state-of-the-art models in a variety of tasks</p></li>
<li><p>The transformer architecture is the back-bone of all current large language models and so far drives the entire “AI revolution”</p></li>
</ul>
</section>
<section id="overview-of-the-architecture" class="slide level2">
<h2>Overview of the architecture</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>The figure on the right represent an abstract overview of a transformer’s architecture</p></li>
<li><p>It can be used for next-token-predictions</p>
<ul>
<li>classic example is translation: e.g., english-to-dutch</li>
<li>but also: question-to-answer, text-to-summary, sentence-to-next-word…</li>
<li>an thus also for text classification: text-to-label</li>
</ul></li>
<li><p>Although models can differ, they generally include:</p>
<ul>
<li>An encoder-decoder framework</li>
<li>Word embeddings + positional embedding</li>
<li>Attention and self-attention modules</li>
</ul></li>
</ul>
<p><span style="font-size:0.55em;">Vaswani et al.&nbsp;2017</span></p>
</div><div class="column" style="width:40%;">
<p><img data-src="img/transformer_architecture.png"></p>
</div>
</div>
</section>
<section id="basic-encoder-decoder-framework-for-translation" class="slide level2">
<h2>Basic Encoder-Decoder Framework (for Translation)</h2>

<img data-src="img/transformer/Slide01.png" class="r-stretch"></section>
<section id="stacked-encoders-and-decoders" class="slide level2">
<h2>Stacked Encoders and Decoders</h2>

<img data-src="img/transformer/Slide02.png" class="r-stretch"></section>
<section id="more-elaborate-encoding-of-words" class="slide level2">
<h2>More elaborate encoding of words</h2>

<img data-src="img/transformer/Slide03.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018</p></section>
<section id="inside-of-an-encoder-and-a-decoder" class="slide level2">
<h2>Inside of an encoder and a decoder</h2>
<ul>
<li><p>The word, position, and time signal embeddings are passed to the first encoder</p></li>
<li><p>Here, they flow through a self-attention layer, which further refines the encoding by “looking at other words” as it encodes a specific word</p></li>
<li><p>The outputs of the self-attention layer are fed to a feed-forward neural network.</p></li>
<li><p>The decoder likewise has both layers as well, but also an extra attention layer that helps to focus on different parts of the input (e.g., the encoders outputs)</p></li>
</ul>

<img data-src="img/transformer/encoder-decoder.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018</p></section>
<section id="putting-it-all-together" class="slide level2">
<h2>Putting it all together</h2>

<img data-src="img/transformer/Slide10.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018</p></section>
<section id="different-types-of-models-for-different-tasks" class="slide level2">
<h2>Different Types of Models for different tasks</h2>

<img data-src="https://heidloff.net/assets/img/2023/02/transformers.png" class="r-stretch"></section>
<section id="bert-vs.-gpt-or-llama" class="slide level2">
<h2>BERT vs.&nbsp;GPT (or Llama)</h2>
<ul>
<li><p>Encoder-Decoder Transformers:</p>
<ul>
<li>BART (Lewis et al., 2019): tanslation, but also text generation ,…</li>
</ul></li>
<li><p>Encoder-Only Transformer</p>
<ul>
<li>BERT (Devlin et al., 2019): Embedding-only, then down-stream tasks…</li>
</ul></li>
<li><p>Decoder-Only Transformer:</p>
<ul>
<li>GPT-series (OpenAI): Text generation, …</li>
<li>Llama (Meta): Text generation…</li>
</ul></li>
</ul>
</section>
<section id="text-classification-with-transformers-encoder-only" class="slide level2">
<h2>Text Classification with Transformers, Encoder-Only</h2>

<img data-src="img/text_analysis_fundamentals/Slide04.png" class="r-stretch"></section>
<section id="text-classification-with-transformers-decoder-only" class="slide level2">
<h2>Text Classification with Transformers, Decoder-Only</h2>

<img data-src="img/text_analysis_fundamentals/Slide05.png" class="r-stretch"></section>
<section id="pre-training-fine-tuning-and-transfer-learning" class="slide level2">
<h2>Pre-training, Fine-tuning, and Transfer Learning</h2>
<ul>
<li><p>Generally, transformer models are pre-trained using specific natural language processing tasks</p>
<ul>
<li>Mask Language Modelling (LM): simply mask some percentage of the input tokens at random, and then predict those masked tokens</li>
<li>Next sentence prediction (NSP): predict sentence B from sentence A to model relationships between sentences</li>
</ul></li>
<li><p>The general idea was to use a pre-trained model and then “fine-tune” it on the specific tasks it is supposed to perform (e.g., annotating text with topics or sentiment)</p></li>
<li><p>Although the transformer’s architecture has made training more efficient (due to the ability to parallelize), it nonetheless requires significant computing power to fine-tune a model</p></li>
<li><p>As pre-training often involves tasks that are different than what we want the model to do, this is often denoted as “transfer learning”, thus a type of learning that transfers to other task as well</p></li>
<li><p>As transformer-based models become larger and larger, the need for fine-tuning decreases as they already do well on down-stream tasks (e.g., using only prompt-engineering)</p></li>
</ul>
</section></section>
<section>
<section id="very-large-language-models-llama-and-gpt" class="title-slide slide level1 centered center" data-background-color="steelblue">
<h1>Very Large Language Models: Llama and GPT</h1>
<center>
<p><img data-src="img/GPT_viz_imagination.jpg" style="width:65.0%"></p>
<p><span style="font-size:55em;">Source: <a href="https://betterprogramming.pub/how-to-create-the-matrix-text-effect-with-javascript-325c6bb7d96e">Christian Behler on Medium</a></span></p>
</center>
</section>
<section id="what-are-large-language-models" class="slide level2">
<h2>What are large language models</h2>
<ul>
<li><p>A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation</p></li>
<li><p>LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation</p></li>
<li><p>LLMs are still just a type of artificial neural networks (mainly transformers!) and are pretrained using self-supervised learning and semi-supervised learning (e.g., Mask Language Modelling)</p></li>
<li><p>As so-called autoregressive language models, they take an <strong>input text</strong> and repeatedly <strong>predicting the next token</strong> or word</p></li>
</ul>
</section>
<section id="current-models" class="slide level2">
<h2>Current models</h2>
<center>
<div class="columns">
<div class="column" style="width:10%;">
<p><img data-src="https://cdn.prod.website-files.com/63da3362f67ed649a19489ea/65a762d88d34c9b08de34039_659f1e3a57ce506fbcc81b42_who%2520owns%2520chatgpt_logo.png"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://beginswithai.com/wp-content/uploads/2024/06/claude-3.5-sonnet.png.webp"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://kodexolabs.com/wp-content/uploads/2024/07/Llama-3.1.webp"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Google_Gemini_logo.svg/2560px-Google_Gemini_logo.svg.png"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/en/1/1f/LowRes_80dpi_Mistral_AI_Logo.png"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://searchengineland.com/figz/wp-content/seloads/2019/10/GoogleBert_1920_3.jpg"></p>
</div><div class="column" style="width:10%;">
<p><img data-src="https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgg06dhS6iRpIv8hvyonlwncW-RC5n59E8vhaWRgIVqTP-Z1AbTBDtdJsX8ClDILimlGWlRAIORuZn8349TfUFmgqYyCRcoctTvNC_Kv70z41hCKd-0Fy4Ic4EgKyY0LxQ5rDt1eXi3jvEcTxgTC62glTl4e5Cffge50iiF0fxCBqmq9v-u7KTfIL4Lxb0/s1600/Gemma_social.png"></p>
</div>
</div>
</center>
<p><img data-src="img/generativeAI2.png"></p>
</section>
<section id="next-token-prediction-as-in-gpt-2" class="slide level2">
<h2>Next token prediction (as in GPT-2)</h2>

<img data-src="img/gpt2-1.png" class="r-stretch"></section>
<section id="next-token-prediction-as-in-gpt-2-1" class="slide level2">
<h2>Next token prediction (as in GPT-2)</h2>

<img data-src="img/gpt2-2.png" class="r-stretch"></section>
<section id="next-token-prediction" class="slide level2">
<h2>Next token prediction</h2>

<img data-src="img/gpt2-3.png" class="r-stretch"></section>
<section id="gpt-series-by-openai" class="slide level2">
<h2>GPT-Series by OpenAI</h2>
<ul>
<li><p>Generative Pre-trained Transformer (GPT), is a set of state-of-the-art large language model developed by OpenAI.</p></li>
<li><p>Particularly GPT-3, released publicly in November 2022 together with a chat interface, caused a lot of public attention</p></li>
<li><p>Millions of users in a very short amount of time (faster than Facebook, Instagram, TikTok, etc…), now 1.5 Billion users</p></li>
</ul>
<center>
<img data-src="https://i0.wp.com/chatgptdutch.org/wp-content/uploads/2023/08/cropped-chatgpt-icon-logo.png" style="width:50.0%">
</center>
</section>
<section id="overview-of-the-gpt-series-by-openai" class="slide level2">
<h2>Overview of the GPT-series by OpenAI</h2>

<img data-src="img/gpt-series.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Wikipedia</p></section></section>
<section>
<section id="a-peek-into-the-architecture-of-gpt" class="title-slide slide level1 centered center" data-background-color="steelblue">
<h1>A Peek into the Architecture of GPT</h1>

</section>
<section id="next-token-prediction-based-on-input-text" class="slide level2">
<h2>Next-token-Prediction based on input text</h2>

<img data-src="img/transformer3/Slide1.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Adapted from Alammar, 2018</p></section>
<section id="intricate-meaning-of-words" class="slide level2">
<h2>Intricate Meaning of Words</h2>

<img data-src="img/harry1.png" class="r-stretch"></section>
<section id="intricate-meaning-of-words-1" class="slide level2">
<h2>Intricate Meaning of Words</h2>

<img data-src="img/harry2.png" class="r-stretch"></section>
<section id="core-idea-better-encoding" class="slide level2">
<h2>Core Idea: Better Encoding</h2>
<ul>
<li><p>Based on static word-embeddings (that we discussed in the last lecture), the word “Harry” would get the same embeddings vector, even though we clearly see that they refer to different persons</p></li>
<li><p>The same is true for words like “mole” (which can refer to an animal or a little skin spot) or “model” (e.g., a fashion model vs.&nbsp;a computer model)</p></li>
<li><p>LLMs like GPT work so well, because their architecture allows them to encode additional information into a token’s embedding vector and take surrounding tokens into account</p></li>
<li><p>This way, they learn which name (e.g., Harry) refers to which person or which word (e.g., model) refers to which meaning</p></li>
</ul>
</section>
<section id="a-large-neural-network" class="slide level2">
<h2>A large neural network</h2>

<img data-src="img/transformer3/Slide2.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018 and 3Blue1Brown, 2024</p></section>
<section id="many-layers-of-attention" class="slide level2">
<h2>Many Layers of Attention</h2>

<img data-src="img/transformer3/Slide3.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018 and 3Blue1Brown, 2024</p></section>
<section id="many-layers-of-attention-1" class="slide level2">
<h2>Many Layers of Attention</h2>

<img data-src="img/transformer3/Slide4.png" class="r-stretch quarto-figure-center"><p class="caption">Source: Alammar, 2018 and 3Blue1Brown, 2024</p></section>
<section id="general-idea-behind-attention" class="slide level2">
<h2>General idea behind Attention</h2>
<ul>
<li><p>In general terms, self-attention works encodes how similar each word is to all the words in the sentence, including itself</p></li>
<li><p>Once the similarities are calculated, they are used to determine how the transformers should update the embeddings of each word</p></li>
</ul>

<img data-src="img/transformer2/Slide2.png" class="r-stretch"></section>
<section id="general-idea-behind-attention-1" class="slide level2">
<h2>General idea behind Attention</h2>
<ul>
<li><p>In general terms, self-attention works encodes how similar each word is to all the words in the sentence, including itself.</p></li>
<li><p>Once the similarities are calculated, they are used to determine how the transformers should update the embeddings of each word</p></li>
</ul>

<img data-src="img/transformer2/Slide3.png" class="r-stretch"></section>
<section id="general-idea-behind-attention-2" class="slide level2">
<h2>General idea behind Attention</h2>
<ul>
<li><p>In general terms, self-attention works encodes how similar each word is to all the words in the sentence, including itself.</p></li>
<li><p>Once the similarities are calculated, they are used to determine how the transformers should update the embeddings of each word</p></li>
</ul>

<img data-src="img/transformer2/Slide5.png" class="r-stretch"></section>
<section id="attention-in-detail" class="slide level2">
<h2>Attention in Detail</h2>

<img data-src="img/transformer3/Slide5.png" class="r-stretch"></section>
<section id="attention-in-detail-1" class="slide level2">
<h2>Attention in Detail</h2>

<img data-src="img/transformer3/Slide6.png" class="r-stretch"></section>
<section id="attention-in-detail-static-embeddings" class="slide level2">
<h2>Attention in Detail: Static Embeddings</h2>

<img data-src="img/transformer3/Slide7.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="attention-in-detail-positional-encoding" class="slide level2">
<h2>Attention in Detail: Positional encoding</h2>

<img data-src="img/transformer3/Slide8.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="attention-in-detail-mechanism" class="slide level2">
<h2>Attention in Detail: Mechanism</h2>

<img data-src="img/transformer3/Slide9.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="attention-in-detail-mechanism-1" class="slide level2">
<h2>Attention in Detail: Mechanism</h2>

<img data-src="img/transformer3/Slide10.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="updating-word-embeddings-in-the-k-dimensional-space" class="slide level2">
<h2>Updating word embeddings in the k-dimensional space</h2>

<img data-src="img/transformer3/Slide11.png" class="r-stretch"></section>
<section id="updating-word-embeddings-in-the-k-dimensional-space-1" class="slide level2">
<h2>Updating word embeddings in the k-dimensional space</h2>

<img data-src="img/transformer3/Slide12.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="updating-word-embeddings-in-the-k-dimensional-space-2" class="slide level2">
<h2>Updating word embeddings in the k-dimensional space</h2>

<img data-src="img/transformer3/Slide13.png" class="r-stretch quarto-figure-center"><p class="caption">Inspiration: 3Blue1Brown, 2024</p></section>
<section id="summary-of-the-gpt-architecture" class="slide level2">
<h2>Summary of the GPT Architecture</h2>
<ul>
<li><p>This was only a very short peek into the architecture of GPT (and to degree also into large language models generally)</p></li>
<li><p>Decoder-only model designed for text generation</p></li>
<li><p>Based on static word-embedding that are updated via postional encoding and many attention layers that can even encode distant relationships between words and sentences</p></li>
<li><p>In principle, just a lot of matrix algebra: We are constantly updating a multitude of vectors that represent words and their meaning in the context of a sentence and the entire text</p></li>
<li><p>Now, we can ask how we can use these models for text classification tasks</p></li>
</ul>
</section></section>
<section>
<section id="using-llms-for-text-classification" class="title-slide slide level1 centered center" data-background-color="steelblue">
<h1>Using LLMs for Text Classification</h1>

</section>
<section id="text-classification-with-large-language-models" class="slide level2">
<h2>Text Classification with Large Language Models</h2>

<img data-src="img/text_analysis_fundamentals/Slide06.png" class="r-stretch"></section>
<section id="zero-shot-one-shot-and-few-shot-classification" class="slide level2">
<h2>Zero-Shot, One-Shot, and Few-Shot Classification</h2>
<ul>
<li><p>Classic machine learning models (last lecture) are typically trained on a specific set of classes, and their performance is evaluated on the same set of classes during testing</p></li>
<li><p>LLMs have the ability to perform a task or make predictions on a set of classes that it has never seen or been explicitly trained on</p></li>
<li><p>In other words, the model can generalize its pre-trained knowledge to new, unseen tasks without training for those tasks</p></li>
<li><p>Depending on the type of <strong>prompt engineering</strong>, i.e., how we describe the task for the LLM, we differentiate three types of classifications:</p>
<ul>
<li>Zero-Shot: No examples are given</li>
<li>One-Shot: One example is given</li>
<li>Few-Shot: More than one example is given</li>
</ul></li>
<li><p>It is not straight-forward which strategy works best for which task.</p></li>
<li><p>At times, zero-shot classification works well as the model is not too tied to the examples</p></li>
</ul>
</section>
<section id="overview-of-classification-without-training" class="slide level2">
<h2>Overview of classification without training</h2>

<img data-src="img/zero-shot.png" class="r-stretch"></section>
<section id="how-to-work-with-llms" class="slide level2">
<h2>How to work with LLMs</h2>
<ul>
<li><p>There are generally three ways in which we can work with LLMs:</p>
<ul>
<li>Assess open source models via the hugging face API (only smaller rates per minute, but still useful)</li>
<li>Assess models such as GPT via their respective API (limited rates per minute, and costs per token)</li>
<li>Download and use models via “ollama” on our own computer (requires some space and can be computationally intensive)</li>
</ul></li>
<li><p>In this course, we are going to “play around” with GPT-3.5 and GPT-4 via the OpenAI API and (if possible) via download small models via ollama</p>
<ul>
<li>The package <code>tidyllm</code> provides a straightforward workflow that intersects with the tidy-style analyses that we already now</li>
<li>API for openAI will be provided by teachers</li>
<li>A tutorial on how to install ollama will be provided</li>
</ul></li>
</ul>
</section>
<section id="a-small-example-in-tidyllm" class="slide level2">
<h2>A small example in <code>tidyllm</code></h2>
<ul>
<li><p>In the package <code>tidyllm</code>, we can create a prompt using the function `llm_message()``</p></li>
<li><p>We then pass this prompt to a chat and specify the model we want to use (this can be a local model, like llama, or a model on a server, like GPT)</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href=""></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb4-2"><a href=""></a><span class="fu">library</span>(tidymodels)</span>
<span id="cb4-3"><a href=""></a><span class="fu">library</span>(glue)</span>
<span id="cb4-4"><a href=""></a><span class="fu">library</span>(tidyllm)</span>
<span id="cb4-5"><a href=""></a></span>
<span id="cb4-6"><a href=""></a><span class="fu">llm_message</span>(<span class="st">"What is this sentence about: To be, or not to be: that is the question."</span>) <span class="sc">|&gt;</span> </span>
<span id="cb4-7"><a href=""></a>  <span class="fu">chat</span>(<span class="fu">ollama</span>(<span class="at">.model =</span> <span class="st">"llama3"</span>, <span class="at">.temperature =</span> <span class="dv">0</span>)) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource output number-lines code-with-copy"><code class="sourceCode"><span id="cb5-1"><a href=""></a>Message History:</span>
<span id="cb5-2"><a href=""></a>system: You are a helpful assistant</span>
<span id="cb5-3"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb5-4"><a href=""></a>user: What is this sentence about: To be, or not to be: that is the question.</span>
<span id="cb5-5"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb5-6"><a href=""></a>assistant: A classic!</span>
<span id="cb5-7"><a href=""></a></span>
<span id="cb5-8"><a href=""></a>This sentence is from the opening of William Shakespeare's play "Hamlet", Act 3, Scene 1. It is a famous soliloquy spoken by Prince Hamlet himself.</span>
<span id="cb5-9"><a href=""></a></span>
<span id="cb5-10"><a href=""></a>The sentence is about the existential crisis and philosophical dilemma that Hamlet is grappling with. He is contemplating whether to take action against his uncle Claudius, who has murdered his father (Hamlet's king) and taken the throne for himself. Hamlet is torn between two options:</span>
<span id="cb5-11"><a href=""></a></span>
<span id="cb5-12"><a href=""></a>1. To be: to exist, to live, to take action, and potentially face the consequences of doing so.</span>
<span id="cb5-13"><a href=""></a>2. Not to be: to not exist, to die, to avoid the troubles and uncertainties of life.</span>
<span id="cb5-14"><a href=""></a></span>
<span id="cb5-15"><a href=""></a>The sentence sets the tone for the rest of the play, which explores themes of mortality, morality, and the human condition.</span>
<span id="cb5-16"><a href=""></a>--------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="zero-shot-classification-prompt-engineering" class="slide level2">
<h2>Zero-Shot Classification: Prompt Engineering</h2>
<ul>
<li><p>In a zero-shot classification framework, we simply provide the task.</p></li>
<li><p>Additionally, we can give potential answer options, which help streamlining the code (otherwise, we get full text answers due to the next-token-prediction logic of LLMs!)</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href=""></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb6-2"><a href=""></a></span>
<span id="cb6-3"><a href=""></a><span class="co"># We create a small test data set</span></span>
<span id="cb6-4"><a href=""></a>test_data <span class="ot">&lt;-</span> science_data <span class="sc">|&gt;</span> </span>
<span id="cb6-5"><a href=""></a>  <span class="fu">sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>)</span>
<span id="cb6-6"><a href=""></a></span>
<span id="cb6-7"><a href=""></a><span class="co"># We create a codebook that includes all texts.</span></span>
<span id="cb6-8"><a href=""></a>codebook <span class="ot">&lt;-</span> <span class="fu">glue</span>(<span class="st">"Identify the discipline based on this abstract: {abstract}</span></span>
<span id="cb6-9"><a href=""></a><span class="st">      </span></span>
<span id="cb6-10"><a href=""></a><span class="st">                  Pick one of the following numerical codes from this list. </span></span>
<span id="cb6-11"><a href=""></a><span class="st">                  Respond only with the code!</span></span>
<span id="cb6-12"><a href=""></a><span class="st">                  </span></span>
<span id="cb6-13"><a href=""></a><span class="st">                  1 = Computer Science</span></span>
<span id="cb6-14"><a href=""></a><span class="st">                  2 = Physics</span></span>
<span id="cb6-15"><a href=""></a><span class="st">                  3 = Statistics"</span>,</span>
<span id="cb6-16"><a href=""></a>                  <span class="at">abstract =</span> test_data<span class="sc">$</span>text)</span>
<span id="cb6-17"><a href=""></a></span>
<span id="cb6-18"><a href=""></a><span class="co"># Create a list of llm-message prompts</span></span>
<span id="cb6-19"><a href=""></a>classification_task <span class="ot">&lt;-</span> <span class="fu">map</span>(codebook, llm_message)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>We create a list that contains already all prompts with all texts</li>
</ul>
</section>
<section id="zero-shot-classification-sequential-prompting" class="slide level2">
<h2>Zero-Shot Classification: Sequential Prompting</h2>
<ul>
<li><p>Next, we create a function that sends the message via the API to the relevant model</p></li>
<li><p>Then, we use the function <code>pmap_dfr()</code> to “map” this function across our list of prompts</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href=""></a><span class="co"># Create a function that to map across the prompts</span></span>
<span id="cb7-2"><a href=""></a>classify_sequential_llama <span class="ot">&lt;-</span> <span class="cf">function</span>(texts, message){</span>
<span id="cb7-3"><a href=""></a>    raw_code <span class="ot">&lt;-</span> message <span class="sc">|&gt;</span></span>
<span id="cb7-4"><a href=""></a>        <span class="fu">chat</span>(<span class="fu">ollama</span>(<span class="at">.model =</span> <span class="st">"llama3"</span>, <span class="at">.temperature =</span> .<span class="dv">0</span>)) <span class="sc">|&gt;</span></span>
<span id="cb7-5"><a href=""></a>        <span class="fu">get_reply</span>() </span>
<span id="cb7-6"><a href=""></a>    <span class="fu">tibble</span>(<span class="at">text =</span> texts, <span class="at">label =</span> raw_code) </span>
<span id="cb7-7"><a href=""></a>}</span>
<span id="cb7-8"><a href=""></a></span>
<span id="cb7-9"><a href=""></a><span class="co"># Run the classification by sequentially prompting LLama3</span></span>
<span id="cb7-10"><a href=""></a>results_llama <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">texts =</span> test_data<span class="sc">$</span>text, </span>
<span id="cb7-11"><a href=""></a>                        <span class="at">message =</span> classification_task) <span class="sc">|&gt;</span> </span>
<span id="cb7-12"><a href=""></a>  <span class="fu">pmap_dfr</span>(classify_sequential_llama, <span class="at">.progress =</span> T) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a result, we get a data set with the relevant code.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href=""></a><span class="fu">head</span>(results_llama)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource output number-lines code-with-copy"><code class="sourceCode"><span id="cb9-1"><a href=""></a># A tibble: 6 × 2</span>
<span id="cb9-2"><a href=""></a>  text                                                                     label</span>
<span id="cb9-3"><a href=""></a>  &lt;chr&gt;                                                                    &lt;chr&gt;</span>
<span id="cb9-4"><a href=""></a>1 "Optimal stopping via reinforced regression In this note we propose a n… 1    </span>
<span id="cb9-5"><a href=""></a>2 "Traces of surfactants can severely limit the drag reduction of superhy… 2    </span>
<span id="cb9-6"><a href=""></a>3 "A Unified Strouhal-Reynolds Number Relationship for Laminar Vortex Str… 2    </span>
<span id="cb9-7"><a href=""></a>4 "Spatio-Temporal Backpropagation for Training High-performance Spiking … 1    </span>
<span id="cb9-8"><a href=""></a>5 "Well quasi-orders and the functional interpretation The purpose of thi… 1    </span>
<span id="cb9-9"><a href=""></a>6 "Concentration of Multilinear Functions of the Ising Model with Applica… 2    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="validation-of-our-classification-with-llama3" class="slide level2">
<h2>Validation of our Classification with LLama3</h2>
<ul>
<li><p>We can use the same functions from the <code>tidymodels</code> package to get our performance scores</p></li>
<li><p>Llama3 actually does amazingly well on this task without any prior training!</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href=""></a><span class="co"># Create predict table</span></span>
<span id="cb10-2"><a href=""></a>predict_llama <span class="ot">&lt;-</span> test_data <span class="sc">|&gt;</span> </span>
<span id="cb10-3"><a href=""></a>  <span class="fu">bind_cols</span>(results_llama <span class="sc">|&gt;</span> <span class="fu">select</span>(<span class="at">predicted =</span> label)) <span class="sc">|&gt;</span> </span>
<span id="cb10-4"><a href=""></a>  <span class="fu">mutate</span>(<span class="at">truth =</span> <span class="fu">factor</span>(label), </span>
<span id="cb10-5"><a href=""></a>          <span class="at">predicted =</span> <span class="fu">factor</span>(<span class="fu">case_when</span>(predicted <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> <span class="st">"computer science"</span>,</span>
<span id="cb10-6"><a href=""></a>                                       predicted <span class="sc">==</span> <span class="dv">2</span> <span class="sc">~</span> <span class="st">"physics"</span>,</span>
<span id="cb10-7"><a href=""></a>                                       predicted <span class="sc">==</span> <span class="dv">3</span> <span class="sc">~</span> <span class="st">"statistics"</span>)))</span>
<span id="cb10-8"><a href=""></a></span>
<span id="cb10-9"><a href=""></a></span>
<span id="cb10-10"><a href=""></a><span class="co"># Define performance scores</span></span>
<span id="cb10-11"><a href=""></a>class_metrics <span class="ot">&lt;-</span> <span class="fu">metric_set</span>(accuracy, precision, recall, f_meas)</span>
<span id="cb10-12"><a href=""></a></span>
<span id="cb10-13"><a href=""></a><span class="co"># Check performance </span></span>
<span id="cb10-14"><a href=""></a>predict_llama <span class="sc">|&gt;</span> </span>
<span id="cb10-15"><a href=""></a>  <span class="fu">class_metrics</span>(<span class="at">truth =</span> truth, <span class="at">estimate =</span> predicted)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource output number-lines code-with-copy"><code class="sourceCode"><span id="cb11-1"><a href=""></a># A tibble: 4 × 3</span>
<span id="cb11-2"><a href=""></a>  .metric   .estimator .estimate</span>
<span id="cb11-3"><a href=""></a>  &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;</span>
<span id="cb11-4"><a href=""></a>1 accuracy  multiclass     0.9  </span>
<span id="cb11-5"><a href=""></a>2 precision macro          0.780</span>
<span id="cb11-6"><a href=""></a>3 recall    macro          0.814</span>
<span id="cb11-7"><a href=""></a>4 f_meas    macro          0.795</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="testing-with-other-models" class="slide level2">
<h2>Testing with other models</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>Our neural network from last week:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href=""></a><span class="co"># Create recipe</span></span>
<span id="cb12-2"><a href=""></a>rec <span class="ot">&lt;-</span> <span class="fu">recipe</span>(label <span class="sc">~</span> text, </span>
<span id="cb12-3"><a href=""></a>              <span class="at">data =</span> science_data) <span class="sc">|&gt;</span>               </span>
<span id="cb12-4"><a href=""></a>  <span class="fu">step_tokenize</span>(text, </span>
<span id="cb12-5"><a href=""></a>                <span class="at">options =</span> <span class="fu">list</span>(<span class="at">strip_punct =</span> T,             </span>
<span id="cb12-6"><a href=""></a>                              <span class="at">strip_numeric =</span> T)) <span class="sc">|&gt;</span>       </span>
<span id="cb12-7"><a href=""></a>  <span class="fu">step_stopwords</span>(text, <span class="at">language =</span> <span class="st">"en"</span>) <span class="sc">|&gt;</span>                        </span>
<span id="cb12-8"><a href=""></a>  <span class="fu">step_tokenfilter</span>(text, <span class="at">min_times =</span> <span class="dv">20</span>, </span>
<span id="cb12-9"><a href=""></a>                   <span class="at">max_tokens =</span> <span class="dv">1000</span>) <span class="sc">|&gt;</span>                </span>
<span id="cb12-10"><a href=""></a>  <span class="fu">step_tf</span>(<span class="fu">all_predictors</span>()) <span class="sc">|&gt;</span> </span>
<span id="cb12-11"><a href=""></a>  <span class="fu">step_normalize</span>(<span class="fu">all_predictors</span>())   </span>
<span id="cb12-12"><a href=""></a></span>
<span id="cb12-13"><a href=""></a><span class="co"># Create workflow</span></span>
<span id="cb12-14"><a href=""></a>ann_workflow <span class="ot">&lt;-</span> <span class="fu">workflow</span>() <span class="sc">|&gt;</span></span>
<span id="cb12-15"><a href=""></a>  <span class="fu">add_recipe</span>(rec_norm) <span class="sc">|&gt;</span>    </span>
<span id="cb12-16"><a href=""></a>  <span class="fu">add_model</span>(nnet_spec)</span>
<span id="cb12-17"><a href=""></a></span>
<span id="cb12-18"><a href=""></a><span class="co"># Create predict table</span></span>
<span id="cb12-19"><a href=""></a>predict_nn <span class="ot">&lt;-</span> <span class="fu">predict</span>(m_ann, test_data) <span class="sc">|&gt;</span> </span>
<span id="cb12-20"><a href=""></a>   <span class="fu">bind_cols</span>(test_data) <span class="sc">|&gt;</span> </span>
<span id="cb12-21"><a href=""></a>   <span class="fu">mutate</span>(<span class="at">truth =</span> <span class="fu">factor</span>(label),</span>
<span id="cb12-22"><a href=""></a>          <span class="at">predicted =</span> .pred_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:48%;">
<p>The same classification with GPT-4:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href=""></a><span class="co"># Create a function that to map across the prompts</span></span>
<span id="cb13-2"><a href=""></a>classify_sequential_gpt4 <span class="ot">&lt;-</span> <span class="cf">function</span>(texts,message){</span>
<span id="cb13-3"><a href=""></a>    raw_code <span class="ot">&lt;-</span> message <span class="sc">|&gt;</span></span>
<span id="cb13-4"><a href=""></a>        <span class="fu">chat</span>(<span class="fu">openai</span>(<span class="at">.model =</span> <span class="st">"gpt-4"</span>, </span>
<span id="cb13-5"><a href=""></a>                    <span class="at">.temperature =</span> .<span class="dv">0</span>)) <span class="sc">|&gt;</span></span>
<span id="cb13-6"><a href=""></a>        <span class="fu">get_reply</span>() </span>
<span id="cb13-7"><a href=""></a>    <span class="fu">tibble</span>(<span class="at">text =</span> texts, <span class="at">label =</span> raw_code) </span>
<span id="cb13-8"><a href=""></a>}</span>
<span id="cb13-9"><a href=""></a></span>
<span id="cb13-10"><a href=""></a><span class="co"># Run commands</span></span>
<span id="cb13-11"><a href=""></a>results_gpt <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">texts =</span> test_data<span class="sc">$</span>text, </span>
<span id="cb13-12"><a href=""></a>                      <span class="at">message =</span> classification_task) <span class="sc">|&gt;</span> </span>
<span id="cb13-13"><a href=""></a>  <span class="fu">pmap_dfr</span>(classify_sequential_gpt4, <span class="at">.progress =</span> T) </span>
<span id="cb13-14"><a href=""></a></span>
<span id="cb13-15"><a href=""></a><span class="co"># Create predict table</span></span>
<span id="cb13-16"><a href=""></a>predict_gpt <span class="ot">&lt;-</span> test_data <span class="sc">|&gt;</span> </span>
<span id="cb13-17"><a href=""></a>  <span class="fu">bind_cols</span>(results_gpt <span class="sc">|&gt;</span> </span>
<span id="cb13-18"><a href=""></a>            <span class="fu">select</span>(<span class="at">predicted =</span> label)) <span class="sc">|&gt;</span> </span>
<span id="cb13-19"><a href=""></a>  <span class="fu">mutate</span>(<span class="at">truth =</span> <span class="fu">factor</span>(label), </span>
<span id="cb13-20"><a href=""></a>          <span class="at">predicted =</span> <span class="fu">parse_number</span>(predicted),</span>
<span id="cb13-21"><a href=""></a>          <span class="at">predicted =</span> <span class="fu">factor</span>(<span class="fu">case_when</span>(predicted <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> <span class="st">"computer science"</span>,</span>
<span id="cb13-22"><a href=""></a>                                       predicted<span class="sc">==</span> <span class="dv">2</span> <span class="sc">~</span> <span class="st">"physics"</span>,</span>
<span id="cb13-23"><a href=""></a>                                       predicted <span class="sc">==</span> <span class="dv">3</span> <span class="sc">~</span> <span class="st">"statistics"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="comparison" class="slide level2">
<h2>Comparison</h2>
<p>As we can see, both LLMs perform almost as good as out neural network, GPT-4 does even better, despite not having been trained on any of the data!</p>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href=""></a><span class="fu">bind_rows</span>(</span>
<span id="cb14-2"><a href=""></a>   predict_llama <span class="sc">|&gt;</span> </span>
<span id="cb14-3"><a href=""></a>   <span class="fu">class_metrics</span>(<span class="at">truth =</span> truth, </span>
<span id="cb14-4"><a href=""></a>                 <span class="at">estimate =</span> predicted) <span class="sc">|&gt;</span> </span>
<span id="cb14-5"><a href=""></a>   <span class="fu">mutate</span>(<span class="at">model =</span> <span class="st">"LLM: llama3"</span>),</span>
<span id="cb14-6"><a href=""></a>   predict_nn <span class="sc">|&gt;</span> </span>
<span id="cb14-7"><a href=""></a>   <span class="fu">class_metrics</span>(<span class="at">truth =</span> truth, </span>
<span id="cb14-8"><a href=""></a>                 <span class="at">estimate =</span> predicted) <span class="sc">|&gt;</span> </span>
<span id="cb14-9"><a href=""></a>   <span class="fu">mutate</span>(<span class="at">model =</span> <span class="st">"Neural Network"</span>),</span>
<span id="cb14-10"><a href=""></a>   predict_gpt <span class="sc">|&gt;</span> </span>
<span id="cb14-11"><a href=""></a>   <span class="fu">class_metrics</span>(<span class="at">truth =</span> truth, </span>
<span id="cb14-12"><a href=""></a>                 <span class="at">estimate =</span> predicted) <span class="sc">|&gt;</span> </span>
<span id="cb14-13"><a href=""></a>   <span class="fu">mutate</span>(<span class="at">model =</span> <span class="st">"LLM: gpt-4"</span>)</span>
<span id="cb14-14"><a href=""></a>) <span class="sc">|&gt;</span> </span>
<span id="cb14-15"><a href=""></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> .metric, <span class="at">y =</span> .estimate, </span>
<span id="cb14-16"><a href=""></a>             <span class="at">fill =</span> model)) <span class="sc">+</span></span>
<span id="cb14-17"><a href=""></a>  <span class="fu">geom_col</span>(<span class="at">position =</span> <span class="fu">position_dodge</span>(), <span class="at">alpha =</span> .<span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb14-18"><a href=""></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">round</span>(.estimate, <span class="dv">3</span>)), </span>
<span id="cb14-19"><a href=""></a>            <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="at">width =</span> <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb14-20"><a href=""></a>  <span class="fu">ylim</span>(<span class="dv">0</span>, <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb14-21"><a href=""></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb14-22"><a href=""></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">"Dark2"</span>) <span class="sc">+</span></span>
<span id="cb14-23"><a href=""></a>  <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">18</span>) <span class="sc">+</span></span>
<span id="cb14-24"><a href=""></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">"Performance Score"</span>, <span class="at">x =</span> <span class="st">""</span>, <span class="at">fill =</span> <span class="st">""</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output-display">
<div>
<figure>
<p><img data-src="04_largelanguagemodels_2024_files/figure-revealjs/unnamed-chunk-13-1.png" width="864"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="examples-in-the-literature" class="slide level2">
<h2>Examples in the literature</h2>
<div class="columns">
<div class="column" style="width:80%;">
<ul>
<li><p>Baluff et al.&nbsp;(2023) investigated a recent case of media capture, a mutually corrupting relationship between political actors and media organizations.</p></li>
<li><p>This case involves former Austrian chancellor who allegedly colluded with a tabloid newspaper to receive better news coverage in exchange for increased ad placements by government institutions.</p></li>
<li><p>They implemented automated content analysis (using BERT) of political news articles from six prominent Austrian news outlets spanning 2012 to 2021 (n = 188,203) and adopted a difference-in-differences approach to scrutinize political actors’ visibility and favorability in news coverage for patterns indicative of the alleged serious breach of professional political and journalistic norms.</p></li>
</ul>
</div><div class="column" style="width:20%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/8/82/Sebastian_Kurz_%282018-02-28%29_%28cropped%29.jpg"></p>
</div>
</div>
</section>
<section id="methods" class="slide level2">
<h2>Methods</h2>
<ul>
<li><p>Used a German-language GottBERT model (Scheible et al., 2020) that they further fine-tuned for the task using publicly available data from the AUTNES Manual Content Analysis of the Media Coverage 2017 and 2019 (Galyga et al., 2022; Litvyak et al., 2022c)</p></li>
<li><p>Comparatively difficult task, but were able to reach a satisfactory F1-Score of 0.77 (precision = 0.77, recall = 0.77).</p></li>
</ul>

<img data-src="img/kurz1.png" class="r-stretch"></section>
<section id="findings" class="slide level2">
<h2>Findings</h2>
<ul>
<li><p>The findings indicate a substantial increase in the news coverage of the former Austrian chancellor within the news outlet that is alleged to have received bribes.</p></li>
<li><p>In contrast, several other political actors did not experience similar shifts in visibility nor are similar patterns identified in other media outlets.</p></li>
</ul>

<img data-src="img/kurz2.png" class="r-stretch"></section></section>
<section>
<section id="summary-and-conclusion" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Summary and conclusion</h1>

</section>
<section id="a-look-back-at-the-chronology-of-nlp-2" class="slide level2">
<h2>A Look Back at the Chronology of NLP</h2>

<img data-src="img/timeline/Slide1b.png" class="r-stretch"></section>
<section id="a-look-back-at-the-chronology-of-nlp-3" class="slide level2">
<h2>A Look Back at the Chronology of NLP</h2>

<img data-src="img/timeline/Slide2b.png" class="r-stretch"></section>
<section id="a-look-back-at-the-chronology-of-nlp-4" class="slide level2">
<h2>A Look Back at the Chronology of NLP</h2>

<img data-src="img/timeline/Slide3b.png" class="r-stretch"></section>
<section id="explosion-in-model-size" class="slide level2">
<h2>Explosion in model size?</h2>

<img data-src="04_largelanguagemodels_2024_files/figure-revealjs/unnamed-chunk-14-1.png" width="768" class="r-stretch"></section>
<section id="performance-depends-strongly-on-scale" class="slide level2">
<h2>Performance depends strongly on scale</h2>

<img data-src="img/scaling_law.png" class="r-stretch quarto-figure-center"><p class="caption">Kaplan et al, 2020</p><ul>
<li><p>Performance depends strongly on scale, weakly on model shape</p></li>
<li><p>Model performance is mostly related toscale, which consists of three factors:</p>
<ul>
<li><p>the number of model parameters (excluding embeddings)</p></li>
<li><p>the size of the dataset</p></li>
<li><p>the amount of compute used for training</p></li>
</ul></li>
<li><p>Within reasonable limits, performance depends very weakly on other architectural hyperparameters such as depth vs.&nbsp;width</p></li>
</ul>
</section>
<section id="environmental-impact" class="slide level2">
<h2>Environmental Impact</h2>

<img data-src="https://hai.stanford.edu/sites/default/files/inline-images/AIIndex_2023_StateofAI_Blog_4.jpg" class="r-stretch"></section>
<section id="ethical-considerations" class="slide level2">
<h2>Ethical considerations</h2>
<ul>
<li><p>Training large language models requires significant computational resources, contributing to a substantial carbon footprint.</p></li>
<li><p>LLMs can inherit and perpetuate biases present in their training data, which can result in the generation of biased or unfair content, reflecting and potentially amplifying societal biases and stereotypes.</p></li>
<li><p>Developers and users must be aware of the potential for bias and take steps to mitigate it during model training and deployment.</p></li>
<li><p>The fact that some LLMs are developed, trained, and employed behind closed doors causes yet another ethical dilemma in using them!</p></li>
</ul>
</section>
<section id="reminder-guidelines" class="slide level2">
<h2>Reminder: Guidelines</h2>

<img data-src="img/ethical_guidelines.png" class="r-stretch"></section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<ul>
<li><p>Advancement in NLP and AI are fast-paced; difficult to keep up</p></li>
<li><p>LLMs promise immense potential for communication research</p></li>
<li><p>Yet, large language models can contain biases or even hallucinate!</p>
<ul>
<li>Validation, validation, validation!</li>
</ul></li>
<li><p>Also: We see already that more and more content online is AI-based. What does it mean if in the future, LLMs are trained on their own content?</p></li>
</ul>
<div class="quarto-layout-panel" data-layout="[32, 32, 32]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://s3.amazonaws.com/static.rogerebert.com/uploads/review/primary_image/reviews/great-movie-ai-artificial-intelligence-2001/EB20110707REVIEWS08110709988AR.jpg"></p>
<figcaption>A.I (Steven Spielberg)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://mir-s3-cdn-cf.behance.net/project_modules/max_1200/ea5cab25137457.563425fa7c93f.png"></p>
<figcaption>Her (Spike Jones)</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://www.azcentral.com/gcdn/-mm-/8595b75e3791fcb4aef999b94940f74898e5ea59/c=3-0-1591-897/local/-/media/2015/04/22/Phoenix/Phoenix/635653113360425789-alicia-vikander.jpg"></p>
<figcaption>Ex Machina (Alex Garland)</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="and-it-doesnt-stop-here" class="slide level2">
<h2>And it doesn’t stop here…</h2>
<ul>
<li>Large language models like “llava” can also identify and describe images…</li>
</ul>
<div class="columns">
<div class="column" style="width:28%;">
<p><img data-src="img/england.jpeg"></p>
</div><div class="column" style="width:70%;">
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href=""></a><span class="fu">llm_message</span>(<span class="st">"Describe this picture? Can you guess where it was made?"</span>,</span>
<span id="cb15-2"><a href=""></a>                                 <span class="at">.imagefile =</span> <span class="st">"img/england.jpeg"</span>) <span class="sc">|&gt;</span></span>
<span id="cb15-3"><a href=""></a>  <span class="fu">chat</span>(<span class="fu">ollama</span>(<span class="at">.model =</span> <span class="st">"llava"</span>, <span class="at">.temperature =</span> <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource output number-lines code-with-copy"><code class="sourceCode"><span id="cb16-1"><a href=""></a>Message History:</span>
<span id="cb16-2"><a href=""></a>system: You are a helpful assistant</span>
<span id="cb16-3"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb16-4"><a href=""></a>user: Describe this picture? Can you guess where it was made?</span>
<span id="cb16-5"><a href=""></a> -&gt; Attached Media Files:  england.jpeg </span>
<span id="cb16-6"><a href=""></a>--------------------------------------------------------------</span>
<span id="cb16-7"><a href=""></a>assistant:  The image shows a scene from London, England. There is a blue taxi cab in the foreground with the words "Look Right" on its side, indicating that drivers should look to their right when approaching an intersection, which is standard practice in the United Kingdom due to driving on the left side of the road. In the background, there's a famous landmark known as Big Ben, which is actually the nickname for the Great Bell housed within the clock tower at the north end of the Palace of Westminster, and the Elizabeth Tower, which houses Big Ben, is visible in the distance. The sky is overcast, suggesting it might be a cool or cloudy day. There are people walking on the sidewalks, and the overall atmosphere suggests a typical day in London with some tourists around. </span>
<span id="cb16-8"><a href=""></a>--------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="thank-you-for-your-attention" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Thank you for your attention!</h1>

</section>
<section id="required-reading" class="slide level2">
<h2>Required Reading</h2>
<p><br><br></p>
<p>Kroon, A., Welbers, K., Trilling, D., &amp; van Atteveldt, W. (2023). Advancing Automated Content Analysis for a New Era of Media Effects Research: The Key Role of Transfer Learning. Communication Methods and Measures, 1-21</p>
<p><br></p>
<p><em>(available on Canvas)</em></p>
</section>
<section id="reference" class="slide level2 smaller">
<h2>Reference</h2>
<ul>
<li><p>Alammar, J. (2018). The illustrated Transformer. Retrieved from: https://jalammar.github.io/illustrated-transformer/</p></li>
<li><p>Andrich, A., Bachl, M., &amp; Domahidi, E. (2023). Goodbye, Gender Stereotypes? Trait Attributions to Politicians in 11 Years of News Coverage. Journalism &amp; Mass Communication Quarterly, 100(3), 473-497. https://doi-org.vu-nl.idm.oclc.org/10.1177/10776990221142248</p></li>
<li><p>Balluff, P., Eberl, J., Oberhänsli, S. J., Bernhard, J., Boomgaarden, H. G., Fahr, A., &amp; Huber, M. (2023, September 15). The Austrian Political Advertisement Scandal: Searching for Patterns of “Journalism for Sale”. https://doi.org/10.31235/osf.io/m5qx4</p></li>
<li><p>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p></li>
<li><p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., … &amp; Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</p></li>
<li><p>Kroon, A., Welbers, K., Trilling, D., &amp; van Atteveldt, W. (2023). Advancing Automated Content Analysis for a New Era of Media Effects Research: The Key Role of Transfer Learning. Communication Methods and Measures, 1-21</p></li>
<li><p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., … &amp; Lample, G. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p></li>
<li><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p></li>
</ul>
</section>
<section id="example-exam-question-multiple-choice" class="slide level2">
<h2>Example Exam Question (Multiple Choice)</h2>
<p>How are word embeddings learned?</p>
<p><br></p>
<p>A. By assigning random numerical values to each word</p>
<p>B. By analyzing the pronunciation of words</p>
<p>C. By scanning the context of each word in a large corpus of documents</p>
<p>D. By counting the frequency of words in a given text</p>
</section>
<section id="example-exam-question-multiple-choice-1" class="slide level2">
<h2>Example Exam Question (Multiple Choice)</h2>
<p>How are word embeddings learned?</p>
<p><br></p>
<p>A. By assigning random numerical values to each word</p>
<p>B. By analyzing the pronunciation of words</p>
<p><strong>C. By scanning the context of each word in a large corpus of documents</strong></p>
<p>D. By counting the frequency of words in a given text</p>
</section>
<section id="example-exam-question-open-format" class="slide level2">
<h2>Example Exam Question (Open Format)</h2>
<p>What does zero-shot learning refer to in the context of large language models?</p>
<div class="fragment">
<p><br></p>
<p>In the context of large language models, zero-shot learning refers to the ability of a model to perform a task or make predictions on a set of classes or concepts that it has never seen or been explicitly trained on. Essentially, the model can generalize its knowledge to new, unseen tasks without specific examples or training data for those tasks.</p>
<p>In traditional machine learning, models are typically trained on a specific set of classes, and their performance is evaluated on the same set of classes during testing. Zero-shot learning extends this capability by allowing the model to handle tasks or categories that were not part of its training set.</p>
<p>In the case of large language models like GPT-3, which is trained on a diverse range of internet text, zero-shot learning means the model can understand and generate relevant responses for queries or prompts related to concepts it hasn’t been explicitly trained on. This is achieved through the model’s ability to capture and generalize information from the vast and varied data it has been exposed to during training.</p>
<div class="quarto-auto-generated-content">
<p><img src="img/logo.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>Computational Analysis of Digital Communication</p>
</div>
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="04_largelanguagemodels_2024_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>