<!DOCTYPE html>
<html lang="en"><head>
<script src="04_unsupervisedlearning_files/libs/clipboard/clipboard.min.js"></script>
<script src="04_unsupervisedlearning_files/libs/quarto-html/tabby.min.js"></script>
<script src="04_unsupervisedlearning_files/libs/quarto-html/popper.min.js"></script>
<script src="04_unsupervisedlearning_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="04_unsupervisedlearning_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04_unsupervisedlearning_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="04_unsupervisedlearning_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.251">

  <meta name="author" content="Dr.&nbsp;Philipp K. Masur">
  <title>Computational Analysis of Digital Communication</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="04_unsupervisedlearning_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="04_unsupervisedlearning_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="04_unsupervisedlearning_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="04_unsupervisedlearning_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="04_unsupervisedlearning_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="04_unsupervisedlearning_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="04_unsupervisedlearning_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="https://storage.googleapis.com/exenzo-jobboard/bb/public/thumb/logo-jpg-57.jpg" data-background-position="top right" data-background-size="auto" class="center">
  <h1 class="title">Computational Analysis of Digital Communication</h1>
  <p class="subtitle">Week 4: Unsupervised Machine Learning - Topic Modeling</p>
  <p class="author">Dr.&nbsp;Philipp K. Masur</p>
</section>

<section class="slide level2">


<img data-src="img/machinelearning.png" class="r-stretch"></section>
<section class="slide level2">

<div class="column" style="width:45%;">
<h3 id="supervised-learning-last-lecture">Supervised learning (last lecture)</h3>
<ul>
<li><p>Algorithms build a model based on sample data, known as “training data”, in order to make predictions or decisions without being explicitly programmed to do so</p></li>
<li><p>Combines the scalability of automatic coding with the validity of manual coding (requires pre-labeled data to train algorithm)</p></li>
<li><p>Examples:</p>
<ul>
<li>Supervised text classification, such as extending manual coding to large text corpora, sentiment analysis…</li>
<li>Pattern recognition: e.g., face recognition, spam filter,…</li>
</ul></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<h3 id="unsupervised-learning-this-lecture">Unsupervised learning (this lecture)</h3>
<ul>
<li><p>Algorithm detects clusters, patterns, or associations in data that has not been labeled previously, but researcher needs to interpret results</p></li>
<li><p>Very helpful to make sense of new data (similar to cluster analysis or exploratory factor analysis)</p></li>
<li><p>Examples:</p>
<ul>
<li><strong>Topic modeling: Extracting topics from unlabeled (text) data</strong></li>
<li>Customer segmentation: Better understanding different customer groups around which to build marketing or other business strategies</li>
</ul></li>
</ul>
</div>
</section>
<section>
<section id="computers-can-detect-topics" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Computers can detect “topics”?</h1>

</section>
<section id="what-are-we-trying-to-achieve" class="slide level2">
<h2>What are we trying to achieve?</h2>

<img data-src="img/topic_slides/Slide1.png" class="r-stretch"></section>
<section id="what-topics-can-be-extracted-from-the-documents" class="slide level2">
<h2>What topics can be extracted from the documents?</h2>

<img data-src="img/topic_slides/Slide2.png" class="r-stretch"></section>
<section id="easy-for-humans" class="slide level2">
<h2>Easy for humans…</h2>

<img data-src="img/topic_slides/Slide3.png" class="r-stretch"></section>
<section id="but-the-computer-can-only-rely-on-words" class="slide level2">
<h2>But the computer can only rely on words</h2>

<img data-src="img/topic_slides/Slide5.png" class="r-stretch"></section>
<section id="but-each-word-may-belong-to-a-certain-topic" class="slide level2">
<h2>But each word may belong to a certain topic!</h2>

<img data-src="img/topic_slides/Slide4.png" class="r-stretch"></section>
<section id="goals-of-topic-modeling" class="slide level2">
<h2>Goals of topic modeling</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>Topic modeling is a method for unsupervised classification of documents</p></li>
<li><p>It is somewhat similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for</p></li>
<li><p>The goal is to find a set of topics consisting of clusters of words that co-occur in these documents according to certain patterns</p></li>
<li><p>However, the researchers must interpret the results of a topic model and there is usually more than one solution…</p></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<p><img data-src="img/blei_lda.png"></p>
<p style="font-size:0.75em;">
</p><p><em>Blei, Ng, &amp; Jordan, 2003</em></p>
<p></p>
</div>
</section>
<section id="why-should-we-care" class="slide level2">
<h2>Why should we care?</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>Topic models were originally developed as a text-mining tool and as such has wide applications in research that is based on understanding texts</p></li>
<li><p>But topic models have applications in other fields such as bioinformatics and computer vision</p>
<ul>
<li>can be used to detect structures in data such as genetic information</li>
<li>can also detect similaries in data based on images or networks</li>
</ul></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<ul>
<li><p>More importantly, they are also used in various practical fields:</p>
<ul>
<li>In news or content recommenders, which aim to suggest content based on similar topics</li>
<li>Used to power conversational agents (e.g., chatbots)</li>
<li>Spam filter (again!)</li>
<li>And many more…</li>
</ul></li>
</ul>
</div>
</section>
<section id="content-of-the-lecture" class="slide level2">
<h2>Content of the lecture</h2>
<ol type="1">
<li><p>What is topic modeling?</p></li>
<li><p>Topic Modeling as Dimensionality Reduction</p></li>
<li><p>Latent Dirichlet Allocation - LDA topic modeling</p>
<ul>
<li>Basic Principles</li>
<li>Dirichlet Distribution</li>
<li>Gibbs Sampling</li>
<li>How to determine the right number of topics</li>
</ul></li>
<li><p>(More) Examples from the literature</p></li>
<li><p>Conclusion and outlook</p></li>
</ol>
</section></section>
<section>
<section id="what-is-topic-modeling" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>What is topic modeling?</h1>

</section>
<section id="lets-start-with-an-example" class="slide level2">
<h2>Let’s start with an example</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>Jacobi, Welbers &amp; Van Atteveld (2016) analyzed the coverage of <em>nuclear technology</em> from 1945 to 2013 in the New York Times</p></li>
<li><p>Overall, they analyzed 51,528 news stories (headline and lead): Way too much for human coding!</p></li>
<li><p>Used Latent Dirichlet Allocation (LDA) topic modeling to extract topics</p></li>
<li><p>They then analyzed the occurrence of topics over time</p></li>
<li><p>It is a nice example of what topic modelling can and can’t do</p></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Nuclear_Power_Plant_Cattenom.jpg"></p>
</div>
</section>
<section id="assumption-of-the-model" class="slide level2">
<h2>Assumption of the model</h2>

<img data-src="img/jacobi_fig1.png" class="r-stretch"></section>
<section id="characteristics-of-a-topic-model" class="slide level2">
<h2>Characteristics of a topic model</h2>
<p>This example highlights a number of interesting points about LDA topic modelling</p>
<ol type="1">
<li><p>The document is split between two main topics, <em>Cold War</em> and <em>Nuclear Accidents</em>: In a coding scheme forced to have a single topic per document, it would be very difficult to choose the dominant topic for this article.</p></li>
<li><p>Not all words are included in the analysis: Most are not used because they are non-substantive words such as determiners or prepositions (“the” or “it”), which the authors excluded, or because they are too rare (“Elbe”, “perish”) or too common (“have” but in this context also “nuclear”, since that was used to select the articles).</p></li>
<li><p>No a priori coding scheme was used by the computer, so the topics in this document were found completely automatically.</p></li>
</ol>
</section>
<section id="so-what-does-a-result-of-a-topic-model-look-like" class="slide level2">
<h2>So what does a result of a Topic model look like?</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>Typical table resulting from a topic model</p></li>
<li><p>Contains most representative words per topic (in this case 10 topics)</p></li>
<li><p>The authors then interpreted the outcome and labeled each topic accordingly</p></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<p><img data-src="img/jacobi_table1.png"></p>
</div>
</section>
<section id="using-topics-for-substantive-analyses" class="slide level2">
<h2>Using topics for substantive analyses</h2>

<img data-src="img/Jacobi_topicmodelling.png" class="r-stretch"></section></section>
<section>
<section id="topic-modeling-as-dimensionality-reduction" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Topic Modeling as Dimensionality Reduction</h1>

</section>
<section id="using-factor-or-cluster-analysis-to-extract-topics" class="slide level2">
<h2>Using factor or cluster analysis to extract topics?</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>Problem: Texts are unstructured data and it is thus difficult to do standard statistical analysis with them</p></li>
<li><p>Solution: Convert the texts into a document-term matrix, where documents represent cases and words are variables/characteristics of these cases</p></li>
<li><p>Now we can use standard cluster/factor analysis techniques to reduce these dimensions?</p></li>
</ul>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<p><img data-src="https://www.researchgate.net/publication/338434397/figure/fig1/AS:844756024311809@1578416939705/The-results-obtained-from-Confirmatory-Factor-Analysis-CAF-for-the-APQ-P-First.png"></p>
</div>
</section>
<section id="dimensionality-reduction-using-pca" class="slide level2">
<h2>Dimensionality reduction using PCA</h2>
<div class="cell" data-r.options="{&quot;width&quot;:80}">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Loadin packages</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co"># Text examples</span></span>
<span id="cb1-6"><a href="#cb1-6"></a>texts <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"LDA is a topic model algorithm"</span>,</span>
<span id="cb1-7"><a href="#cb1-7"></a>           <span class="st">"Topic modeling is an interesting example of LDA"</span>,</span>
<span id="cb1-8"><a href="#cb1-8"></a>           <span class="st">"News algorithms are important in journalism"</span>,</span>
<span id="cb1-9"><a href="#cb1-9"></a>           <span class="st">"Journalism is important for society"</span>,</span>
<span id="cb1-10"><a href="#cb1-10"></a>           <span class="st">"Journalists use algorithms in their work"</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Text preprocessing</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>(dtm_factor <span class="ot">&lt;-</span> texts <span class="sc">%&gt;%</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>  <span class="fu">tokens_remove</span>(<span class="fu">stopwords</span>(<span class="st">"en"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>  dfm <span class="sc">%&gt;%</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>  as_tibble <span class="sc">%&gt;%</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>  <span class="fu">select</span>(<span class="sc">-</span>doc_id))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment" data-r.options="{&quot;width&quot;:80}">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 15
    lda topic model algorithm modeling interesting example  news algorithms
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;
1     1     1     1         1        0           0       0     0          0
2     1     1     0         0        1           1       1     0          0
3     0     0     0         0        0           0       0     1          1
4     0     0     0         0        0           0       0     0          0
5     0     0     0         0        0           0       0     0          1
# … with 6 more variables: important &lt;dbl&gt;, journalism &lt;dbl&gt;, society &lt;dbl&gt;,
#   journalists &lt;dbl&gt;, use &lt;dbl&gt;, work &lt;dbl&gt;</code></pre>
</div>
</div>
</section>
<section id="dimensionality-reduction-using-pca-1" class="slide level2">
<h2>Dimensionality reduction using PCA</h2>
<div class="column" style="width:45%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="fu">prcomp</span>(dtm_factor, <span class="at">rank. =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Standard deviations (1, .., p=5):
[1] 1.254104e+00 9.807140e-01 7.845689e-01 5.915016e-01 2.686400e-16

Rotation (n x k) = (15 x 2):
                   PC1         PC2
lda         -0.4297844  0.05075217
topic       -0.4297844  0.05075217
model       -0.1865506  0.01595800
algorithm   -0.1865506  0.01595800
modeling    -0.2432338  0.03479417
interesting -0.2432338  0.03479417
example     -0.2432338  0.03479417
news         0.1888600  0.13729363
algorithms   0.2970694 -0.28984625
important    0.3215750  0.37638771
journalism   0.3215750  0.37638771
society      0.1327150  0.23909408
journalists  0.1082094 -0.42713988
use          0.1082094 -0.42713988
work         0.1082094 -0.42713988</code></pre>
</div>
</div>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:45%;">
<ul>
<li><p>Factor/Principal component analysis can reduce the number of columns (= reduce the dimensionality of the dataset)</p></li>
<li><p>It assumes the manifest words are determined by (fewer) underlying latent factors</p></li>
<li><p>In this simple example, it works to a certain degree, but we also see problems</p></li>
</ul>
</div>
</section>
<section id="latent-semantic-analysis" class="slide level2">
<h2>Latent Semantic Analysis</h2>
<div class="column" style="width:60%;">
<ul>
<li><p>Latent Semantic Analysis assumes that words that are close in meaning will occur in similar pieces of text</p></li>
<li><p><em>Singular value decomposition</em> (SVD) is used to reduce the number of rows in a document-feature matrix while preserving the similarity structure among columns</p></li>
<li><p>Overall, similar to factor analysis, but developed in the field of natural language processing</p></li>
<li><p>Found to mimic (some) human generalizations and even errors</p></li>
<li><p>Also problematic to interpret:</p>
<ul>
<li>Negative values</li>
<li>Not robust to ambiguous terms and antonyms</li>
<li>No theoretical interpretation of mechanism</li>
</ul></li>
</ul>
<p style="font-size:0.65em;">
</p><p><em>e.g.&nbsp;Deerwester et al., 1990</em></p>
<p></p>
</div>
<div class="column" style="width:4%;">

</div>
<div class="column" style="width:32%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/e/e9/Singular_value_decomposition.gif"></p>
</div>
</section>
<section id="preliminary-conclusion" class="slide level2">
<h2>Preliminary conclusion</h2>
<ul>
<li><p>Clustering, factor analysis, principal component analyis and its extension in the form of latent semantic analysis can all be used to reduce the dimensionality of a document-term matrix</p></li>
<li><p>The resulting dimension may be a basis for extracting topics (after all it is words with loadings onto different factors = topics)</p></li>
<li><p>Yet, there are several problems</p>
<ul>
<li>Negative values difficult to interpret</li>
<li>The underlying model is hard to intepret</li>
</ul></li>
<li><p>These models are perhaps useful for understand which words load onto a topic, but they are less good at representing the distribution of topics in documents</p></li>
</ul>
</section></section>
<section>
<section id="latent-dirichlet-allocation---lda-topic-modeling" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Latent Dirichlet Allocation - LDA Topic Modeling</h1>

</section>
<section id="latent-dirichlet-allocation---lda-topic-modeling-1" class="slide level2">
<h2>Latent Dirichlet Allocation - LDA topic modeling</h2>
<ul>
<li><p>Evolution of Latent Semantic Analysis</p></li>
<li><p>Is Based on generative statistical model that aligns with how articles or documents are written</p>
<ul>
<li>It ‘assumes’ an author who…</li>
<li>chooses a mix of topics to write about.</li>
<li>For each word, s/he select one of the topics…</li>
<li>and then select a word from this topic.</li>
</ul></li>
<li><p>This basically results in a mixture model:</p>
<ul>
<li>Words can be in multiple topics (→ deals with ambiguity)</li>
<li>Documents in multiple topics (→ deals with mixed content)</li>
<li>But skewed towards a couple of topics, depending on <span class="math inline">\(\alpha\)</span> hyperparameter (more on this later)</li>
</ul></li>
</ul>
</section>
<section id="example-for-the-generative-model" class="slide level2">
<h2>Example for the generative model</h2>
<p>Let’s assume that you are a journalist writing a 500 word news item.</p>
<p><br></p>
<div class="fragment">
<ol type="1">
<li>You would choose one or more topics to write about, for example 70% healthcare and 30% economy.</li>
</ol>
</div>
<div class="fragment">
<ol start="2" type="1">
<li>For each word in the item, you randomly pick one of these topics based on their respective weight.</li>
</ol>
</div>
<div class="fragment">
<ol start="3" type="1">
<li>Finally, you pick a random word from the words associated with that topic, where again each word has a certain probability for that topic. For example, “hospital” might have a high probability for healthcare while “effectiveness” might have a lower probability but could still occur</li>
</ol>
<p><br></p>
<p style="font-size:0.75em;">
</p><p><em>Van Atteveldt, Trilling &amp; Calderon, 2021</em></p>
<p></p>
</div>
</section>
<section id="intuitions-behind-lda-topic-modelling" class="slide level2">
<h2>Intuitions behind LDA topic modelling</h2>

<img data-src="img/blei_lda.png" class="r-stretch"><p style="font-size:0.75em;">
</p><p><em>Blei et al., 2003</em></p>
<p></p>
</section>
<section id="sampling-from-the-dirichlet-distribution" class="slide level2">
<h2>Sampling from the Dirichlet Distribution</h2>
<ul>
<li><p>In topic modeling, we assume the following:</p>
<ul>
<li>Every topic and document is a probability distribution (over words / topics resp.)</li>
<li>We wonder: How likely is word <span class="math inline">\(w\)</span> in topic <span class="math inline">\(z\)</span>, or topic <span class="math inline">\(z\)</span> in document <span class="math inline">\(d\)</span>?</li>
<li>These distributions are themselves randomly drawn from the “dirichlet distribution” which yields multinomial distributions</li>
</ul></li>
</ul>
<div class="column" style="width:65%;">
<ul>
<li><p>So, what is a “Dirichlet distribution”?</p></li>
<li><p>Named after Peter Gustav Lejeune Dirichlet</p></li>
<li><p>It is a family of continuous multivariate probability distributions parameterized by a vector <span class="math inline">\(\alpha\)</span> of positive reals</p></li>
<li><p>Can also be seen as a multivariate generalization of the beta distribution</p></li>
</ul>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:25%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/3/32/Peter_Gustav_Lejeune_Dirichlet.jpg"></p>
</div>
</section>
<section id="explaining-a-dirichlet-distribution" class="slide level2">
<h2>Explaining a Dirichlet Distribution…</h2>
<ul>
<li><p>You walk into the room for a get-to-gether (e.g., at a conference) and want to sit somewhere</p></li>
<li><p>You are afraid to sit alone, so prefer a table with people</p>
<ul>
<li><span class="math inline">\(P(t_i) = n_i / sum(n)\)</span></li>
<li>To avoid <span class="math inline">\(sum(n)=0\)</span> , every table starts at a baseline <span class="math inline">\(n = \alpha\)</span></li>
</ul></li>
<li><p>Everyone does the same as they enter</p>
<ul>
<li>Empty tables stay empty, full tables keep getting more people proportionally</li>
<li>After many people have entered, an equilibrium emerges</li>
</ul></li>
</ul>
<p><br></p>
<center>
– <a href="http://topicmodels.west.uni-koblenz.de/ckling/tmt/restaurant.html?parameters=3,2,1,5">Visual Demonstration of the Dirichlet Distribution</a> –
</center>
</section>
<section id="effect-of-the-hyperparameter-alpha" class="slide level2">
<h2>Effect of the hyperparameter <span class="math inline">\(\alpha\)</span></h2>
<ul>
<li><p>The restaurant converges to a multinomial distribution</p>
<ul>
<li>E.g. the topics per document, or words per topic</li>
</ul></li>
<li><p>The initial number of people at the tables is the <span class="math inline">\(\alpha\)</span> hyperparameter</p>
<ul>
<li>Hyperparameter = ‘setting’/choice that affects how other parameters are estimated</li>
</ul></li>
<li><p>Intuitive effect of lower alpha:</p>
<ul>
<li>initial choices of ‘customers’ have larger effect</li>
<li>likelier that a single table will get all participants</li>
</ul></li>
<li><p>Lower alpha = fewer topics per document</p>
<ul>
<li>but means topic have to include more words / have more overlap, as each word needs to be assigned</li>
</ul></li>
</ul>
</section>
<section id="plate-notation-of-lda" class="slide level2">
<h2>Plate notation of LDA</h2>
<div class="column" style="width:55%;">
<p><img data-src="img/lda_process.png"></p>
<p style="font-size:0.75em;">
</p><p><em>Blei et al., 2003</em></p>
<p></p>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:35%;">
<p>For each document <em>d</em>:</p>
<ul>
<li>Draw random topic proportions <span class="math inline">\(\theta_d\)</span></li>
<li>For each word <em>n</em> in document <em>d</em>:
<ul>
<li>Draw a single topic <span class="math inline">\(Z\)</span> from <span class="math inline">\(\theta_d\)</span></li>
<li>Draw a word <span class="math inline">\(W\)</span> from <span class="math inline">\(\beta_z\)</span></li>
</ul></li>
</ul>
<p><span class="math inline">\(\beta_k\)</span> and <span class="math inline">\(\theta_d\)</span> are drawn from <span class="math inline">\(Dir(\alpha)\)</span> and thus not observed directly</p>
</div>
<div class="fragment">
<p><strong>Question:</strong> How can we determine these parameters?</p>
</div>
</section>
<section id="think-about-a-regression-analysis" class="slide level2">
<h2>Think about a regression analysis…</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>A regression analysis aims to predict <span class="math inline">\(x\)</span> from <span class="math inline">\(y\)</span></p></li>
<li><p>It assumes a linear relationship between the two variables that can be expressed as</p></li>
</ul>
<p><span class="math inline">\(y_i = \beta_0 + \beta_1*x_i + e_i\)</span></p>
<ul>
<li><p>Similar to the topic model, we have <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, but we don’t actually have the parameter of interest: <span class="math inline">\(\beta_0\)</span>, and <span class="math inline">\(\beta_1\)</span></p></li>
<li><p>Through methods such as ordinary least squares, we try to find the value of the <span class="math inline">\(\beta\)</span>’s that minimize the sum of squared errors</p></li>
</ul>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:45%;">
<div class="cell">
<div class="cell-output-display">
<p><img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-3-1.png" width="480"></p>
</div>
</div>
</div>
</section>
<section id="reverse-engineering-the-generative-model" class="slide level2">
<h2>Reverse-Engineering the Generative Model</h2>
<div class="column" style="width:45%;">
<ul>
<li><p>The generative model assumes we know the parameters and want to find the words</p></li>
<li><p>But similar to the regression analysis, our challenge is the opposite:</p>
<ul>
<li>We know the actual words in the corpus</li>
<li>How can we find out the parameters of the model the explain their occurence?</li>
</ul></li>
<li><p>Task: Find the parameters that maximize the <em>likelihood</em> of the corpus</p></li>
</ul>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:45%;">
<ul>
<li><p>Unfortunately, there is no analytic solution</p>
<ul>
<li>Contrast with e.g.&nbsp;SVD and OLS that can be computed directly</li>
<li>Similar to multilevel models, this is not the case for LDA</li>
</ul></li>
<li><p>Need to do iterative approximation of best solution</p></li>
</ul>
</div>
</section>
<section id="gibbs-sampling-in-lda" class="slide level2">
<h2>Gibbs Sampling in LDA</h2>
<div class="column" style="width:30%;">
<p><img data-src="https://mr-easy.github.io/files/blog/gibbs/gibbs.gif"></p>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:60%;">
<ul>
<li><p>refers to a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution</p></li>
<li><p>Suppose you know the topics of all words except for one word <span class="math inline">\(w\)</span></p></li>
<li><p>This new word <span class="math inline">\(w\)</span> is the new guest entering the restaurant</p>
<ul>
<li>Pick a topic within the document proportional to existing topics in the document (plus alpha)</li>
<li>Pick a topic for the word proportional to existing topics for this word (plus eta)</li>
</ul></li>
<li><p>This gives a joint probability for all topics for this word</p></li>
</ul>
</div>
</section>
<section id="iterative-gibbs-sampling" class="slide level2">
<h2>Iterative Gibbs Sampling</h2>
<ol type="1">
<li><p>Start with random assignments of topics to words</p></li>
<li><p>For each word <span class="math inline">\(w\)</span> in document <span class="math inline">\(d\)</span></p>
<ul>
<li>Compute proportion of topics in word and document (disregarding <span class="math inline">\(w\)</span> itself)</li>
<li>Compute probability of each topic <span class="math inline">\(z\)</span> given those proportions</li>
<li>Pick a new topic from that probability</li>
<li>Update proportions for next iteration</li>
</ul></li>
<li><p>Repeat from 2 until converged</p></li>
</ol>
<p><br><br></p>
<div class="fragment">
<ul>
<li><p>This may sounds complicated and we won’t dive deeper into this topic.</p></li>
<li><p>However, if you are interested in how this works in detail, check out the blog post <a href="http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/">“LDA under the hood”</a> by A. Brook!</p></li>
</ul>
</div>
</section>
<section id="lets-look-at-an-example-in-r" class="slide level2">
<h2>Let’s look at an Example in R</h2>
<div class="cell" data-r.options="{&quot;width&quot;:80}">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="fu">library</span>(quanteda)</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">library</span>(quanteda.textplots)</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb5-5"><a href="#cb5-5"></a></span>
<span id="cb5-6"><a href="#cb5-6"></a>s1 <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">"data/science articles/train.csv"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb5-7"><a href="#cb5-7"></a>  <span class="fu">select</span>(<span class="at">id =</span> ID, <span class="at">title =</span> TITLE, <span class="at">abstract =</span> ABSTRACT) <span class="sc">%&gt;%</span></span>
<span id="cb5-8"><a href="#cb5-8"></a>  <span class="fu">slice</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>) <span class="co"># Make it a little smaller to run faster</span></span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="fu">head</span>(s1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment" data-r.options="{&quot;width&quot;:80}">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 3
     id title                                                           abstract
  &lt;dbl&gt; &lt;chr&gt;                                                           &lt;chr&gt;   
1     1 Reconstructing Subject-Specific Effect Maps                     "Predic…
2     2 Rotation Invariance Neural Network                              "Rotati…
3     3 Spherical polyharmonics and Poisson kernels for polyharmonic f… "We int…
4     4 A finite element approximation for the stochastic Maxwell--Lan… "The st…
5     5 Comparative study of Discrete Wavelet Transforms and Wavelet T… "Fourie…
6     6 On maximizing the fundamental frequency of the complement of a… "Let $\…</code></pre>
</div>
</div>
</section>
<section id="text-preprocessing-and-dtm" class="slide level2">
<h2>Text Preprocessing and DTM</h2>
<div class="cell" data-r.options="{&quot;width&quot;:120}">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>dtm <span class="ot">&lt;-</span> s1 <span class="sc">%&gt;%</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>  <span class="fu">corpus</span>(<span class="at">text =</span> <span class="st">"abstract"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>  <span class="fu">tokens</span>(<span class="at">remove_punct =</span> T, <span class="at">remove_numbers =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>  <span class="fu">tokens_remove</span>(<span class="fu">stopwords</span>(<span class="st">"en"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>  <span class="fu">tokens_select</span>(<span class="at">min_nchar =</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>  dfm <span class="sc">%&gt;%</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>  <span class="fu">dfm_trim</span>(<span class="at">min_termfreq =</span> <span class="dv">5</span>)</span>
<span id="cb7-8"><a href="#cb7-8"></a>dtm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment" data-r.options="{&quot;width&quot;:120}">
<div class="cell-output cell-output-stdout">
<pre><code>Document-feature matrix of: 5,000 documents, 9,238 features (99.33% sparse) and 2 docvars.
       features
docs    predictive models allow subject-specific inference analyzing disease related alterations neuroimaging
  text1          2      3     1                4         4         1       3       1           1            2
  text2          0      0     0                0         0         0       0       0           0            0
  text3          0      0     0                0         0         0       0       0           0            0
  text4          0      0     0                0         0         0       0       0           0            0
  text5          0      0     0                0         0         0       0       0           0            0
  text6          0      0     0                0         0         0       0       0           0            0
[ reached max_ndoc ... 4,994 more documents, reached max_nfeat ... 9,228 more features ]</code></pre>
</div>
</div>
</section>
<section id="text-preprocessing" class="slide level2">
<h2>Text preprocessing</h2>
<ul>
<li><p>Text preprocessing is more important for topic modeling than for other machine learning approaches</p></li>
<li><p>For example keeping stopwords will lead to non-sensical “topics” because they co-occur so often</p></li>
<li><p>Stemming or lemmatizing will streamline words and thereby improve coherence between topics.</p></li>
<li><p>We want to make sure that the model focuses on those words that really tell something about the topic</p></li>
<li><p>Text preprocessing stepts are often pivotal for the interpretability of the resulting topics</p></li>
</ul>
</section>
<section id="estimating-a-topic-model-in-r" class="slide level2">
<h2>Estimating a topic model in R</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="fu">library</span>(topicmodels)</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="co"># Convert dtm to topicmodels' specific format</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>dtm <span class="ot">&lt;-</span> <span class="fu">convert</span>(dtm, <span class="at">to =</span> <span class="st">"topicmodels"</span>) </span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="co"># Set seed to make it reproducible</span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb9-8"><a href="#cb9-8"></a></span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="co"># Fit topic model</span></span>
<span id="cb9-10"><a href="#cb9-10"></a>m <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dtm, </span>
<span id="cb9-11"><a href="#cb9-11"></a>         <span class="at">method =</span> <span class="st">"Gibbs"</span>, </span>
<span id="cb9-12"><a href="#cb9-12"></a>         <span class="at">k =</span> <span class="dv">6</span>,  </span>
<span id="cb9-13"><a href="#cb9-13"></a>         <span class="at">control =</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.1</span>))</span>
<span id="cb9-14"><a href="#cb9-14"></a>m</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>A LDA_Gibbs topic model with 6 topics.</code></pre>
</div>
</div>
<p><br></p>
<ul>
<li><p>We need to chose the sampling method: usually Gibbs sampling</p></li>
<li><p>We need to specify the number of topics (<span class="math inline">\(k\)</span>) a priori, but we can of course try out different solutions</p></li>
<li><p>We can set the alpha parameter for the Dirichlet distribution</p></li>
</ul>
</section>
<section id="inspecting-lda-results" class="slide level2">
<h2>Inspecting LDA results</h2>
<p>By using the function <code>terms()</code>, we can have a look at the most probable words in each of the six topics:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="fu">terms</span>(m, <span class="dv">15</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>  as_tibble</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 15 × 6
   `Topic 1`    `Topic 2`   `Topic 3`    `Topic 4`   `Topic 5` `Topic 6` 
   &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     
 1 algorithm    data        observations learning    show      can       
 2 problem      can         data         data        prove     magnetic  
 3 data         system      mass         network     also      phase     
 4 model        paper       galaxies     networks    graph     energy    
 5 method       systems     stars        neural      mathbb    model     
 6 can          network     find         model       paper     quantum   
 7 methods      using       star         can         group     state     
 8 paper        analysis    can          deep        space     field     
 9 show         based       using        training    study     states    
10 results      time        emission     method      graphs    system    
11 distribution different   stellar      models      given     dynamics  
12 models       model       present      using       result    spin      
13 approach     information galaxy       propose     results   two       
14 optimization used        two          performance finite    systems   
15 time         control     results      methods     set       properties</code></pre>
</div>
</div>
<p><br></p>
<div class="fragment">
<p><strong>Question:</strong> What topics do these wordlists stand for?</p>
</div>
</section>
<section id="top-words-in-each-topic" class="slide level2">
<h2>Top words in each topic</h2>
<p>But because we gain a probability with which a word is in a topic, we can also look at these probabilities per word per topic:</p>
<p><br></p>
<div class="column" style="width:45%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>topic <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>words <span class="ot">&lt;-</span> <span class="fu">posterior</span>(m)<span class="sc">$</span>terms[topic, ]</span>
<span id="cb13-3"><a href="#cb13-3"></a>topwords <span class="ot">&lt;-</span> <span class="fu">sort</span>(words, </span>
<span id="cb13-4"><a href="#cb13-4"></a>                 <span class="at">decreasing =</span> T) <span class="sc">%&gt;%</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>  <span class="fu">head</span>(<span class="at">n =</span> <span class="dv">50</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="fu">head</span>(topwords, <span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb13-7"><a href="#cb13-7"></a>  as.data.frame</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                       .
observations 0.005133322
data         0.004952844
mass         0.004927061
galaxies     0.004359843
stars        0.004256713
find         0.003895756
star         0.003844190
can          0.003766843
using        0.003766843
emission     0.003689495</code></pre>
</div>
</div>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:45%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">library</span>(wordcloud)</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="fu">wordcloud</span>(<span class="fu">names</span>(topwords), topwords)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-9-1.png" width="960"></p>
</div>
</div>
</div>
</section>
<section id="alternative-visualization" class="slide level2">
<h2>Alternative visualization</h2>

<img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-10-1.png" width="960" class="r-stretch"></section>
<section id="probabilites-of-topics-per-document" class="slide level2">
<h2>Probabilites of topics per document</h2>
<ul>
<li><p>From the topic model, we can extract a table that shows us the probabilities of a topic being written about in the different texts (bear in mind, several topics can be in one text)</p></li>
<li><p>We see for example, that there is a high probability of the text 7 being about topic 3 (astrophysics)</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a><span class="fu">posterior</span>(m)<span class="sc">$</span>topics <span class="sc">%&gt;%</span> </span>
<span id="cb16-2"><a href="#cb16-2"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb16-3"><a href="#cb16-3"></a>  <span class="fu">rownames_to_column</span>(<span class="st">"doc"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>  as_tibble</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5,000 × 7
   doc        `1`      `2`     `3`      `4`      `5`     `6`
   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
 1 text1  0.234   0.000569 0.0177  0.695    0.000569 0.0518 
 2 text2  0.00198 0.00198  0.00198 0.753    0.239    0.00198
 3 text3  0.00224 0.00224  0.00224 0.00224  0.989    0.00224
 4 text4  0.391   0.00162  0.00162 0.00162  0.375    0.229  
 5 text5  0.341   0.00136  0.00136 0.654    0.00136  0.00136
 6 text6  0.0105  0.000956 0.0583  0.000956 0.919    0.0105 
 7 text7  0.00198 0.0415   0.951   0.00198  0.00198  0.00198
 8 text8  0.00150 0.00150  0.00150 0.0465   0.00150  0.947  
 9 text9  0.0179  0.00162  0.229   0.00162  0.00162  0.748  
10 text10 0.00124 0.373    0.00124 0.212    0.100    0.311  
# … with 4,990 more rows</code></pre>
</div>
</div>
</section>
<section id="is-text-7-really-about-astrophysics" class="slide level2">
<h2>Is text 7 really about astrophysics?</h2>
<div class="column" style="width:45%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a>s1 <span class="sc">%&gt;%</span></span>
<span id="cb18-2"><a href="#cb18-2"></a>  <span class="fu">filter</span>(id <span class="sc">==</span> <span class="dv">7</span>) <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>  <span class="fu">select</span>(abstract) <span class="sc">%&gt;%</span></span>
<span id="cb18-4"><a href="#cb18-4"></a>  <span class="fu">as.character</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "We observed the newly discovered hyperbolic minor planet 1I/`Oumuamua (2017\nU1) on 2017 October 30 with Lowell Observatory's 4.3-m Discovery Channel\nTelescope. From these observations, we derived a partial lightcurve with\npeak-to-trough amplitude of at least 1.2 mag. This lightcurve segment rules out\nrotation periods less than 3 hr and suggests that the period is at least 5 hr.\nOn the assumption that the variability is due to a changing cross section, the\naxial ratio is at least 3:1. We saw no evidence for a coma or tail in either\nindividual images or in a stacked image having an equivalent exposure time of\n9000 s.\n"</code></pre>
</div>
</div>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:45%;">
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/2/2f/Hubble_ultra_deep_field.jpg"></p>
</div>
</section>
<section id="probabilites-of-topics-per-document-1" class="slide level2">
<h2>Probabilites of topics per document</h2>
<p>Let’s check out another one: text 2 seems to be primarily about topic 4 (machine learning?):</p>
<div class="column" style="width:62%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>topic <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb20-2"><a href="#cb20-2"></a>words <span class="ot">&lt;-</span> <span class="fu">posterior</span>(m)<span class="sc">$</span>terms[topic, ]</span>
<span id="cb20-3"><a href="#cb20-3"></a>topwords <span class="ot">&lt;-</span> <span class="fu">head</span>(<span class="fu">sort</span>(words, <span class="at">decreasing =</span> T), <span class="at">n=</span><span class="dv">50</span>)</span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="fu">wordcloud</span>(<span class="fu">names</span>(topwords), topwords)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-13-1.png" width="960"></p>
</div>
</div>
</div>
<div class="column" style="width:35%;">
<div class="cell" data-output-location="fragment">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>s1 <span class="sc">%&gt;%</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>  <span class="fu">filter</span>(id <span class="sc">==</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb21-3"><a href="#cb21-3"></a>  <span class="fu">select</span>(abstract) <span class="sc">%&gt;%</span></span>
<span id="cb21-4"><a href="#cb21-4"></a>  <span class="fu">as.character</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n"</code></pre>
</div>
</div>
</div>
</section>
<section id="substantive-analyses" class="slide level2">
<h2>Substantive analyses</h2>
<p>Depending on the research questions, we now may for example want to describe the topic prevalence in the corpus.</p>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a>(table <span class="ot">&lt;-</span>  <span class="fu">posterior</span>(m)<span class="sc">$</span>topics <span class="sc">%&gt;%</span></span>
<span id="cb23-2"><a href="#cb23-2"></a>  as.data.frame <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a href="#cb23-3"></a>  <span class="fu">rownames_to_column</span>(<span class="st">"docs"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-4"><a href="#cb23-4"></a>  <span class="fu">gather</span>(topic, value, <span class="sc">-</span>docs) <span class="sc">%&gt;%</span></span>
<span id="cb23-5"><a href="#cb23-5"></a>  as_tibble <span class="sc">%&gt;%</span></span>
<span id="cb23-6"><a href="#cb23-6"></a>  <span class="fu">arrange</span>(docs, value))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 30,000 × 3
   docs   topic    value
   &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;
 1 text1  2     0.000569
 2 text1  5     0.000569
 3 text1  3     0.0177  
 4 text1  6     0.0518  
 5 text1  1     0.234   
 6 text1  4     0.695   
 7 text10 1     0.00124 
 8 text10 3     0.00124 
 9 text10 5     0.100   
10 text10 4     0.212   
# … with 29,990 more rows</code></pre>
</div>
</div>
</div>
</section>
<section id="visualization-of-topic-prevalence" class="slide level2">
<h2>Visualization of topic prevalence</h2>
<ul>
<li><p>Bear in mind that this is not the proportion of articles that are about this topic!</p></li>
<li><p>It is the overall proportion of the topic in the corpus given that articles can be about several topics</p></li>
</ul>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a>table <span class="sc">%&gt;%</span></span>
<span id="cb25-2"><a href="#cb25-2"></a>  <span class="fu">group_by</span>(topic) <span class="sc">%&gt;%</span></span>
<span id="cb25-3"><a href="#cb25-3"></a>  <span class="fu">summarize</span>(<span class="at">prop =</span> <span class="fu">mean</span>(value)) <span class="sc">%&gt;%</span></span>
<span id="cb25-4"><a href="#cb25-4"></a>  <span class="fu">mutate</span>(<span class="at">topic =</span> <span class="fu">recode</span>(topic, </span>
<span id="cb25-5"><a href="#cb25-5"></a>         <span class="st">"1"</span> <span class="ot">=</span> <span class="st">"Statistics?"</span>, </span>
<span id="cb25-6"><a href="#cb25-6"></a>         <span class="st">"2"</span> <span class="ot">=</span> <span class="st">"Computer Science?"</span>, </span>
<span id="cb25-7"><a href="#cb25-7"></a>         <span class="st">"3"</span> <span class="ot">=</span> <span class="st">"(Astro-)Physics"</span>, </span>
<span id="cb25-8"><a href="#cb25-8"></a>         <span class="st">"4"</span> <span class="ot">=</span> <span class="st">"Artificial Intelligence"</span>, </span>
<span id="cb25-9"><a href="#cb25-9"></a>         <span class="st">"5"</span> <span class="ot">=</span> <span class="st">"Math"</span>, </span>
<span id="cb25-10"><a href="#cb25-10"></a>         <span class="st">"6"</span> <span class="ot">=</span> <span class="st">"Physics"</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb25-11"><a href="#cb25-11"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">fct_reorder</span>(topic, prop), </span>
<span id="cb25-12"><a href="#cb25-12"></a>             <span class="at">y =</span> prop, <span class="at">fill =</span> topic)) <span class="sc">+</span></span>
<span id="cb25-13"><a href="#cb25-13"></a>  <span class="fu">geom_col</span>() <span class="sc">+</span></span>
<span id="cb25-14"><a href="#cb25-14"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb25-15"><a href="#cb25-15"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb25-16"><a href="#cb25-16"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">""</span>, </span>
<span id="cb25-17"><a href="#cb25-17"></a>       <span class="at">y =</span> <span class="st">"Proportion in the corpus"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output-display">
<p><img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-16-1.png" width="480"></p>
</div>
</div>
</div>
</section>
<section id="validation" class="slide level2">
<h2>Validation</h2>
<ul>
<li><p>The first step after fitting a model is inspecting the results and establishing face validity</p></li>
<li><p>Top words per topic are a good place to start, but one should also look at the top documents per topic to better understand how words are used in context.</p></li>
<li><p>Also, it is good to inspect the relationships between topics and look at documents that load high on multiple topics to understand the relationship</p></li>
</ul>
<div class="fragment">
<ul>
<li><p>If one using topic models in a more confirmatory manner, e.g., the topics should match some sort of predefined categorization, you should use regular gold standard techniques for validation:</p>
<ul>
<li>code a sufficiently large random sample of documents with your predefined categories, and test whether the LDA topics match those categories (compute accuracy, precision, recall, F1-score)</li>
<li>In general, however, in such cases it is a better idea to use a dictionary or supervised analysis technique as topic models often do not exactly capture our categories</li>
</ul></li>
</ul>
</div>
</section>
<section id="choosing-the-right-number-of-topics" class="slide level2">
<h2>Choosing the right number of topics</h2>
<ul>
<li><p>Topic models such as LDA allow you to specify the number of topics in the model</p>
<ul>
<li>This is a nice thing because it allows you to adjust the granularity of what topics measure: between a few broad topics and many more specific topics.</li>
<li>But it begets the question what the best number of topics is.</li>
</ul></li>
</ul>
<div class="fragment">
<ul>
<li><p>The short and perhaps disappointing answer is: The best number of topics does not exist</p>
<ul>
<li>there is no singular idea of what a topic even is is</li>
<li>it depends on what you are interested in: e.g., if you want know what a corpus is about, you want to have a limited number of topics that provide a good representation of overall themes</li>
</ul></li>
</ul>
</div>
<div class="fragment">
<ul>
<li><p>But even if the best number of topics does not exist, some values for k (i.e.&nbsp;the number of topics) are better than others.</p>
<ul>
<li>If we use too few topics, there will be variance in the data that is not accounted for,</li>
<li>If you use too many topics you will overfit and get topics that are not interesting</li>
</ul></li>
</ul>
</div>
</section>
<section id="how-well-does-our-model-fit-the-data" class="slide level2">
<h2>How well does our model fit the data?</h2>
<ul>
<li><p>One approach to the “best” number of topics is to check which model best predicts the data</p></li>
<li><p>This is comparable to goodness-of-fit measures for statistical models (e.g., log likelihood, CFI, etc.)</p></li>
<li><p>For LDA topic models, a commonly usd indicator is <strong>perplexity</strong> (Blei, Ng, &amp; Jordan, 2003), where lower perplexity indicate better predictin/fit</p></li>
<li><p>To calculate perplexity, we follow an already known procedure (last lecture!):</p>
<ul>
<li>We first train an LDA model on a portion of the data</li>
<li>Then, we model is evaluatd using the held-out portion of the data</li>
<li>This procedure is repeated for models with different numbers of topics so that it becomes clear which one leads to the lowest perplexity</li>
</ul></li>
</ul>
</section>
<section id="calculating-perplexity-in-r" class="slide level2">
<h2>Calculating perplexity in R</h2>
<ul>
<li><p>We first have to split up our data into data for training and testing the model</p></li>
<li><p>This way we prevent overfitting the model</p></li>
<li><p>Here we’ll use 75% for training, and held-out the remaining 25% for test data.</p></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a><span class="co"># Split sample</span></span>
<span id="cb26-2"><a href="#cb26-2"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rownames</span>(dtm), <span class="fu">nrow</span>(dtm) <span class="sc">*</span> .<span class="dv">75</span>)</span>
<span id="cb26-3"><a href="#cb26-3"></a>dtm_train <span class="ot">&lt;-</span> dtm[<span class="fu">rownames</span>(dtm) <span class="sc">%in%</span> train, ]</span>
<span id="cb26-4"><a href="#cb26-4"></a>dtm_test <span class="ot">&lt;-</span> dtm[<span class="sc">!</span><span class="fu">rownames</span>(dtm) <span class="sc">%in%</span> train, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>Then, we train the model and then, we calculate the perplexity by testing the model on the test data</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a>m_train <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dtm_train, <span class="at">method =</span> <span class="st">"Gibbs"</span>, <span class="at">k =</span> <span class="dv">6</span>,  <span class="at">control =</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.01</span>))</span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="fu">perplexity</span>(m, dtm_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1977.736</code></pre>
</div>
</div>
</section>
<section id="estimating-several-model-and-perplexity-scores" class="slide level2">
<h2>Estimating several model and perplexity scores</h2>
<ul>
<li><p>A single perplexity score is not really useful as we have nothing to compare it against</p></li>
<li><p>Instead, we calculate the perplexity score for models with different parameters, to see how this affects the perplexity</p></li>
</ul>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a>p <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">k =</span> <span class="fu">c</span>(<span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">12</span>,<span class="dv">24</span>,<span class="dv">48</span>,<span class="dv">96</span>),</span>
<span id="cb29-2"><a href="#cb29-2"></a>                <span class="at">perplexity =</span> <span class="cn">NA</span>)</span>
<span id="cb29-3"><a href="#cb29-3"></a></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="do">## loop over the values of k in data.frame p </span></span>
<span id="cb29-5"><a href="#cb29-5"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(p)) {</span>
<span id="cb29-6"><a href="#cb29-6"></a>  <span class="co"># calculate perplexity for the given value of k</span></span>
<span id="cb29-7"><a href="#cb29-7"></a>  m <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dtm_train, </span>
<span id="cb29-8"><a href="#cb29-8"></a>           <span class="at">method =</span> <span class="st">"Gibbs"</span>, </span>
<span id="cb29-9"><a href="#cb29-9"></a>           <span class="at">k =</span> p<span class="sc">$</span>k[i],  </span>
<span id="cb29-10"><a href="#cb29-10"></a>           <span class="at">control =</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.01</span>))</span>
<span id="cb29-11"><a href="#cb29-11"></a>  <span class="co"># store result in our data.frame</span></span>
<span id="cb29-12"><a href="#cb29-12"></a>  p<span class="sc">$</span>perplexity[i] <span class="ot">=</span> <span class="fu">perplexity</span>(m, dtm_test)</span>
<span id="cb29-13"><a href="#cb29-13"></a>}</span>
<span id="cb29-14"><a href="#cb29-14"></a></span>
<span id="cb29-15"><a href="#cb29-15"></a><span class="co"># Output</span></span>
<span id="cb29-16"><a href="#cb29-16"></a>p</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output cell-output-stdout">
<pre><code>   k perplexity
1  3   2665.177
2  6   2417.928
3 12   2170.333
4 24   1991.856
5 48   1854.943
6 96   1800.028</code></pre>
</div>
</div>
</div>
</section>
<section id="visualizing-a-perplexity-curve" class="slide level2">
<h2>Visualizing a perplexity curve</h2>
<ul>
<li><p>Technically, the best fitting model is the one with the lowest perplexity score</p>
<ul>
<li>But this will always be a model with a lot of topics!</li>
</ul></li>
<li><p>If we want to use topic modelling for bottom-up inductive analyses of text corpora, we need to look for a “knee” in the plot</p></li>
</ul>
<div class="cell columns column-output-location">
<div class="column">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="fu">ggplot</span>(p, <span class="fu">aes</span>(<span class="at">x =</span> k, <span class="at">y =</span> perplexity)) <span class="sc">+</span> </span>
<span id="cb31-2"><a href="#cb31-2"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb31-3"><a href="#cb31-3"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb31-4"><a href="#cb31-4"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">"Number of topics (k)"</span>,</span>
<span id="cb31-5"><a href="#cb31-5"></a>       <span class="at">y =</span> <span class="st">"Perplexity"</span>,</span>
<span id="cb31-6"><a href="#cb31-6"></a>       <span class="at">title =</span> <span class="st">"Perplexity curve"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div><div class="column fragment">
<div class="cell-output-display">
<p><img data-src="04_unsupervisedlearning_files/figure-revealjs/unnamed-chunk-20-1.png" width="480"></p>
</div>
</div>
</div>
</section>
<section id="new-model-with-12-topics" class="slide level2">
<h2>New model with 12 topics</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">LDA</span>(dtm, </span>
<span id="cb32-2"><a href="#cb32-2"></a>         <span class="at">method =</span> <span class="st">"Gibbs"</span>, </span>
<span id="cb32-3"><a href="#cb32-3"></a>         <span class="at">k =</span> <span class="dv">12</span>,  </span>
<span id="cb32-4"><a href="#cb32-4"></a>         <span class="at">control =</span> <span class="fu">list</span>(<span class="at">alpha =</span> <span class="fl">0.1</span>))</span>
<span id="cb32-5"><a href="#cb32-5"></a><span class="fu">terms</span>(m2, <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell fragment">
<div class="cell-output cell-output-stdout">
<pre><code>      Topic 1        Topic 2     Topic 3     Topic 4       Topic 5       
 [1,] "mass"         "method"    "network"   "system"      "data"        
 [2,] "observations" "using"     "networks"  "systems"     "model"       
 [3,] "galaxies"     "images"    "model"     "can"         "models"      
 [4,] "stars"        "image"     "can"       "performance" "distribution"
 [5,] "find"         "signal"    "dynamics"  "paper"       "method"      
 [6,] "star"         "can"       "different" "code"        "can"         
 [7,] "stellar"      "noise"     "nodes"     "power"       "methods"     
 [8,] "galaxy"       "based"     "models"    "network"     "estimation"  
 [9,] "emission"     "detection" "time"      "data"        "results"     
[10,] "can"          "imaging"   "social"    "memory"      "used"        
      Topic 6       Topic 7        Topic 8         Topic 9       Topic 10 
 [1,] "data"        "algorithm"    "control"       "magnetic"    "mathbb" 
 [2,] "analysis"    "problem"      "system"        "phase"       "show"   
 [3,] "research"    "algorithms"   "can"           "energy"      "prove"  
 [4,] "can"         "problems"     "approach"      "field"       "group"  
 [5,] "paper"       "optimization" "learning"      "spin"        "also"   
 [6,] "using"       "can"          "policy"        "quantum"     "mathcal"
 [7,] "information" "show"         "reinforcement" "state"       "paper"  
 [8,] "users"       "number"       "model"         "states"      "study"  
 [9,] "study"       "optimal"      "environment"   "temperature" "groups" 
[10,] "different"   "matrix"       "using"         "surface"     "finite" 
      Topic 11   Topic 12   
 [1,] "learning" "equation" 
 [2,] "neural"   "solutions"
 [3,] "model"    "equations"
 [4,] "network"  "system"   
 [5,] "deep"     "theory"   
 [6,] "data"     "can"      
 [7,] "training" "method"   
 [8,] "networks" "numerical"
 [9,] "models"   "systems"  
[10,] "can"      "nonlinear"</code></pre>
</div>
</div>
</section></section>
<section>
<section id="example-from-the-literature" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Example from the literature</h1>

</section>
<section id="what-communication-scholars-write-about" class="slide level2">
<h2>What communication scholars write about</h2>
<ul>
<li><p>Günther and Domahidi (2017) used LDA topic modeling to get an overview of research topics in 80 years of communication research</p></li>
<li><p>Documents were 15,000 abstracts from academic journals</p></li>
<li><p>To find a reasonable value k (number of topics), they ran 40 topic models with k = 5 to k = 200 and systematically compared them: The final model hat 145 topics</p></li>
<li><p>For each document (abstract), they selected the two topics with the highest probability (minimum probability was .1)</p></li>
<li><p>To ensure a meaningful interpretation, the authors validated the labels of the inferred topics by manually checking publications from every decade that contained the topic with high probability.</p></li>
</ul>
</section>
<section id="core-topics-in-communication-research" class="slide level2">
<h2>Core topics in communication research</h2>

<img data-src="img/gunther_table1_full.png" class="r-stretch"></section>
<section id="evolution-of-topics-over-time" class="slide level2">
<h2>Evolution of topics over time</h2>

<img data-src="img/gunther_fig1.png" class="r-stretch"></section>
<section id="type-of-media-researched-over-time" class="slide level2">
<h2>Type of media researched over time</h2>

<img data-src="img/gunther_fig4.png" class="r-stretch"></section></section>
<section>
<section id="conclusions-and-outlook" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Conclusions and outlook</h1>

</section>
<section id="what-is-the-state-of-the-art" class="slide level2">
<h2>What is the state of the art?</h2>
<ul>
<li><p>LDA is still one of the most used approaches to topic modeling and performs well in many circumstances</p></li>
<li><p>Yet, there are many new approaches including e.g., structural topic modeling (see R package <code>stm</code>; Roberts et al., 2019)</p>
<ul>
<li>an extension of LDA that allow us to explicitly model text metadata such as date or author as covariates of the topic prevalence and/or topic words distributions</li>
<li>Text contextual information into account!</li>
</ul></li>
<li><p>Recent advances have been made by using pre-trained transformer-based language models (BERT, BERTopic; Grootendorst, 2022)</p>
<ul>
<li>generates document embedding with pre-trained transformer-based language models</li>
<li>clusters these embeddings</li>
<li>generates topic representations with the class-based TF-IDF procedure.</li>
</ul></li>
</ul>
</section>
<section id="conclusion" class="slide level2">
<h2>Conclusion</h2>
<div class="column" style="width:45%;">
<p><img data-src="img/topic_slides/Slide4.png"></p>
</div>
<div class="column" style="width:5%;">

</div>
<div class="column" style="width:45%;">
<ul>
<li><p>Topic Modeling reduces the dimensionality of a document-term matrix by clustering words and documents into “latent” topics</p></li>
<li><p>Interpretation of resulting “topic solutions” must be done by the researcher (face validity)</p></li>
<li><p>LDA is driven by the Dirichlet process that yields distributions</p>
<ul>
<li>skewed towards few topics, but controllable with alpha parameter</li>
</ul></li>
<li><p>Gibbs sampling is a non-deterministic way to fit LDA models</p></li>
</ul>
</div>
</section></section>
<section>
<section id="thank-you-for-your-attention" class="title-slide slide level1 center" data-background-color="steelblue">
<h1>Thank you for your attention!</h1>

</section>
<section id="required-reading" class="slide level2">
<h2>Required Reading</h2>
<p><br><br></p>
<p>Günther, E. , &amp; Domahidi, E. (2017). What Communication Scholars Write About: An Analysis of 80 Years of Research in High-Impact Journals. International Journal of Communication 11(2017), 3051–3071</p>
<p>Jacobi,C., van Atteveldt, W. &amp; Welbers, K. (2016) Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital Journalism, 4(1), 89-106, DOI: 10.1080/21670811.2015.1093271</p>
<p><br></p>
<p><em>(available on Canvas)</em></p>
</section>
<section id="references" class="slide level2 smaller">
<h2>References</h2>
<ul>
<li><p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.</p></li>
<li><p>Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794</p></li>
<li><p>Roberts, M. E., Stewart, B. M., &amp; Tingley, D. (2019). Stm: An R package for structural topic models. Journal of Statistical Software, 91, 1-40.</p></li>
<li><p>Steyvers, M., &amp; Griffiths, T. (2007). Probabilistic topic models. In Handbook of latent semantic analysis (pp.&nbsp;439-460). Psychology Press.</p></li>
<li><p>van Atteveldt, W., Trilling, D., &amp; Calderon, C. A. (2022). Computational Analysis of Communication. John Wiley &amp; Sons.</p></li>
</ul>
</section>
<section id="example-exam-question-multiple-choice" class="slide level2">
<h2>Example Exam Question (Multiple Choice)</h2>
<p>Which of the following statements is correct? In an LDA model…</p>
<p>A. …each word can be in every topic and every document can be about every topic.</p>
<p>B. …each word is linked to one specific topic, but every document can be about several topics.</p>
<p>C. …each word can be in every topic, but every document is about one specific topic.</p>
<p>D. …each word is linked to one specific topic and every document is about one specific topic.</p>
</section>
<section id="example-exam-question-multiple-choice-1" class="slide level2">
<h2>Example Exam Question (Multiple Choice)</h2>
<p>Which of the following statements is correct? In an LDA model…</p>
<p><strong>A. …each word can be in every topic and every document can be about every topic.</strong></p>
<p>B. …each word is linked to one specific topic, but every document can be about several topics.</p>
<p>C. …each word can be in every topic, but every document is about one specific topic.</p>
<p>D. …each word is linked to one specific topic and every document is about one specific topic.</p>
</section>
<section id="example-exam-question-open-question" class="slide level2">
<h2>Example Exam Question (Open Question)</h2>
<p>Why is it important to carefully consider preprocessing steps in topic modelling?</p>
<div class="fragment">
<p>A topic model does not analyse documents directly, but uses a so-called docu- ment–term matrix based on these documents. This matrix lists the frequency for each term (word) in each document. The first step in creating this matrix is tokenization, which means splitting the text into a list of words. For many machine learning approaches, no further steps are necessary. However, for topic modeling it is worth considering whether further preprocessing steps such as stemming or lemmatization, removal of stopwords (or otherwise frequent but non-informative words) or frequency trimming is fruitful as they can signicantly improve the interpretability of the resulting topics.</p>
<p>For example, stopwords do not really represent “topics”. If they are kept in the document-term matrix, they would drive the topic selection in undesirable ways. Furthermore, differently conjugated words could end up in different topics, even though they stand for the same topic. Hence stemming or lemmatizing is usually a good choice for topic modelling.</p>
<p><img src="img/logo.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="04_unsupervisedlearning_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        function fireSlideChanged(previousSlide, currentSlide) {

          // dispatch for htmlwidgets
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for reveal
        if (window.Reveal) {
          window.Reveal.addEventListener("slidechanged", function(event) {
            fireSlideChanged(event.previousSlide, event.currentSlide);
          });
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>