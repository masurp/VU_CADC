---
title: "Computational Analysis of Digital Communication"
subtitle: "Week 3: Supervised Text Classification"
author: "Dr. Philipp K. Masur"
format:
  revealjs: 
    theme: [default, theme.scss]
    logo: img/logo.png
    background-transition: fade
    title-slide-attributes:
        data-background-image: https://storage.googleapis.com/exenzo-jobboard/bb/public/thumb/logo-jpg-57.jpg
        data-background-position: "top right"
        data-background-size: auto
editor: visual
---

# Machine learning??? {background-color="steelblue"}

## If science fiction stories...

-   are to be believed, the invention of **artificial intelligence** inevitably leads to apocalyptic wars between machines and their makers

-   It begins with today's reality: computers learning how to **play simple games** and automate routines

-   They later are **given control** over traffic lights and communications, followed by military drones and missiles

-   This evolution takes a bad turn once computers become **sentient** and learn how to teach themselves....

## Machines are taking over!

![](img/thematrix.jpg)

## And then?

::: {.column width="45%"}
-   Having no more need for human programmers, humankind is simply **deleted**...

-   Is this what machine learning is about?

<br> <br> <br> <br> <br> <br> <br> <br>

*(Stills from the movies "Ex Machina" and "Her")*
:::

::: {.column width="4%"}
:::

::: {.column width="45%"}
![](https://filmkrant.nl/wp-content/uploads/2015/03/ex_machina-1000x563.jpg)

![](https://m.media-amazon.com/images/M/MV5BMTc1MDY0MjI4NV5BMl5BanBnXkFtZTgwMzAxNjM3MDE@._V1_.jpg)
:::

## So what are we actually talking about?

-   Machine learning is the study of computer algorithms that can **improve automatically** through experience and by the use of data

-   The field originated in an environment where the available data, statistical methods, and computing power rapidly and simultaneously evolved

-   Due to the "black box" nature of the algorithm's operations, it is often seen as a form of **artificial intelligence**

-   But in simple term: Machines are not good at asking questions or even knowing what questions are

-   They are much better in **answering them**, provided the question is stated in a way that a computer can comprehend (remember the main challenge of text analysis?)

## Applications of Machine Learning

-   ![](https://upload.wikimedia.org/wikipedia/commons/1/18/AI-ML-DL.png){style="float: right; padding-left: 40px;" width="400"}Machine learning is most successful when it **augments**, rather than replaces, the specialized knowledge of a subject-matter expert.

-   Machine learning is used in a wide variety of applications and contexts, such as in businesses, hospitals, scientific laboratories, or governmental organizations

-   In communication science, we can use these techniques to **automate** text analysis!

<br>

*Lantz, 2013*

## Some success stories

![](https://www.electricmotorengineering.com/files/2020/07/Autonomous-driving-Barcelona.jpg){style="float: right; padding-left: 40px;" width="400"}Applying machine learning in practical context:

-   Identification of spam messages in mails
-   Segmentation of customers for targeted advertising
-   Weather forecasts and long-term climate changes
-   Reduction of fraudulent credit card transactions
-   Prediction of election outcomes
-   Auto-piloting and self-driving cars
-   Face recognition
-   Optimization of energy use in homes and buildings
-   Discovery of genetic sequences linked to diseases -....

## Content of this lecture

**1.** What is machine learning?

**2.** Supervised text classification

-   Overview
-   Principles
-   Validation
-   Example: Predicting genre from song lyrics

**3.** Examples from the literature

**4.** Outlook and conclusion

# What is machine learning? {background-color="steelblue"}

Differences between supervised and unsupervised approaches.

## Deductive vs. inductive approaches

-   In the previous lecture, we talked about deductive approaches (e.g., dictionary approaches)

-   These are **deterministic** and are based on text theory (e.g., happy -\> positive, hate -\> negative)

-   Yet, natural language is often ambiguous and **probabilistic** coding may be better

-   Dictionary-based or generally rule-based approaches are not very similar to manual coding; a human being assesses much more than just a list of words!

-   Inductive approaches promise to combine the scalability of automatic coding with the validity of manual coding (supervised learning) or can even identify things or relations that we as human beings cannot identify (unsupervised learning)

------------------------------------------------------------------------

![](img/machinelearning.png)

------------------------------------------------------------------------

::: {.column width="45%"}
### Supervised learning

-   Algorithms build a model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to do so

-   Combines the scalability of automatic coding with the validity of manual coding (requires pre-labeled data to train algorithm)

-   Examples:

    -   Supervised text classification, such as extending manual coding to large text corpora, sentiment analysis...
    -   Pattern recognition: e.g., face recognition, spam filter,...
:::

::: {.column width="4%"}
:::

::: {.column width="45%"}
### Unsupervised learning (next lecture)

-   Algorithm detects clusters, patterns, or associations in data that has not been labeled previously, but researcher needs to interpret results

-   Very helpful to make sense of new data (similar to cluster analysis or exploratory factor analysis)

-   Examples:

    -   Topic modeling: Extracting topics from unlabeled (text) data
    -   Customer segmentation: Better understanding different customer groups around which to build marketing or other business strategies
:::

# Supervised text classification {background-color="steelblue"}

Training algorithms to make good predictions!

## Supervised text classification

We can now use machine learning models to classify text into specific sets of categories. This is known as **supervised learning**. The basic process is:

<br>

**1.** Manually code a small set of documents (say N = 1,000) for whatever variable(s) you care about

**2.** Train a machine learning model on the hand-coded data, using the variable as the outcome of interest and the text features of the documents as the predictors

**3.** Evaluate the effectiveness of the machine learning model via cross-validation (test it on new data/gold standard)

**4.** Once you have trained a model with sufficient predictive accuracy, apply the model to the remaining set of documents that have never been hand-coded (e.g., N = 100,000) or use it in the planned application (e.g., a spam filter detection software)

## Basic Procedure

![](img/machinelearning_process.png)

## Example: Spam Detection

-   Suppose we would want to develop a tool to automatically filter spam messages

-   How would you do this if you could only use a dictionary?

    -   Compare spam and not spam emails to see which words and word combinations occur a lot in spam and not in not spam.
    -   Downsides: Extremely time consuming and difficult!

. . .

-   Machine learning solution

    -   Transform the emails into data (e.g., a document-term matrix!)
    -   Let the computer figure out how to compute a probability for whether an email is spam
    -   Different ML algorithms figure this out in different ways

-   The resulting "classifier" can then be integrated in a software tool that can be used to detect spam mails automatically

## General Idea

-   ![](https://quantdare.com/wp-content/uploads/2016/03/Imagen1-1150x552.png){style="float: right; padding-left: 40px;" width="600"}Model relation between...
    -   Input features
        -   Similar to independent variables in statistics
        -   Can be MANY features (e.g., all words in a DTM)
    -   Output class
        -   Similar to dependent variables
        -   Can be categories (e.g., sentiment, topic classification) or continuous (e.g., stock market value)

## Statistical Modeling vs. Machine Learning?

::: {.column width="45%"}
-   Machine learning is thus *similar* to normal statistical modeling

-   Learn $f$ so you can predict $y$ from $x$:

<center>$y = f(x)$</center>

-   In a linear regression model, we aim to find the best fitting "line" that best predicts `y` based on `x`.
:::

::: {.column width="4%"}
:::

::: {.column width="49%"}
```{r, echo = F}
#| fig-height: 5
#| fig-width: 5
library(tidyverse)
data <- tibble(x = rnorm(50, 3, 2),
               y = 2*x + rnorm(50, 0, 4))
ggplot(data, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = F, color = "darkgrey") +
  theme_classic() +
  geom_point(aes(x = 2.5, y = 4.75), shape = 1, size = 4, color = "steelblue") +
  geom_hline(yintercept = 4.75, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = 2.5, linetype = "dashed", color = "grey")
```
:::

## Differences

- Goal of 'normal' modeling: explaining/understanding
    
    - Serves to make inferences about a population ("Does X relate to Y?")
    - Doesn't use too many variables to avoid the difficult of interpreting too many parameters
    - Requires interpretable parameters

- Goal of machine learning: best possible prediction
    
    - make generalizable predictions ("How to best predict Y?")
    - Use as many variables as you need, and don’t worry about interpretability of parameters
    - Always train (fit) and test (validate) on distinct data

. . .

**Note:** Machine learning models often have 1000's of collinear independent variables and can have many latent variables!

## Supervised Approaches

### Advantages

-   independent of language and topic; we only need consistently coded training material

-   can be connected to traditional content analysis (same operationalization, similar criteria in terms of validity and reliability)

-   efficient (analysis of very large samples and text corpora possible)

### Disadvantages

-   Requires large amounts of (manually) coded training data

-   Requires in-depth validation

# Principles of Supervised Text Classification {background-color="steelblue"}

How do these algorithms work?

## Overview of different algorithms

-   There are many different "algorithms" or classifiers that we can use:

    -   Naive Bayes
    -   Support Vector Machines
    -   Logistic regression
    -   k-Nearest neighbors
    -   ... and many more

-   Most of these algorithms have certain hyperparameters that need to be set

    -   e.g., learning rate, regularization, structure...

-   Unfortunately, there is no good theoretical basis for selecting an algorithm

    -   Solution: choose algorithm that performs best

## The Naive Bayes Algorithm

-   ![](https://s3.ap-south-1.amazonaws.com/techleer/204.png){style="float: right; padding-left: 40px;" width="600"}Computes the prior probability ( *P* ) for every category ( *c* = outcome variable ) based on the training data set

-   Computes the probability of every feature ( *x* ) to be a characteristic of the class ( *c* ); i.e., the relative frequency of the feature in category

-   For every probability of a category in light of certain features ( *P(c\|X)* ), all feature probabilities ( *x* ) are multiplied

-   The algorithm hence chooses the class that has highest weighted sum of inputs

## Applying the Naive Bayes classifier: Spam filtering

-   Without any knowledge of an incoming mail, the best estimate of whether or not it is spam would be *P(spam)*, i.e., the probability that any prior message is spam (= class prior probability, e.g., 20%)

. . .

-   The algorithm now "learns" (based on the document-feature matrix) that the word "viagra" can often be found in spam mails. This probability is known as the likelihood *P(viagra\|spam)* (= predictor prior probability, e.g., .20%)

. . .

-   The algorithm also "learns" the probability of the word "viagra" appearing in any mail *P(viagra)* (likelihood, e.g. 5%). Applying the Bayes Theorem, we get:

. . .

<center>$P(spam|viagra) = \frac{P(viagra|spam) * P(spam)}{P(viagra)} = \frac{.20 * .20}{.05} = .80$</center>

## Overview: Naive Bayes

### Strengths

-   Simple, fast, very effective
-   Does well with noisy and missing data
-   Requires relatively few examples for training, but also works with large numbers of examples
-   Easy to obtain the estimated probability of an estimation

### Weaknesses

-   Relies on an often-faulty assumption that all features are equally important and independent
-   Not ideal for data sets with many numeric features
-   Estimated probabilities are less reliable than predicted classes \|

<br>

*Lantz, 2013*

## Support Vector Machines

-   ![](https://upload.wikimedia.org/wikipedia/commons/7/72/SVM_margin.png){style="float: right; padding-left: 40px;" width="600"}Very often used machine learning method

-   Can be imagined as a "surface" that creates a boundary between points of data plotted in a multidimensional space representing examples and their feature values

-   Tries to find decision boundary between points that maximizes margin between classes while minimizing errors

-   More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space

## Overview: Support Vector Machines

### Strengths

-   Can be use for classification or numeric prediction
-   Not overly influenced by noisy data, not prone to overfitting\
-   Easier to use than neural networks
-   Often high accuracy

### Weaknesses

-   Finding the best model requires testing of various combinations of model parameters
-   Can be slow to train, particularly with many features
-   Results in a complex 'black box' that is difficult, if not impossible to understand

<br>

*Lantz, 2013*

## Neural networks

-   ![](img/neuralnetwork.png){style="float: right; padding-left: 40px;" width="400"}Inspired by human brain (but abstracted to mathematical model)

-   Each 'neuron' is a linear model with activation function: <br><br>$y = f(w_1x_1 + … + w_nx_n)$<br><br>

-   Normal activation functions: logistic, linear, block, tanh, ...

-   Each neuron is practically a generalized linear model

-   Networks differ with regard to three main characteristics:

    -   The number of (hidden) layers
    -   Whether information is allowed to travel backward
    -   The number of nodes within each layer

## Neural networks: Hidden layers

-   ![](img/neuralnetwork2.png){style="float: right; padding-left: 40px;" width="400"}Simple model cannot combine features
-   But we can add "hidden" layers (latent variables)
-   Estimated iteratively via backpropagation<br><br>1. Start with random assignments<br>2. Optimize final layer to predict output<br>3. Optimize second layer (etc) given 'best' final layer<br>4. Repeat from 2 until converged

<br>

**Universal approximator:** Neural Networks with single hidden layer can represent every continuous function

## Overview: Neural Networks

### Strengths

-   Can be adapted to any prediction problem
-   Capable of modelling more complex patterns than nearly any algorithm\
-   Makes few assumptions about the data's underlying assumptions

### Weaknesses

-   Extremely computationally intensive and thus slow
-   Very prone to overfitting
-   Results in a complex 'black box' that is difficult, if not impossible to interpret

<br>

*Lantz, 2013*

## How to know which performs best?

-   Sufficiently complex algorithms can "predict" all training data perfectly

-   But such an algorithm does not generalize to new data

-   Essentially, we want the model to have a good fit to the data, but we also want it to optimize on things that are specific to the training data set

-   Problem of under- vs- overfit

## Statistical modeling

```{r, echo = F, fig.width = 10, fig.height = 5.5}
library(tidyverse)
library(ggthemes)
set.seed(42)
x <- runif(15, 1, 7)
y <- x^3 + rnorm(15, 0, 10)


ggplot(NULL, aes(x = x, y = y)) +
  geom_point() +
  theme_get() +
  labs(subtitle = "data: y = x^3  + e",
       x = "", y = "") +
  theme(plot.margin = unit(c(0, 3, 0, 3), units = "cm"))
```

## Linear fit (underfit)

```{r, echo = F, fig.width = 10, fig.height = 5.5}
ggplot(NULL, aes(x = x, y = y)) +
  geom_point() +
  theme_get() +
  geom_smooth(method = "lm", se = F) +
  labs(subtitle = "data: y = x^3  + e, linear regression",
       x = "", y = "")+
  theme(plot.margin = unit(c(0, 3, 0, 3), units = "cm"))
```

## Good fit

```{r, echo = F, fig.width = 10, fig.height = 5.5}
ggplot(NULL, aes(x = x, y = y)) +
  geom_point() +
  theme_get() +
  stat_smooth(formula = y ~ x^3, se = F) +
  labs(subtitle = "data: y = x^3  + e, exponential fit",
       x = "", y = "")+
  theme(plot.margin = unit(c(0, 3, 0, 3), units = "cm"))
```

## Overfit

```{r, echo = F, fig.width = 10, fig.height = 5.5}
ggplot(NULL, aes(x = x, y = y)) +
  geom_point() +
  theme_get() +
  stat_smooth(method="lm", se = F, formula=y ~ poly(x, 9)) +
  labs(subtitle = "data: y = x^3  + e, fit with 9-degree polynomial",
       x = "", y = "")+
  theme(plot.margin = unit(c(0, 3, 0, 3), units = "cm"))
```

## Preventing overfitting in machine learning

-   Regularization

    -   When fitting a model, 'punish' complexity / flexibility

-   Out-of-sample validation detects overfitting

    -   To see whether a model generalizes to new data, simply test it on new data!

-   In sum, we need to validate our new classifier on unseen data

# Validation {background-color="steelblue"}

Best practices and processes.

## Typical Machine Learning Process: Training vs. Testing

-   Models (almost) always overfit: performance on training data is not a good indicator of real quality

-   Solution

    -   Split data into train and test sets (there are different ways of doing this, like split-half, leave-1-out, or k-fold)
    -   Train model on training data
    -   Test model on unseen test data
    -   We can again estimate accuracy, precision, recall, and F-Score (see last lecture)

. . .

-   So... why don't we do this with statistics?

    -   Less complex models, so less risk of overfitting (but it's still a risk!)
    -   Less focus on prediction
    -   But we sometimes could, and should use this approach!

## Example: Predicting music genre from lyrics

-   This data is scraped from the "Vagalume" website, so it depends on their storing and sharing millions of song lyrics (not really representative or complete)

-   Many different songs, but not all types of music are represented in this data set

```{r, echo = T}
#| output-location: column-fragment
s <- read_csv("data/lyrics-data.csv")
a <- read_csv("data/artists-data.csv")

a <- a %>%
  group_by(Artist) %>%
  filter(row_number()==1) %>%
  rename(ALink = Link)

d <- left_join(s, a) %>%
  unique %>% group_by(SLink) %>%
  filter(row_number() == 1) %>%
  filter(!is.na(Genre))
d
```

## Understanding the data set

::: {.column width="45%"}
-   Contains artist name, song name, lyrics, and genre of the artist (not the song)

-   The following genres are in the data set:

    -   Rock
    -   Hip Hop
    -   Pop Music
    -   Sertanejo (Basically the Brazilian version of Country Music)
    -   Funk Carioca (Originated in 60s US Funk, a completely different genre in Brazil nowadays)
    -   Samba (Typical Brazilian music)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r, echo = T}
#| output-location: fragment
#| fig-width: 4
#| fig.height: 3
d %>%
  group_by(Genre) %>% 
  tally %>%
  ggplot(aes(x = reorder(Genre, n), 
             y = n, fill = Genre)) +
  geom_col(stat = "identity") +
  scale_fill_brewer(palette = "Pastel2") +
  coord_flip() +
  theme_classic() +
  guides(fill = F) +
  labs(x = "", 
       y = "number of songs", 
       title = "Songs per Genre")
```
:::

## How is the data stored and encoded?

```{r, echo = T, R.options = list(width = 140)}
d %>%
  ungroup %>%
  filter(Artist == "Britney Spears" & SName == "...Baby One More Time") %>%
  select(Artist, SName, Lyric, Genre)
```

<br>

```{r, echo = T, R.options = list(width = 140)}
d %>%
  ungroup %>%
  filter(Artist == "Drake" & SName == "God's Plan") %>%
  select(Artist, SName, Lyric, Genre)
```

## Note: Dealing with non-determinism

-   Many machine learning algorithms are non-deterministic

-   Random initial state and/or random parameter improvements - Even deterministic algorithm require random data split

-   Problem: research is not replicable, outcome may be affected

-   For replicability: set random seed in R: `set.seed(123)`

-   For valid outcome: repeat X times and report average performance

## Creating a corpus

```{r}
#| echo: true
#| output-location: fragment
library(quanteda)
library(quanteda.textmodels)
d <- d %>%
  filter(Genre == "Rock" | Genre == "Pop" | Genre == "Hip Hop")

music <- corpus(d, docid_field = "SLink", text_field = "Lyric")
music
```

## Splitting the data into a train and a test set

::: {.column width="63%"}
```{r}
#| echo: true
# Set seed to insure replicability
set.seed(42)

# Sample rows for testset and create subsets
testset <- sample(docnames(music), nrow(d)/2)
music_test <-  music %>% 
  corpus_subset(docnames(music) %in% testset)
music_train <- music %>% 
  corpus_subset(!docnames(music) %in% testset)

# Define outcome variable for each set
genre_train <- as.factor(docvars(music_train, "Genre"))
genre_test <- as.factor(docvars(music_test, "Genre"))
```
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
-   The procedure is always the same

-   Split data into train and test set

    -   split-half (what we do here)
    -   leave-1-out
    -   k-fold

-   We store the outcome variable (`Genre`) for each subset
:::

## Text preprocessing (remember lecture 3?)

::: {.column width="45%"}
-   Step 1: Tokenization (including removing 'noise') and normalization

-   Step 2: Removing stop words

-   Step 3: Stemming

-   Step 4: Create document-feature matrix (DFM)

-   Step 5: Remove too short (\< 2 characters) and rare words

-   (Step 6: Transforms the dtm so that words with a high document frequency weight less)
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r}
#| echo: true
dfm_train <- music_train %>% 
  tokens(remove_punct = T, 
         remove_numbers = T, 
         remove_symbols = T) %>%
  tokens_tolower %>%
  tokens_remove(stopwords('en')) %>%
  tokens_wordstem %>%
  dfm %>%
  dfm_select(min_nchar = 2) %>% 
  dfm_trim(min_docfreq=20) %>%
  dfm_tfidf()  ## weighting process
```
:::

## Choose algorithm and train model

```{r, R.options = list(width = 120)}
#| echo: true
#| output-location: fragment
library(quanteda.textmodels)
m_nb <- textmodel_nb(x = dfm_train, y = genre_train) ## Naive Bayes
summary(m_nb)
```

## Predict genre in test set using the algorithm

::: {.column width="45%"}
-   To see how well the model does, we test it on the test (held-out) data

-   For this, it is important that the test data uses the same features (vocabulary) as the training data

-   The model contains parameters for these features, not for words that only occur in the test data

-   In other words, we have to "match" or "align" the train and test data

    -   Same textprocessing
    -   Matching of the features
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r}
#| echo: true
#| output-location: fragment
# Matching
dfm_test <- music_test %>% 
  tokens(remove_punct = T, 
         remove_numbers = T, 
         remove_symbols = T) %>%
  tokens_remove(stopwords('en')) %>%
  tokens_wordstem %>%
  dfm %>% 
  dfm_match(featnames(dfm_train)) %>% 
  dfm_tfidf()

# Actual prediction
nb_pred <- predict(m_nb, newdata = dfm_test)
head(nb_pred, 2)
```
:::

## Evaluating the Prediction

::: {.column width="45%"}
-   As we can see in the confusion matrix, there are a lot of false positives and false negatives!

-   **Overall Accuracy**: `r papaja::printnum(mean(nb_pred == genre_test)*100)`%

-   Precision, Recall and F1-Score are not too good for each genre

    -   Precision is slightly better for Rock and Pop,
    -   Recall is better for Hip Hop
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r, echo = T, R.options = list(width = 120)}
#| output-location: fragment
library(caret)
cm_nb <- confusionMatrix(nb_pred, genre_test)
cm_nb$table
```

<br>

```{r, echo = T, R.options = list(width = 120)}
#| output-location: fragment
(cm_nb2 <- cm_nb$byClass %>%
  as.data.frame %>%
  rownames_to_column("Genre") %>%
  select(Genre, Precision, Recall, F1) %>%
  as.data.frame %>%
  mutate_if(is.numeric, round, 2))

```
:::

## Different algorithm = better results?

::: {.column width="45%"}
```{r, echo = T}
m_svm <- textmodel_svm(x = dfm_train, y = genre_train, type = 2) 
svm_pred <- predict(m_svm, newdata = dfm_test)
cm_svm <- confusionMatrix(svm_pred, genre_test)
```

-   When we refit the model with support vector machines, there are still a lot of false positives and false negatives

-   **Overall Accuracy**: `r papaja::printnum(mean(svm_pred == genre_test)*100)`%

-   However, Precision, Recall and F1-Score all have improved!
:::

::: {.column width="5%"}
:::

::: {.column width="45%"}
```{r, echo = T, R.options = list(width = 120)}
#| output-location: fragment
cm_svm <- confusionMatrix(svm_pred, genre_test)
cm_svm$table
```

```{r, echo = T, R.options = list(width = 120)}
#| output-location: fragment
(cm_svm2 <- cm_svm$byClass %>%
  as.data.frame %>%
  rownames_to_column("Genre") %>%
  select(Genre, Precision, Recall, F1) %>%
  as.data.frame %>%
  mutate_if(is.numeric, round, 2))
```
:::

## Comparison between Algorithms

```{r, echo = T, R.options = list(width = 120)}
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 3
bind_rows(cm_nb2, cm_svm2) %>%
  bind_cols(Model = c(rep("Naive Bayes", 3), 
                      rep("SVM", 3))) %>%
  pivot_longer(Precision:F1) %>%
  ggplot(aes(x = Genre, 
             y = value, 
             fill = Model)) +
  geom_bar(stat= "identity",
           position = "dodge", 
           color = "white") +
  scale_fill_brewer(palette = "Pastel1") +
  facet_wrap(~name) +
  coord_flip() +
  ylim(0, 1) +
  theme_grey() +
  theme(legend.position = "bottom")

```

## Drivers of model performance

1.  Task difficulty

2.  Amount of training data

3.  Choice of features (n-grams, lemmata, etc)

4.  Text preprocessing (e.g., exclude or include stopwords?)

5.  Tuning of algorithm (if required)

## What is the effect of text preprocessing on model performance?

![](img/scharkow_table3.png)

*Scharkow, 2013*

# Examples from the literature {background-color="steelblue"}

How is this used in research?

## Example 1: Validating different approaches

-   Van Atteveldt et al. (2021) re-analysised data reported in Boukes et al. (2020) to understanding the validity of different text classification approaches

-   The data incldued news from a total of ten newspapers and five websites published between February 1 and July 7, 2015:

    -   three quality newspapers (NRC Handelsblad, Trouw, de Volkskrant)
    -   a financial newspaper (Financieel Dagblad)
    -   three popular newspapers (Algemeen Dagblad, Metro, De Telegraaf)
    -   three regional outlets (Dagblad van het Noorden, de Gelderlander, Noordhollands Dagblad)

## Methods

-   They analyzed the paper using different methods and compared the results

    -   Gold standard (manual coding by the three authors)
    -   Manual coding (1 or 3 coders)
    -   Crowd-Coding (1, 3 or 5 coders on a online platform)
    -   Sentiment dictionaries (various versions)
    -   Different supervised machine learning algorithms (NB, SVM, convolutional neural networks)

-   Investigated performance results of all models

## Main results

![](img/vanatteveldt_table2.png)

## So which method is valid?

-   Manual coding still outperforms all other approaches

-   Supervised text classification (particularly deep learning) is better than dictionary approaches (not too surprising)

-   Particularly supervised learning gets better with more training data (more is more!)

-   Nonetheless strongly depends on quality of training data

-   Recommendation for dictionary: Apply any applicable off-the-shelf dictionaries and if any of these is sufficiently valid as determined by comparison with the gold standard, use this for the text analysis

    -   Dictionaries give very good transparency
    -   Replicability for a low cost

## Example 2: Incivility in Facebook comments

-   Study examined the extent and patterns of incivility in the comment sections of 42 US news outlets' Facebook pages in 2015--2016

-   News source outlets included

    -   National-news outlets (e.g., ABC, CBS, CNN...)
    -   Local-new outlets (e.g., The Denver Post, San Francisco Chronicle...)
    -   Conservative and liberal partisan news outlets (e.g., Breitbart, The Daily Show...)

-   Implemented a combination of manual coding and supervised machine learning to code comments with regard to:

    -   Civility
    -   Interpersonal rudeness
    -   Personal rudeness
    -   Impersonal extreme incivility
    -   Personal extreme incivility

## Results: Incivility over time

::: {.column width="30%"}
-   Despite several discernible spikes, the percentage of extremely uncivil personal comments on national-news outlets' pages shifted only modestly

-   On conservative outlets' Facebook pages, the proportions of both extremely uncivil and rude comments fluctuated dramatically across the sampling window

*Su et al., 2018*
:::

::: {.column width="5%"}
:::

::: {.column width="60%"}
![](img/su_fig1.png)
:::

## Overall differences

![](img/sug_fig2.png)

# Outlook and Conclusion {background-color="steelblue"}

## Deep learning

-   Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning

-   Learning can again be supervised, semi-supervised or unsupervised

-   Generally refers to large neural networks with many hidden layers

    -   Possible because of modern computing power, large training sets
    -   Powers e.g. automatic translation, self-driving cars, chess computers, etc.

-   Originally developed to deal with image recognition, now also adapted for text analysis

-   Use the combination of words, bi-grams, word-embeddings, rather than feature frequencies

## Deep Learning Neural Networks

![](img/deep-learning.jpg)

## Example: TV gender representation

::: {layout="[60, 50]"}
![](img/juergen_face.png)

![](img/juergen_face2.png)
:::


## BERT language models

- Bidirectional Encoder Representations from Transformers (BERT)

- A machine learning technique for natural language processing, pre-training and developed by Google

- A deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection

- BERT is pre-trained on two different tasks: Masked Language Modeling and Next Sentence Prediction.

    - Masked Language Model training is to hide a word in a sentence and then have the program predict what word has been hidden (masked) based on the hidden word's context
    - Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random

## BERT language models - Schematic Overview

![](https://miro.medium.com/max/1400/1*p4LFBwyHtCw_Qq9paDampA.png)


## BERT in R?

::: {.column width="50%"}

- There is a new package (very recently released) that allows to use such pre-trained, large scale language models in R

- If you are interested check the package "text": https://r-text.org/

- But: Be mindful! Running a BERT model can take a long time and might even require a more powerful computer than yours!

:::

::: {.column width="2%"}
:::

::: {.column width="45%"}


![](https://r-text.org/reference/figures/text_logo_animation.gif)
:::

## Is machine learning really useful?

-   Is it okay to use a model we can't possibly understand?

-   Machine learning in the social sciences generally used to solve an engineering problem

-   Output of Machine Learning is input for "actual" statistical model (e.g., we classify text, but run an analysis of variance with the output)

## Conclusion

-   Machine learning is a useful tool for generalizing from sample

-   It is very useful to reduce the amount of manual coding needed

-   Many different models exist (each with many parameters/options)

-   We always need to validate model on unseen and representative test data!

# Thank you for your attention! {background-color="steelblue"}

## Required Reading

<br><br>

van Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.. (2021). The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms. Communication Methods and Measures, (15)2, 121-140, https://doi.org/10.1080/19312458.2020.1869198

Su, L. Y.-F., Xenos, M. A., Rose, K. M., Wirz, C., Scheufele, D. A., & Brossard, D. (2018). Uncivil and personal? Comparing patterns of incivility in comments on the Facebook pages of news outlets. New Media & Society, 20(10), 3678--3699. https://doi.org/10.1177/1461444818757205

<br>

*(available on Canvas)*

------------------------------------------------------------------------

## References {.smaller}

-   Boumans, J. W., & Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital journalism, 4(1), 8-23.

-   Günther, E. , & Domahidi, E. (2017). What Communication Scholars Write About: An Analysis of 80 Years of Research in High-Impact Journals. International Journal of Communication 11(2017), 3051--3071

-   Hvitfeld, E. & Silge, J. (2021). Supervised Machine Learning for Text Analysis in R. CRC Press. https://smltar.com/

-   Jürgens, P., Meltzer, C., & Scharkow, M. (2021, in press). Age and Gender Representation on German TV: A Longitudinal Computational Analysis. Computational Communication Research.

-   Lantz, B. (2013). Machine learning in R. Packt Publishing Ltd.

-   Scharkow, M. (2013). Thematic content analysis using supervised machine learning: An empirical evaluation using german online news. Quality & Quantity, 47(2), 761--773. https://doi.org/10.1007/s11135-011-9545-7

-   Su, L. Y.-F., Xenos, M. A., Rose, K. M., Wirz, C., Scheufele, D. A., & Brossard, D. (2018). Uncivil and personal? Comparing patterns of incivility in comments on the Facebook pages of news outlets. New Media & Society, 20(10), 3678--3699. https://doi.org/10.1177/1461444818757205

-   van Atteveldt, W., van der Velden, M. A. C. G., & Boukes, M.. (2021). The Validity of Sentiment Analysis: Comparing Manual Annotation, Crowd-Coding, Dictionary Approaches, and Machine Learning Algorithms. Communication Methods and Measures, (15)2, 121-140, https://doi.org/10.1080/19312458.2020.1869198

## Example Exam Question (Multiple Choice)

Van Atteveldt and colleagues (2020) tested the validity of various automated text analysis approaches. What was their main result?

<br>

A. English dictionaries performed better than Dutch dictionaries in classifying the sentiment of Dutch news paper headlines.

B. Dictionary approaches were as good as machine learning approaches in classifying the sentiment of Dutch news paper headlines.

C. Of all automated approaches, supervised machine learning approaches performed the best in classifying the sentiment of Dutch news paper headlines.

D. Manual coding and supervised machine learning approaches performed similarly well in classifying the sentiment of Dutch news paper headlines.

------------------------------------------------------------------------

## Example Exam Question (Multiple Choice)

Van Atteveldt and colleagues (2020) tested the validity of various automated text analysis approaches. What was their main result?

<br>

A. English dictionaries performed better than Dutch dictionaries in classifying the sentiment of Dutch news paper headlines.

B. Dictionary approaches were as good as machine learning approaches in classifying the sentiment of Dutch news paper headlines.

**C. Of all automated approaches, supervised machine learning approaches performed the best in classifying the sentiment of Dutch news paper headlines.**

D. Manual coding and supervised machine learning approaches performed similarly well in classifying the sentiment of Dutch news paper headlines.

## Example Exam Question (Open Format)

Describe the typical process used in supervised text classification.

. . .

Any supervised machine learning procedure to analyze text usually contains at least 4 steps:

1.  One has to manually code a small set of documents for whatever variable(s) you care about (e.g., topics, sentiment, source,...).

2.  One has to train a machine learning model on the hand-coded /gold-standard data, using the variable as the outcome of interest and the text features of the documents as the predictors.

3.  One has to evaluate the effectiveness of the machine learning model via cross-validation. This means one has to test the model test on new (held-out) data.

4.  Once one has trained a model with sufficient predictive accuracy, precision and recall, one can apply the model to more documents that have never been hand-coded or use it for the purpose it was designed for (e.g., a spam filter detection software)
