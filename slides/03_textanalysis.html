<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Computational Analysis of Digital Communication</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: left, middle, title-slide

# Computational Analysis of Digital Communication
## Week 3: Basics of Automatic Text Analysis and Dictionary Approaches
### 
### Dr. Philipp K. Masur | <a href="mailto:p.k.masur@vu.nl" class="email">p.k.masur@vu.nl</a>

---









&lt;style type="text/css"&gt;
.pull-left2 {
  float: left;
  width: 30%;
}
.pull-right2 {
  float: right;
  width: 60%;
}

.pull-left2b {
  float: left;
  width: 60%;
}
.pull-right2b {
  float: right;
  width: 30%;
}

.pull-left3 {
  float: left;
  width: 45%;
  padding-right: 5% 
}
.pull-right3 {
  float: right;
  width: 45%;
  padding-left: 5% 
}

.my-one-page-font {
  font-size: 17px;
}
&lt;/style&gt;

# Much of what we know about human behavior...

...is based on what people are telling us:

- via self-reports in surveys

- via responses in experimental research

- in qualitative interviews

---

# But a lot of (mass) communication looks like this...

![](img/nyt_page.png)
---

# ...or is based on user-generated content.

![](img/socialmedia.jpg)
---

# Increasing amount of (text) data available online

![](img/hilbert_lopez.png)

_Hilbert &amp; Lopez, 2011_

---

# Problem or opportunity?




- A lot of communication is encoded in texts

- But text does not look like data we can easily analyze...

.pull-left[

**Experimental data**


```
# A tibble: 6 x 5
     id condition sns_use well_being
  &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;
1     1 A            3.31    -3.50  
2     2 B            3.57     8.41  
3     3 A            1.53     4.54  
4     4 B            2.40     0.0963
5     5 A            1.95    -0.983 
6     6 B            4.06    -2.18  
# … with 1 more variable: pers1 &lt;dbl&gt;
```
]

.pull-right[

**Text data**


```
# A tibble: 6 x 1
  text                                  
  &lt;chr&gt;                                 
1 "North Korea launched a ballistic mis…
2 "Tributes poured in for former Republ…
3 "An Indian couple have arrived for th…
4 "Unique events that led to civilisati…
5 "Billionaire Peter Thiel is facing op…
6 "In some ways, accounts of “human ori…
```
]

---

# Traditional content/text analysis

Typical steps in a *content/text* analysis:

**1.** Selecting the content one wants to analyze

**2.** Choosing the texts that contain the content and one wants to analyze

**3.** Define the units and categories of analysis

**4.** Develop a set of rules for the manual coding process

**5.** Coding the text according to the rules

**6.** Analyze frequencies, relationships, differences, similarities between units/codes

--
&lt;br&gt;&lt;br&gt;

**Problem:** Requires a lot of work and there are always more texts than humans can possibly code manually!

---

# Solution: Automation


&lt;iframe width="760" height="415" src="https://www.youtube.com/embed/DfGs2Y5WJ14?autoplay=1&amp;controls=0&amp;amp;start=19&amp;mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;


---

# Content of this lecture

&lt;br&gt;

**1.** Text as Data

**2.** Automated Text Analysis

**3.** Deductive Approaches: Dictionary-based Text Analysis

**4.** Examples from the literature

---

class: inverse, center, middle

# Text as Data

How can we analyze texts with computers?

---

# Definition

.pull-left[
&gt; Text analysis is "a research technique for making replicable and valid inferences from texts (or other meaningful matter) to the contexts of their use" 
&gt;
&gt;Krippendorff, 2004

]

.pull-right2b[

![](https://www.asc.upenn.edu/sites/default/files/styles/360x360/public/2020-09/Klaus_Krippendorff_360_0.jpg?h=5273c5c2&amp;itok=d-IFcUP8)

]

---

# What is text?


.pull-left[

![](img/symbols.jpg)

]

.pull-right[

![](img/symbols2.png)

]

---

# Symbols and Meaning

- Text consists of *symbols*

- Symbols by themselves do not have meaning

- A symbol itself is a mark, sign, or word that indicates, signifies, or is understood as representing an idea, object, or relationship

- Symbols thereby allow people to go beyond what is known or seen by creating linkages between otherwise very different concepts and experiences 

- Text (a collection of symbols) only attains meaning when interpreted (in its context)

- Main challenge in Automatic Text Analysis: Bridge the gap from symbols to meaningful interpretation

---

# Understanding language

&lt;br&gt;


.pull-left2b[

&gt;"As natural language processing (NLP) practitioners, we bring our assumptions about what language is and how language works into the task of creating modeling features from natural language and using those features as inputs to statistical models. This is true even when we don't think about how language works very deeply or when our understanding is unsophisticated or inaccurate; speaking a language is not the same as having an explicit knowledge of how that language works. We can improve our machine learning models for text by heightening that knowledge."
&gt;
&gt;_Hvitfeldt &amp; Silge, 2021_

]

.pull-right2b[

![](img/hvitfeld_book.png)
]

---

# A short overview of linguistics

.pull-left[


- Each field studies a *different level* at which language exhibits organization

- When we engage in text analysis, we use these levels of organization to create language features (e.g., tokens, n-grams,...)

- They often depend on the morphological characteristics of language, such as when text is broken into sequences of characters, words, sentences

_Hvitfeldt &amp; Silge, 2021, chap. 1_

]

.pull-right[

| Subfield |	What does it focus on? |
|:--------- |:---------|
| Phonetics	      |  Sounds that people use in language |
| Phonology	      |  Systems of sounds in particular languages |
| Morphology	    |  How words are formed |
| Syntax	        |  How sentences are formed from words |
| Semantics	      |  What sentences mean |
| Pragmatics	    |  How language is used in context |

]


---

class: inverse, center, middle

# Automatic Text Analysis

How we can study large quantities of documents and texts?

---

# Steps in Automatic Text Analysis

&lt;br&gt;&lt;br&gt;

![](img/textanalysis.png)
&lt;br&gt;&lt;br&gt;

_van Atteveldt, Welbers, &amp; Van der Velden, 2019_
---

# (1) Obtaining texts

- Publicly available data sets
  - e.g.: Political texts, news from publisher / library
  - Great if you can find it, often not available

- Scraping primary sources
  - e.g.: Press releases from party website, existing archives
  - Writing scrapers can be trivial or very complex depending on web site
  - Make sure to check legal issues

- Proprietary texts from third parties:
  - e.g.: digital archives (LexisNexis, factiva etc.), social media APIs
  - Often custom format, API restrictions, API changes
  - Terms of use not conducive to research, sharing

---

# (2) From text to data

- Algorithms (or generally R) process numbers, they do not read text

- First step in any analysis is to convert text to series of numbers

- This is done through a number of steps (also know as preprocessing) which include: 

    - Tokenization 
    - Removing stopwords 
    - Normalization 
    - Frequency trimming

- The resulting structured text is then often used to create a document-feature matrix (DTM)

    - Table containing frequency of each word in each document
    - Called “bag of words” approach -&gt; ignores word order

---

# Text cleaning, stemming, lemmatizing

- Text contains a lot of noise
    - Very uncommon words
    - Spelling, scraping mistakes (HTML code, boilerplate, etc)
    - Stop words (e.g., a, the, I, will)
    - Conjugations of the same word (want, wants)
    - Near synonyms (wants, loves)
    - Of course, what is noise depends on your RQ!

- Cleaning steps needed to reduce noise:
    - Removing unnecessary symbols (e.g., punctuations, numbers...)
    - Removing stopwords (e.g., 'a', 'the'...)
    - Normalization: Stemming (wants -&gt; want) OR lemmatizing (ran -&gt; run)
    - Frequency trimming (removing rare words)
    
---

# Tokenization

- In a first step, we need to break text down into the features that we want to analyze (so-called tokens)

- We take an input (a string) and a token type (a meaningful unit of text, such as a word) and we split the input (string) into pieces (tokens) that correspond to the type (e.g., word) 


```r
library(quanteda)
text &lt;- "This is an Example of Preprocessing Techniques. This is what happens during the preprocessing procedure!1!"
text
```

```
[1] "This is an Example of Preprocessing Techniques. This is what happens during the preprocessing procedure!1!"
```

--


```r
toks &lt;- tokens(text, remove_punct = TRUE, remove_numbers = TRUE) # tokenize into unigrams, remove punctuation
toks
```

```
Tokens consisting of 1 document.
text1 :
 [1] "This"          "is"            "an"            "Example"       "of"            "Preprocessing" "Techniques"   
 [8] "This"          "is"            "what"          "happens"       "during"       
[ ... and 3 more ]
```

_Manning, Raghavan, and Schütze 2008_

---

# Types of Tokens

- Thinking of a token as a **word** is a useful start (and most used approach -&gt; bag-of-words)

- However, we can generalize the idea of a token beyond only a single word to other units of text:

    - characters ("I", "l", "o", "v", "e", "y", "o", "u")
    - words  ("I", "love", "you")
    - sentences ("I love you")
    - lines ("He went to her. I love")
    - paragraphs, and
    - n-grams (e.g., "I love")

&lt;br&gt;&lt;br&gt;

_Hvitfeldt &amp; Silge, 2021, chap. 2.2_

---

# Removing stopwords

- Once we have split text into tokens, it often becomes clear that not all words carry the same amount of information

- Common words that carry little (or perhaps no) meaningful information are called **stop words** (e.g., 'a', 'the', 'didn't', 'of'...)

- It is common advice and practice to remove stop words for various text analysis tasks, but stop word removal is more nuanced than many resources may lead you to believe



```r
# removing stopwords
toks &lt;- tokens_remove(toks, stopwords("en"))
toks
```

```
Tokens consisting of 1 document.
text1 :
[1] "Example"       "Preprocessing" "Techniques"    "happens"      
[5] "preprocessing" "procedure"    
```

_Hvitfeldt &amp; Silge, 2021, chap. 2.2_

---

# Normalization

- Before we move to the more sophisticated normalization, including stemming or lemmatizing, we need to align similar words. 

- R does differ between lower and upper case letters. We hence need to transfrom to lower case to make words comparable. 


```r
toks &lt;- tokens_tolower(toks) # lowercasing
toks
```

```
Tokens consisting of 1 document.
text1 :
[1] "example"       "preprocessing" "techniques"    "happens"      
[5] "preprocessing" "procedure"    
```

---

# Stemming

- What if we aren’t interested in the difference between e.g., "trees" and "tree" and we want to treat both together? 

- Stemming refers to the process of identifying the base word (or stem) for a data set of words and is thus concerned with the linguistics subfield of morphology (i.e., how words are formed). 


```r
toks # original tokens
```

```
Tokens consisting of 1 document.
text1 :
[1] "example"       "preprocessing" "techniques"    "happens"      
[5] "preprocessing" "procedure"    
```

--


```r
final_toks &lt;- tokens_wordstem(toks) 
final_toks # after stemming
```

```
Tokens consisting of 1 document.
text1 :
[1] "exampl"     "preprocess" "techniqu"   "happen"     "preprocess"
[6] "procedur"  
```

---

# Lemmatization

- Instead of using set rules to cut words down to their stems, lemmatization uses knowledge about a language's structure to reduce words down to their lemmas, the canonical or dictionary forms of words

- Lemmatizers often use a rich lexical database like 'WordNet' as a way to look up word meanings for a given part-of-speech use (Miller 1995)



```r
toks # original tokens
```

```
Tokens consisting of 1 document.
text1 :
[1] "example"       "preprocessing" "techniques"    "happens"      
[5] "preprocessing" "procedure"    
```

--


```r
# Lemmatization
tokens_replace(toks, pattern = lexicon::hash_lemmas$token, replacement = lexicon::hash_lemmas$lemma)
```

```
Tokens consisting of 1 document.
text1 :
[1] "example"    "preprocess" "technique"  "happen"     "preprocess"
[6] "procedure" 
```


---

# Document-term matrix or document-feature matrix

- Finally, we create a representation of these "tokens" (or terms or features) 

- A document-feature matrix refers to a mathematical matrix that describes the frequency of terms that occur in a collection of documents

- In this matrix, each row corresponds to a document in the collection and columns correspond to terms.

- Each cell contains the number of times a term is in that particular document


```r
dtm &lt;- dfm(final_toks)
dtm
```

```
Document-feature matrix of: 1 document, 5 features (0.00% sparse) and 0 docvars.
       features
docs    exampl preprocess techniqu happen procedur
  text1      1          2        1      1        1
```

---

# Word frequencies and summaries



```r
library(quanteda.textstats)
textstat_frequency(dfm(toks), n = 10)  
```

```
        feature frequency rank docfreq group
1 preprocessing         2    1       1   all
2       example         1    2       1   all
3    techniques         1    2       1   all
4       happens         1    2       1   all
5     procedure         1    2       1   all
```

```r
textstat_summary(dfm(toks))
```

```
  document chars sents tokens types puncts numbers symbols urls tags emojis
1    text1    NA    NA      6     5      0       0       0    0    0      0
```


---
# Word frequencies

![](img/wordcloud1.png)

---

# (3) Analyzing the structured data

.pull-left[
- Rule-based analyses (deductive approaches)
    - Assigns meaning by researcher
    - “if word X occurs, the text means Y”
    
- Supervised machine learning (inductive approaches)
    - Train a model on coded training examples
    - Generalizes meaning in human coding of training material
    - “Text X is like other texts that were negative, so X is probably negative”
]

.pull-right[
- Unsupervised machine learning (inductive approaches)
    - Find clusters of words that co-occur
    - Meaning is assigned afterwards by researcher interpretation
    - “These words form a pattern, which I think means X”
    
    
&lt;br&gt;&lt;br&gt;&lt;br&gt;

_Boumans &amp; Trilling, 2016_
]



---

# (4) Evaluating the validity of the analysis

- Many text analysis processes are 'black boxes'
    - even manual coding
    - dictionaries are ultimately opaque

- Computer does not 'understand' natural language 

- Need to prove to reader that analysis is valid
    - Validate by comparing to known good 
    - Comparison: often manual annotation of 'gold standard'

--

&lt;br&gt;&lt;br&gt;

**Note:** We will talk about different validation approaches when we discuss rule-based and supervised learning methods

---


class: inverse, center, middle

# Deductive Approaches: Dictionary/Lexical Analysis

Using fixed set of terms per concept to automatically code texts.

---

# What are deductive approaches?

- Code rules are set *a priori* based on a predefined "text theory"

- Computer uses these rules to decode text in a deterministic way

- Rules can differ substantially:
    - based on individual words or group of words (e.g., articles that contain "government" are coded as "politics")
    - based on patterns (e.g., the sender of a mail can be identified by looking for "FROM:")
    - combinations of both 


---
# Basic idea of dictionary approaches

.pull-left[

- Quite old technique of content analysis (since 60s)

- Dictionaries or lexicons contain a lot of information about links between words and meanings

- We define a word list for every category that we want to code

- Computer searches for the existence of these words in text
      - if the word exist, assign the 'code' to that document (or sentence)
      - e.g. positive = {good, happy, nice}, negative = {bad, stupid, expensive}


]

.pull-right[
![](img/lexicon.jpg)
]


---
# Advantages and disadvantages

- Advantages: 
    - Technically easy, many dictionaries exists
    - Transparent and replicable, if dictionary is shared
    - Few resources needed, efficient

- Disadvantages:
    - Low validity for non-trivial concepts (sentiment, frames, specific topics)
        - categories needs be identifiable by simple word lists!
    - May require considerable preprocessing to reduce ambiguity
    - Difficult to create/maintain large dictionaries
    - Can encode biases 

---

# How can we develop a dictionary?

- Define categories

- Inspecting keywords-in-context lists

- Manually coding and comparing word frequencies per category

- Inclusion of different spellings

- Deletion of words the lead to false-positives

- Testing, testing, testing...


---

# Special case: Sentiment analysis


- Coding and counting of evaluative statements per document

- Analysis of sentiment: Positive vs. negative?

- Subjectivity analysis: Is there any sentiment at all?

- Emotional analysis: Are there any emotions?

- In most cases, a score will be computed (e.g. sentiment = positive - negative words)

- Problems: Negation, irony, descriptive rather than evaluative statements


---

# Example: The State of the Union Speech Corpus


```r
library(quanteda.corpora)
library(quanteda.dictionaries)
library(quanteda.textplots)

data_df &lt;- convert(data_corpus_sotu, "data.frame") %&gt;%
  as_tibble
data_df
```

```
# A tibble: 241 x 8
   doc_id    text                     FirstName President Date       delivery type  party 
   &lt;chr&gt;     &lt;chr&gt;                    &lt;chr&gt;     &lt;chr&gt;     &lt;date&gt;     &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt; 
 1 Washingt… "Fellow-Citizens of the… George    Washingt… 1790-01-08 spoken   SOTU  Indep…
 2 Washingt… "Fellow-Citizens of the… George    Washingt… 1790-12-08 spoken   SOTU  Indep…
 3 Washingt… "Fellow-Citizens of the… George    Washingt… 1791-10-25 spoken   SOTU  Indep…
 4 Washingt… "Fellow-Citizens of the… George    Washingt… 1792-11-06 spoken   SOTU  Indep…
 5 Washingt… "Fellow-Citizens of the… George    Washingt… 1793-12-03 spoken   SOTU  Indep…
 6 Washingt… "Fellow-Citizens of the… George    Washingt… 1794-11-19 spoken   SOTU  Indep…
 7 Washingt… "Fellow-Citizens of the… George    Washingt… 1795-12-08 spoken   SOTU  Indep…
 8 Washingt… "Fellow-Citizens of the… George    Washingt… 1796-12-07 spoken   SOTU  Indep…
 9 Adams-17… "Gentlemen of the Senat… John      Adams     1797-11-22 spoken   SOTU  Feder…
10 Adams-17… "Gentlemen of the Senat… John      Adams     1798-12-08 spoken   SOTU  Feder…
# … with 231 more rows
```

---

# Understanding the data: Speeches per President

.pull-left[

```r
data_df %&gt;%
  group_by(President) %&gt;%
  summarize(n = n()) %&gt;%
  ggplot(aes(x = reorder(President, n), y = n)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  coord_flip() +
  theme_wsj()
```
]

.pull-right[

&lt;img src="03_textanalysis_files/figure-html/plot-label-out1-1.png" width="100%" /&gt;

]


---

# Text preprocessing: Creating a corpus


```r
corp &lt;- corpus(data_df, docid_field = "doc_id", text_field = "text")
corp 
```

```
Corpus consisting of 241 documents and 6 docvars.
Washington-1790 :
"Fellow-Citizens of the Senate and House of Representatives: ..."

Washington-1790b :
"Fellow-Citizens of the Senate and House of Representatives: ..."

Washington-1791 :
"Fellow-Citizens of the Senate and House of Representatives: ..."

Washington-1792 :
"Fellow-Citizens of the Senate and House of Representatives: ..."

Washington-1793 :
"Fellow-Citizens of the Senate and House of Representatives: ..."

Washington-1794 :
"Fellow-Citizens of the Senate and House of Representatives: ..."

[ reached max_ndoc ... 235 more documents ]
```

---

# What are the most used words in these speeches?



![](img/wordcloud2.png)


---

# How do Trump and Obama compare?

.pull-left[

&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-19-1.png" width="100%" /&gt;

]


.pull-right[
&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-20-1.png" width="100%" /&gt;

]

---

# Keyword-in-context

- We can also search for specific text passages that contain a specific word (e.g., "terror")


```r
k = kwic(corp, 'terror', window = 4)
head(k, 10)
```

```
Keyword-in-context with 10 matches.                                                                                                 
     [Monroe-1817, 2948]        will henceforth lose their | terror | . Fortifications in those  
   [Fillmore-1850, 5411]              a source of constant | terror | and annoyance to the       
    [Johnson-1867, 5594]                insecurity, by the | terror | of confiscation, and       
      [Grant-1874, 7207]          were committed to spread | terror | among those whose political
     [Arthur-1883, 5637]                 , Amphitrite, and | Terror | have been launched on      
     [Arthur-1884, 6227] double-turreted monitors Puritan, | Terror | , and Amphitrite,          
  [Cleveland-1886, 8344]             the cause of constant | terror | to the settlers of         
  [Cleveland-1893, 8537]    and the coast-defense monitors | Terror | , Puritan, Amphitrite      
 [Roosevelt-1904, 14184]            The peace of tyrannous | terror | , the peace of             
     [Wilson-1918, 3366]            saving from the German | terror | and whom we must           
```


---




# Using an exisiting dictionary: The General Sentiment Dictionary

- Dictionary with a list of positive and negative words according to the psychological Harvard-IV dictionary as used in the General Inquirer software

- This is a general-purpose dictionary developed by the Harvard University

.pull-left[

**Negative words**


```
 [1] "abandon"     "abandonment"
 [3] "abate"       "abdicate"   
 [5] "abhor"       "abject"     
 [7] "abnormal"    "abolish"    
 [9] "abominable"  "abrasive"   
[11] "abrupt"      "abscond"    
[13] "absence"     "absent"     
```
]

.pull-right[

**Positive words**


```
 [1] "abide"      "ability"   
 [3] "able"       "abound"    
 [5] "absolve"    "absorbent" 
 [7] "absorption" "abundance" 
 [9] "abundant"   "accede"    
[11] "accentuate" "accept"    
[13] "acceptable" "acceptance"
```

]

---

# Running the actual sentiment analysis

.pull-left[

- We use the text corpus containing all speeches

- We tokenize the speeches (separating them into words)

- We do not stem or lemmatize the text as dictionaries contain full words!

- We create a document-feature matrix

- We look up how often certain words are in the different text documents

- Number of 'negative' and 'positive' words per speech is the outcome of this analysis

]

--

.pull-right[

```r
dtm &lt;- corp %&gt;%    
  tokens() %&gt;%
  dfm
res &lt;- dtm %&gt;%
  dfm_lookup(dictionary(DictionaryGI))
res
```

```
Document-feature matrix of: 241 documents, 2 features (0.00% sparse) and 6 docvars.
                  features
docs               negative positive
  Washington-1790        26      126
  Washington-1790b       34      108
  Washington-1791        49      185
  Washington-1792        65      147
  Washington-1793        70      125
  Washington-1794       130      194
[ reached max_ndoc ... 235 more documents ]
```

]

---

# Combining all information to a final data set


```r
dict_results &lt;- convert(res, 'data.frame') %&gt;%
  mutate(length=ntoken(dtm))
variables &lt;- cbind(doc_id = docnames(res), docvars(res))
d &lt;- left_join(variables, dict_results) %&gt;%
  as_tibble
head(d)
```

```
# A tibble: 6 x 10
  doc_id    FirstName President  Date       delivery type  party  negative positive length
  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;      &lt;date&gt;     &lt;fct&gt;    &lt;fct&gt; &lt;fct&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;int&gt;
1 Washingt… George    Washington 1790-01-08 spoken   SOTU  Indep…       26      126   1167
2 Washingt… George    Washington 1790-12-08 spoken   SOTU  Indep…       34      108   1501
3 Washingt… George    Washington 1791-10-25 spoken   SOTU  Indep…       49      185   2471
4 Washingt… George    Washington 1792-11-06 spoken   SOTU  Indep…       65      147   2282
5 Washingt… George    Washington 1793-12-03 spoken   SOTU  Indep…       70      125   2116
6 Washingt… George    Washington 1794-11-19 spoken   SOTU  Indep…      130      194   3192
```

---

# Amount of negative and positive words per president

.pull-left[

- We can know do several frequency analyses

- For example, how many positive and negative words have president used on average?

]

.pull-right[
&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-27-1.png" width="100%" /&gt;
]

---

# Investigating Sentiment: Computing a Score

- In most sentiment analyses, researchers compute some sort of overall sentiment score

- A common score is to subtract the negative count from the positive count and divide by either by the number of sentiment words or the total number of words (length):

`\(sentiment1 = (positive - negative) / (positive + negative)\)`

`\(sentiment2 = (positive - negative) / length\)`

- We can also compute a measure of subjectivity to get an idea of how much sentiment is expressed in total:

`\(subjectivy = (positive + negative) / length)\)`



---

# Sentiment scores over time

.pull-left[

&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-28-1.png" width="100%" /&gt;


]

.pull-right[

&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-29-1.png" width="100%" /&gt;


]

---

# Subjectivity in SOTU-speeches over time

&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-30-1.png" width="100%" /&gt;


]

---

# Sentiment of different presidents

&lt;img src="03_textanalysis_files/figure-html/unnamed-chunk-31-1.png" width="100%" /&gt;

---

class: inverse, center, middle

# Reliability and Validity

Making sure the automated coding is correct!


---

# Reliability and Validity

- **Reliability** means accuracy, but also reproducibility and stability of the results

- **Validity** means that codings/classifications represent the actual theoretical concepts

- To estimate both, we manually code a random sample of the documents and compare the coding with the dictionary results

- General procedure

    - Draw a random subsample of the documents
    - Code actual documents with human coders (the 'gold standard')
    - Combine the manual coding results with the results from the automated sentiment analysis
    - Produce reliability and validation scores, e.g., correlation between manual and automated coding, precision, recall...
    
- Agreement with manual coding, which is often regarded as the 'gold-standard', is often seen as validity (but this holds only if human beings are inerrant!)

---

# Reliability

.pull-left[
- Basis is always the confusion matrix between algorithm and manual coder that assessed the same content

- Accuracy (Holsti-Coefficient) = sum of the diagonale / sum of the entire matrix

- Disadvantages
      - No correction of random chance agreements
      - easy computable for 2 coders
      - only nominal scales
      - no accounting for missings
      
- Alternative: Cronbach's Alpha (but often seen as too strict)
]

.pull-right[
![](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png)

]
---

# Validity

- Again Accuracy: agreement between algorithm and coder

- Precision: Probability of a positively coded document is relevant

- Recall: Probability that a relevant document is coded positively

- F-Score: Mean between precision and recall

---

# Validation-Scores

.pull-left[
![](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png)

]

.pull-right[

- From the confusion matrix, we can compute all relevant scores:

  - **Precision** = TP / (TP + FP)

  - **Recall** = TP / (TP + FN)

  - **F-Score** = 2 * (Precision * Recall) / (Precision + Recall)

- No clear thresholds: need to be assessed in the research context, but values closer to 1 are desirable

- Can be used to compare models!

]
---

class: inverse, center, middle

# Examples in the literature

How is this used in research?

---

# Political migration discourses on social media


- Analyzed migration discourses in the Facebook accounts of political actors (n=1702) across six European countries (Spain, UK, Germany, Austria, Sweden and Poland)

- present new insights into the visibility of migration as a topic

- investigated sentiment about migration, revealing country- and party-specific patterns

&lt;br&gt;&lt;br&gt;

_Heidenreich et al., 2019_
    
---

# Methods

.pull-left[
- Downloaded textual data for all migration-related posts (n = 24,578) of members of parliaments (n = 1702) in six countries

![](img/heidenreich_table1.png)
]

.pull-right[

- Used automated contend analysis to estimate sentiment towards migration in each post

- Machine translated the whole corpus into English

- Used a dictionary-approach (Lexicoder; Young and Soroka, 2012) to cound positive and negative words

- Computed sentiment for each document by calculating as the sum of the scores
for all words bearing positive sentiment (Pi) minus the sum of all scores from negative
words (Ni), divided by the number of words (Wi). 

]

---

# Results: Visibility

.pull-left[

- In Germany and Austria there is indeed descriptive evidence that parties of the right discuss migration more frequently in their Facebook status posts than other parties

- Conversely, in Spain, the UK and Poland the topic tended to be more prominent in the posts of left-wing parties

- At first glance, Sweden seems to be an outlier.

- In sum there is no consistent overall pattern supporting the hypothesis that right wing parties pay more attention to the topic of migration on Facebook than left leaning parties 



]


.pull-right[
![](img/heidenreich_table2.png)
]

---
# Results: Sentiment

![](img/heidenreich_table3.png)

---

# Results: Predicting sentiment

![](img/heidenreich_table4.png)
- Best predictor was whether the politicians were either far-left or far-right!
---

# Conclusion of the study

- Migration is a more prominent in countries with positive net migration than in countries where net migration is neutral or negative

- They did not find support for the assumption that right-leaning parties talk more, and more negatively, about migration

- However, political actors from parties of the extreme left and the extreme right of the political spectrum address migration more frequently and more negatively than more moderate political players

- Potential limitations

    - How valid is the automated translation of text into English?
    - How valid is the sentiment approximation based on dictionaries?

---

class: inverse, center, middle

# Thank you for your attention!

---

# Required Reading

&lt;br&gt;&lt;br&gt;

Heidenreich, T., Eberl, J.-M., Lind, F. &amp; Boomgaarden, H. (2020). Political migration discourses on social media: a comparative perspective on visibility and sentiment across political Facebook accounts in Europe. Journal of Ethnic and Migration Studies, (46)7, 1261-1280,  https://doi.org/10.1080/1369183X.2019.1665990

&lt;br&gt;

_(available on Canvas)_


---
class: my-one-page-font

# References

- Boumans, J. W., &amp; Trilling, D. (2016). Taking stock of the toolkit: An overview of relevant automated content analysis approaches and techniques for digital journalism scholars. Digital journalism, 4(1), 8-23.

- Heidenreich, T., Eberl, J.-M., Lind, F. &amp; Boomgaarden, H. (2020). Political migration discourses on social media: a comparative perspective on visibility and sentiment across political Facebook accounts in Europe. Journal of Ethnic and Migration Studies, (46)7, 1261-1280, DOI: 10.1080/1369183X.2019.1665990

- Hilbert, M., &amp; López, P. (2011). The World’s Technological Capacity to Store, Communicate, and Compute Information. Science, 332(6025), 60 –65. https://doi.org/10.1126/science.1200970

- Hvitfeld, E. &amp; Silge, J. (2021). Supervised Machine Learning for Text Analysis in R. CRC Press. https://smltar.com/

- Krippendorff, K. (2004). Content Analysis. Sage. 

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": false,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
