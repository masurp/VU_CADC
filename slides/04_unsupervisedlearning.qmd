---
title: "Computational Analysis of Digital Communication"
subtitle: "Week 4: Unsupervised Machine Learning - Topic Modeling"
author: "Dr. Philipp K. Masur"
format:
  revealjs: 
    theme: [default, theme.scss]
    logo: img/logo.png
    background-transition: fade
    title-slide-attributes:
        data-background-image: https://storage.googleapis.com/exenzo-jobboard/bb/public/thumb/logo-jpg-57.jpg
        data-background-position: "top right"
        data-background-size: auto
editor: visual
---

![](img/machinelearning.png)


---

::: {.column width="45%"}
### Supervised learning

-   Algorithms build a model based on sample data, known as "training data", in order to make predictions or decisions without being explicitly programmed to do so

-   Combines the scalability of automatic coding with the validity of manual coding (requires pre-labeled data to train algorithm)

-   Examples:

    -   Supervised text classification, such as extending manual coding to large text corpora, sentiment analysis...
    -   Pattern recognition: e.g., face recognition, spam filter,...
:::

::: {.column width="4%"}
:::

::: {.column width="45%"}
### Unsupervised learning (next lecture)

-   Algorithm detects clusters, patterns, or associations in data that has not been labeled previously, but researcher needs to interpret results

-   Very helpful to make sense of new data (similar to cluster analysis or exploratory factor analysis)

-   Examples:

    -   Topic modeling: Extracting topics from unlabeled (text) data
    -   Customer segmentation: Better understanding different customer groups around which to build marketing or other business strategies
:::

# Computers can detect "topics"? {background-color="steelblue"}


## What are we trying to achieve?

![](img/topic_slides/Slide1.png)

## What topics can be extracted from the documents?

![](img/topic_slides/Slide2.png)


## Easy for humans...

![](img/topic_slides/Slide3.png)

## Computer can only rely on the words

![](img/topic_slides/Slide5.png)

## But each word may belong to a certain topic!

![](img/topic_slides/Slide4.png)

## Goals of topic modeling


::: {.column width="45%"}
- Topic modeling is a method for unsupervised classification of documents

- It is somewhat similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for

- The goal is to find a set of topics consisting of clusters of words that co-occur in these documents according to certain patterns

- However, the researchers must interpret the results of a topic model and there is usually more than one solution...

:::

::: {.column width="4%"}
:::

::: {.column width="45%"}

![](img/blei_lda.png)

<p style="font-size:0.75em;">_Blei, Ng, & Jordan, 2003_</p>
:::

## Content of the lecture


1. What is topic modeling?

2. Topic Models as Dimensionality Reduction

3. Latent Dirichlet Allocation - LDA topic modeling

    - Basic Principles
    - Dirichlet Distribution
    - Gibbs Sampling
    - How to determine the right number of topics

4. (More) Examples from the literature

5. Conclusion and outlook


# What is topic modeling? {background-color="steelblue"}


## Let's start with an example


::: {.column width="45%"}

- Jacobi, Welbers & Van Atteveld (2016) analyzed the coverage of *nuclear technology* from 1945 to 2013 in the New York Times

- Overall, they analyzed 51,528 news stories (headline and lead): Way too much for human coding!

- Used Latent Dirichlet Allocation (LDA) topic modeling to extract topics

- They then analyzed their occurrence over time

- It is a nice example of what topic modelling can and can't do

:::
:::{.column width="4%"}
:::
:::{.column width="45%"}


![](https://upload.wikimedia.org/wikipedia/commons/4/4e/Nuclear_Power_Plant_Cattenom.jpg)
:::

## Assumption of the model

![](img/jacobi_fig1.png)

## Characteristics of a topic model


This example highlights a number of interesting points about LDA topic modelling

1. The document is split between two main topics, *Cold War* and *Nuclear Accidents*: In a coding scheme forced to have a single topic per document, it would be very difficult to choose the dominant topic for this article.

2. Not all words are included in the analysis: Most are not used because they are non-substantive words such as determiners or prepositions ("the" or "it"), which the authors excluded, or because they are too rare ("Elbe", "perish") or too common ("have" but in this context also "nuclear", since that was used to select the articles). 

3. No a priori coding scheme was used by the computer, so the topics in this document were found completely automatically. 



## So what does a result of a Topic model look like?

::: {.column width="45%"}

- Typical table resulting from a topic model

- Contains most representative words per topic (in this case 10 topics)

- The authors then interpreted the outcome and labeled each topic accordingly


:::
::: {.column width="4%"}
:::
::: {.column width="45%"}

![](img/jacobi_table1.png)
:::


## Using topics for substantive analyses

![](img/Jacobi_topicmodelling.png)


# Topic Models as Dimensionality Reduction {background-color="steelblue"}

## Using factor analysis to extract topics?

::: {.column width="45%"}

- Problem: Texts are unstructured data and it is thus difficult to do standard statistical analysis with them

- Solution: Convert the texts into a document-term matrix, where documents represent cases and words are variables/characteristics of these cases

- Now we can use standard cluster/factor analysis techniques to reduce these dimensions?

:::
::: {.column width="4%"}
:::
::: {.column width="45%"}



![](https://www.researchgate.net/publication/338434397/figure/fig1/AS:844756024311809@1578416939705/The-results-obtained-from-Confirmatory-Factor-Analysis-CAF-for-the-APQ-P-First.png)
:::


## Dimensionality reduction using PCA

```{r, R.options = list(width = 80)}
#| echo: true
#| output-location: fragment
# Loadin packages
library(tidyverse)
library(quanteda)

# Text examples
texts <- c("LDA is a topic model algorithm",
           "Topic modeling is an interesting example of LDA",
           "News algorithms are important in journalism",
           "Journalism is important for society",
           "Journalists use algorithms in their work")

# Text preprocessing
(dtm_factor <- texts %>%
  tokens(remove_punct = T) %>%
  tokens_remove(stopwords("en")) %>%
  dfm %>%
  as_tibble %>%
  select(-doc_id))
```


## Dimensionality reduction using PCA

::: {.column width="45%"}
```{r}
#| echo: true
#| output-location: fragment
prcomp(dtm_factor, rank. = 2)

```

:::
::: {.column width="4%"}
:::
::: {.column width="45%"}

- Factor/Principal component analysis can reduce the number of columns (= reduce the dimensionality of the dataset)

- It assumes the manifest words are determined by (fewer) underlying latent factors

- In this simple example, it works to a certain degree, but we also see problems

:::

## Latent Semantic Analysis

- Apply Singular Value Decomposition to the DTM

    - Similar to factor analysis

- Found to mimic (some) human generalizations and even errors

- Also problematic to interpret:

    - Negative values
    - Not robust to ambiguous terms and antonyms
    - No theoretical interpretation of mechanism
 
<p style="font-size:0.65em;">_e.g. Deerwester et al., 1990_</p>


## Preliminary conclusion

- Clustering, factor analysis, principal component analyis and its extension "latent semantic analysis" can all be used to reduce the dimensionality of a document-term matrix

- The resulting dimension may be a basis for extracting topics (after all it is words with loadings onto different factors = topics)

- Yet, there are several problems

    - Negative values difficult to interpret
    - The underlying model is hard to intepret


# Latent Dirichlet Allocation - LDA Topic Modeling {background-color="steelblue"}


## Intuitions behind LDA topic modelling

![](img/blei_lda.png)

<p style="font-size:0.75em;">_Blei et al., 2003_</p>



## Latent Dirichlet Allocation - LDA topic modeling

- Evolution of Latent Semantic Analysis

- Is Based on generative statistical model that aligns with how articles or documents are written

    - It ‘assumes’ an author who...
    - chooses a mix of topics to write about.
    - For each word, s/he select one of the topics... 
    - and then select a word from this topic.

- This basically results in a mixture model:

    - Words can be in multiple topics (→ deals with ambiguity)
    - Documents in multiple topics (→ deals with mixed content)
    - But skewed towards a couple of topics, depending on $\alpha$ hyperparameter (more on this later)


## Example for the generative model

Let's assume that you are a journalist writing a 500 word news item. 

1. You would choose one or more topics to write about, for example 70% healthcare and 30% economy. 

2. For each word in the item, you randomly pick one of these topics based on their respective weight. 

3. Finally, you pick a random word from the words associated with that topic, where again each word has a certain probability for that topic. For example, "hospital" might have a high probability for healthcare while "effectiveness" might have a lower probability but could still occur

<p style="font-size:0.75em;">_Van Atteveldt, Trilling & Calderon, 2021_</p>


## Dirichlet Distribution

- Every topic and document is a probability distribution (over words / topics resp.)
    - How likely is word w in topic z, or topic z in document d?

- These distributions are themselves randomly drawn

    - From a "dirichlet distribution" which yields multinomial distributions

- So, what is a "Dirichlet distribution"?

## The conference dinner...

- You walk into the room for the conference dinner and want to sit somehwere

- You are afraid to sit alone, so prefer a table with people 
  
  - $P(t_i) = n_i / sum(n)$
  - To avoid $sum(n)=0$ , every table starts at a baseline $n = \alpha$

- Everyone does the same as they enter
    
  - Empty tables stay empty, full tables keep getting more people proportionally
  - After many people enter, an equilibrium emerges

## Effect of the hyperparameter $\alpha$

- The restaurant converges to multinomial distribution
      
  - E.g. the topics per document, of words per topic

- The initial number of people at the tables is the alpha hyperparameter
    
    - Hyperparameter = ‘setting’/choice that affects how other parameters are estimated

- Intuitive effect of lower alpha:
    
    - initial choices of ‘customers’ have larger effect
    - likelier that a single table will get all participants

- Lower alpha = fewer topics per document
    
    - but means topic have to include more words / have more overlap, as each word needs to be assigned


## Plate notation of LDA


::: {.column width="55%"}


![](img/lda_process.png)
<p style="font-size:0.75em;">_Blei et al., 2003_</p>


:::
::: {.column width="5%"}
:::
::: {.column width="35%"}


For each document *d*:

- Draw random topic proportions $\theta_d$
- For each word *n* in document *d*:
    - Draw a single topic $Z$ from $\theta_d$
    - Draw a word $W$ from $\beta_z$

$\beta_k$ and $\theta_d$ are drawn from $Dir(\alpha)$ and thus not observed directly

:::

- How can we determine these parameters?

## Think about a regression analysis...


::: {.column width="45%"}

- A regression analysis aims to predict $x$ from $y$

- It assumes a linear relationship between the two variables that can be expressed as

$y_i = \beta_0 + \beta_1*x_i + e_i$

- Similar to the topic model, we have $x$ and $y$, but we don't actually have the parameter of interest: $\beta_0$, and $\beta_1$

- Through methods such as least squares (including its most common variant, ordinary least squares), we try to find the value of the $\beta$'s  that minimize the sum of squared errors (distance from points to the regression line)

:::
::: {.column width="5%"}
:::
::: {.column width="45%"}


```{r}
#| fig-width: 5
#| fig-height: 5
set.seed(42)
x <- rnorm(100, 0, 1)
y <- 1.5*x + rnorm(100, 0, 2)

ggplot(NULL, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_classic() +
  labs(x = "Independent variable", y = "Dependent variable")


```


:::


## Reverse-Engineering the generative model

- The generative model assumes we know the parameters and want the words

- But similar to the regression analysis, our challenge is the opposite:

    - We know the actual words in the corpus
    - How can we find out the parameters?

- Task: Find the parameters that maximize the *likelihood* of the corpus

- Unfortunately, there is no analytic solution

    - Contrast with e.g. SVD and OLS that can be computed directly
    - Similar to multilevel models, this is not the case for LDA

- Need to do iterative approximation of best solution


## Gibbs Sampling in LDA

- Gibbs sampling refers to a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution from we cannot sample directly

- Suppose you know the topics of all words except for one word $w$

- This new word $w$ is the new guest entering the restaurant

    - Pick a topic within the document proportional to existing topics in the document (plus alpha)
    - Pick a topic for the word proportional to existing topics for this word (plus eta)

- This gives a joint probability for all topics for this word

## Iterative Gibbs

1. Start with random assignments of topics to words

2. For each word $w$ in document $d$

    - Compute proportion of topics in word and document (disregarding $w$ itself)
    - Compute probability of each topic $z$ given those proportions
    - Pick a new topic from that probability
    - Update proportions for next iteration

3. Repeat from 2 until converged

<p style="font-size:0.75em;">_Steyvers & Griths, 2007_</p>


## Let's look at an Example in R

```{r, R.options = list(width = 80)}
#| echo: true
#| output-location: fragment
library(quanteda)
library(quanteda.textplots)
library(tidyverse)
library(topicmodels)

s1 <- read_csv("data/science articles/train.csv") %>%
  select(id = ID, title = TITLE, abstract = ABSTRACT) %>%
  slice(1:5000) # Make it a little smaller to run faster

head(s1)
```


## Text Preprocessing and DTM

```{r, R.options = list(width = 120)}
#| echo: true
#| output-location: fragment
dtm <- s1 %>%
  corpus(text = "abstract") %>%
  tokens(remove_punct = T, remove_numbers = T) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_select(min_nchar = 2) %>%
  dfm %>%
  dfm_trim(min_termfreq = 5)
dtm
```

## Text preprocessing

- Text preprocessing is more important for topic modeling than for other machine learning approaches

- For example keeping stopwords will lead to non-sensical "topics" because they co-occur so often

- Stemming or lemmatizing will streamline words and thereby improve coherence between topics. 

- We want to make sure that the model focuses on those words that really tell something about the topic

- Text preprocessing stepts are often pivotal for the interpretability of the resulting topics


## Estimating a topic model in R

```{r}
#| echo: true
#| output-location: fragment
library(topicmodels)

# Convert dtm to topicmodels' specific format
dtm <- convert(dtm, to = "topicmodels") 

# Set seed to make it reproducible
set.seed(1)

# Fit topic model
m <- LDA(dtm, 
         method = "Gibbs", 
         k = 6,  
         control = list(alpha = 0.1))
m
```

<br>

- We need to chose the sampling method: usually Gibbs sampling

- We need to specify the number of topics ($k$) a priori, but we can of course try out different solutions

- We can set the alpha parameter for the Dirichlet distribution

## Inspecting LDA results

By using the function `terms()`, we can have a look at the most probable words in each of the six topics:

```{r}
#| echo: true
terms(m, 15) %>%
  as_tibble
```

<br>

. . .

- What topics do these wordlists stand for?

## Top words in each topic

But because we gain a probability with which a word is in a topic, we can also look at these probabilities per word per topic:

<br>

::: {.column width="45%"}

```{r}
#| echo: true
#| output-location: fragment
topic <- 3
words <- posterior(m)$terms[topic, ]
topwords <- sort(words, 
                 decreasing = T) %>%
  head(n = 50)
head(topwords, 10) %>% 
  as.data.frame
```


:::
::: {.column width="5%"}
:::
::: {.column width="45%"}

```{r}
#| echo: true
#| output-location: fragment
library(wordcloud)
wordcloud(names(topwords), topwords)
```

:::

## Alternative visualization

```{r}
tibble(topic = 1:6) %>%
  apply(1, function(x) posterior(m)$terms[x,]) %>%
  as.data.frame() %>%
  rownames_to_column("terms") %>%
  as_tibble %>%
  gather(key, value, -terms) %>%
  mutate(key = recode(key, V1 = "Statistics?", V2 = "Computer Science?", V3 = "(Astro-)Physics", 
                         V4 = "Artificial Intelligence", V5 = "Math", V6 = "Physics")) %>%
  group_by(key) %>%
  arrange(key, -value) %>%
  slice(1:10) %>%
  ggplot(aes(x = fct_reorder(terms, value), 
                y = value,
             fill = key)) +
  geom_col() +
  facet_wrap(~key, scales = "free", nrow = 2) +
  coord_flip() +
  theme(legend.position = "none") +
  labs(y = "Frequency", x = "",
       title = "Topwords per Topic")
```


## Probabilites of topics per document

- From the topic model, we can extract a table that shows us the probabilities of a topic being written about in the different texts (bear in mind, several topics can be in one text)

- We see for example, that there is a high probability of the text 7 being about topic 3 (astrophysics)


```{r}
#| echo: true
#| output-location: fragment
posterior(m)$topics %>% 
  as.data.frame() %>%
  rownames_to_column("doc") %>%
  as_tibble
```


## Is text 7 really about astrophysics?


```{r}
#| echo: true
#| output-location: fragment
s1 %>%
  filter(id == 7) %>%
  select(abstract) %>%
  as.character()
```

## Probabilites of topics per document

Let's check out another one: text 2 seems to be primarily about topic 4 (machine learning?):

::: {.column width="62%"}

```{r}
#| echo: true
#| output-location: fragment
topic <- 4
words <- posterior(m)$terms[topic, ]
topwords <- head(sort(words, decreasing = T), n=50)
wordcloud(names(topwords), topwords)
```

:::
::: {.column width="35%"}

```{r}
#| echo: true
#| output-location: fragment
s1 %>%
  filter(id == 2) %>%
  select(abstract) %>%
  as.character()
```

:::

## Substantive analyses

Depending on the research questions, we now may for example want to describe the topic prevalence in the corpus. 

```{r}
#| echo: true
#| output-location: column-fragment
(table <-  posterior(m)$topics %>%
  as.data.frame %>%
  rownames_to_column("docs") %>%
  gather(topic, value, -docs) %>%
  as_tibble %>%
  arrange(docs, value))
```


## Visualization of topic prevalence

- Bear in mind that this is not the proportion of articles that are about this topic!

- It is the overall proportion of the topic in the corpus given that articles can be about several topics


```{r}
#| echo: true
#| output-location: column-fragment
#| fig-width: 5
#| fig-height: 4
table %>%
  group_by(topic) %>%
  summarize(prop = mean(value)) %>%
  mutate(topic = recode(topic, 
         "1" = "Statistics?", 
         "2" = "Computer Science?", 
         "3" = "(Astro-)Physics", 
         "4" = "Artificial Intelligence", 
         "5" = "Math", 
         "6" = "Physics")) %>%
  ggplot(aes(x = fct_reorder(topic, prop), 
             y = prop, fill = topic)) +
  geom_col() +
  coord_flip() +
  theme(legend.position = "none") +
  labs(x = "", 
       y = "Proportion in the corpus")
```



## Validation

- The first step after fitting a model is inspecting the results and establishing face validity

- Top words per topic are a good place to start, but one should also look at the top documents per topic to better understand how words are used in context. 

- Also, it is good to inspect the relationships between topics and look at documents that load high on multiple topics to understand the relationship

. . .

- If one using topic models in a more confirmatory manner, e.g., the topics should match some sort of predefined categorization, you should use regular gold standard techniques for validation: 

    - code a sufficiently large random sample of documents with your predefined categories, and test whether the LDA topics match those categories (compute accuracy, precision, recall, F1-score)
    - In general, however, in such cases it is a better idea to use a dictionary or supervised analysis technique as topic models often do not exactly capture our categories

## Choosing the right number of topics

- Topic models such as LDA allow you to specify the number of topics in the model

    - This is a nice thing because it allows you to adjust the granularity of what topics measure: between a few broad topics and many more specific topics. 
    - But it begets the question what the best number of topics is.


. . .

- The short and perhaps disappointing answer is: The best number of topics does not exist
    
    - there is no singular idea of what a topic even is is
    - it depends on what you are interested in: e.g., if you want know what a corpus is about, you want to have a limited number of topics that provide a good representation of overall themes

. . .

- But even if the best number of topics does not exist, some values for k (i.e. the number of topics) are better than others. 
  
    - If we use too few topics, there will be variance in the data that is not accounted for, 
    - If you use too many topics you will overfit and get topics that are not interesting


## How well does our model fit the data?

- One  approach to the "best" number of topics is to check which model best predicts the data

- This is comparable to goodness-of-fit measures for statistical models (e.g., log likelihood, CFI, etc.)

- For LDA topic models, a commonly usd indicator is **perplexity** (Blei, Ng, & Jordan, 2003), where lower perplexity indicate better predictin/fit

- To calculate perplexity, we follow an already known procedure (last lecture!):

    - We first train an LDA model on a portion of the data 
    - Then, we model is evaluatd using the held-out portion of the data
    - This procedure is repeated for models with different numbers of topics so that it becomes clear which one leads to the lowest perplexity


## Calculating perplexity in R

- We first have to split up our data into data for training and testing the model

- This way we prevent overfitting the model

- Here we'll use 75% for training, and held-out the remaining 25% for test data.

```{r}
#| echo: true
# Split sample
train <- sample(rownames(dtm), nrow(dtm) * .75)
dtm_train <- dtm[rownames(dtm) %in% train, ]
dtm_test <- dtm[!rownames(dtm) %in% train, ]
```

- Then, we train the model and then, we calculate the perplexity:

```{r}
#| echo: true
#| output-location: fragment
m_train <- LDA(dtm_train, method = "Gibbs", k = 6,  control = list(alpha = 0.01))
perplexity(m, dtm_test)
```

## Estimating several model and perplexity scores

- A single perplexity score is not really useful as we have nothing to compare it against 

- Instead, we calculate the perplexity score for models with different parameters, to see how this affects the perplexity

```{r}
#| echo: true
#| output-location: column-fragment
p <- data.frame(k = c(3,6,12,24,48,96),
                perplexity = NA)

## loop over the values of k in data.frame p 
for (i in 1:nrow(p)) {
  # calculate perplexity for the given value of k
  m <- LDA(dtm_train, 
           method = "Gibbs", 
           k = p$k[i],  
           control = list(alpha = 0.01))
  # store result in our data.frame
  p$perplexity[i] = perplexity(m, dtm_test)
}

# Output
p
```


## Visualizing a perplexity curve

- Technically, the best fitting model is the one with the lowest perplexity score
  
  - But this will always be a model with a lot of topics!

- If we want to use topic modelling for bottom-up inductive analyses of text corpora, we need to look for a "knee" in the plot

```{r}
#| echo: true
#| output-location: column-fragment
#| fig-height: 4
#| fig-width: 5
ggplot(p, aes(x = k, y = perplexity)) + 
  geom_line(color = "red") +
  theme_minimal() +
  labs(x = "Number of topics (k)",
       y = "Perplexity",
       title = "Perplexity curve")
```


# Example from the literature {background-color="steelblue"}

## What communication scholars write about

- Günther and Domahidi (2017) used LDA topic modeling to get an overview of research topics in 80 years of communication research

- Documents were 15,000 abstracts from academic journals

- To find a reasonable value k (number of topics), they ran 40 topic models with k = 5 to k = 200 and systematically compared them: The final model hat 145 topics

- For each document (abstract), they selected the two topics with the highest probability (minimum probability was .1)

- To ensure a meaningful interpretation, the authors validated the labels of the inferred topics by manually checking publications from every decade that contained the topic with high probability. 

## Core topics in communication research

![](img/gunther_table1_full.png)


## Evolution of topics over time

![](img/gunther_fig1.png)


## Type of media researched over time

![](img/gunther_fig4.png)


# Conclusions and outlook {background-color="steelblue"}

## What is the state of the art?

- LDA is still one of the most used approaches to topic modeling and performs well in many circumstances

- Yet, there are many new approaches including e.g., structural topic modeling (see R package `stm`; Roberts et al., 2019)
    
    - an extension of LDA that allow us to explicitly model text metadata such as date or author as covariates of the topic prevalence and/or topic words distributions
    - Text contextual information into account!

- Recent advances have been made by using pre-trained transformer-based language models (BERT, BERTopic; Grootendorst, 2022) 

    - generates document embedding with pre-trained transformer-based language models
    - clusters these embeddings
    - generates topic representations with the class-based TF-IDF procedure.

## Conclusion

- Topic Modeling reduces the dimensionality of a document-term matrix by clustering words and documents into "latent" topics

- Interpretation of resulting "topic solutions" must be done by the researcher (face validity)

- LDA is driven by the Dirichlet process that yields distributions 

  - skewed towards few topics, but controllable with alpha parameter
  
- Gibbs sampling is a non-deterministic way to fit LDA models


# Thank you for your attention! {background-color="steelblue"}

## Required Reading

<br><br>

Günther, E. , & Domahidi, E. (2017). What Communication Scholars Write About: An Analysis of 80 Years of Research in High-Impact Journals. International Journal of Communication 11(2017), 3051--3071

Jacobi,C., van Atteveldt, W. & Welbers, K. (2016) Quantitative analysis of large amounts of journalistic texts using topic modelling. Digital Journalism, 4(1), 89-106, DOI: 10.1080/21670811.2015.1093271

<br>

*(available on Canvas)*

## References {.smaller}

- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.

- Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794

- Roberts, M. E., Stewart, B. M., & Tingley, D. (2019). Stm: An R package for structural topic models. Journal of Statistical Software, 91, 1-40.

- Steyvers, M., & Griffiths, T. (2007). Probabilistic topic models. In Handbook of latent semantic analysis (pp. 439-460). Psychology Press.

- van Atteveldt, W., Trilling, D., & Calderon, C. A. (2022). Computational Analysis of Communication. John Wiley & Sons.

------------------------------------------------------------------------

## Example Exam Question (Multiple Choice)

Which of the following statements is false? In an LDA model...

A. ...each word can be in every topic and every document can be about every topic.

B. ...each word is linked to one specific topic, but every document can be about several topics.

C. ...each word can be in every topic, but every document is about one specific topic.

D. ...each word is linke to one specific topic and every document is about one specific topic. 


## Example Exam Question (Multiple Choice)

Which of the following statements is false? In an LDA model...

**A. ...each word can be in every topic and every document can be about every topic.**

B. ...each word is linked to one specific topic, but every document can be about several topics.

C. ...each word can be in every topic, but every document is about one specific topic.

D. ...each word is linke to one specific topic and every document is about one specific topic. 



## Example Exam Question (Open Question)


Why is it important to carefully consider preprocessing steps in topic modelling?

. . .

A topic model does not analyse documents directly, but uses a so-called docu-
ment–term matrix based on these documents. This matrix lists the frequency for each
term (word) in each document. The first step in creating this matrix is tokenization,
which means splitting the text into a list of words. For many machine learning approaches, no further steps are necessary. However, for topic modeling it is worth considering whether further preprocessing steps such as stemming or lemmatization, removal of stopwords (or otherwise frequent but non-informative words) or frequency trimming is fruitful as they can signicantly improve the interpretability of the resulting topics. 

For example, stopwords do not really represent "topics". If they are kept in the document-term matrix, they would drive the topic selection in undesirable ways. Furthermore, differently conjugated words could end up in different topics, even though they stand for the same topic. Hence stemming or lemmatizing is usually a good choice for topic modelling.  

